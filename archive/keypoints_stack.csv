Key Points
"### **Key Points:**
1. To resolve issues with Minikube, try starting it with a different driver: ```minikube start --driver=none```."
"### **Key Points:**
1. Add Minikube IP to the `no_proxy` environment variable to ensure proper connectivity.  
   ```bash
   export no_proxy=$no_proxy,$(minikube ip)
   ```"
"### **Key Points:**
1. Configure the Ingress resource with TLS and backend service details, ensuring the correct alignment of ports.  
   ```yaml
   apiVersion: extensions/v1beta1
   kind: Ingress
   metadata:
     name: api
     namespace: production
     annotations:
       kubernetes.io/tls-acme: ""true""
       kubernetes.io/ingress.class: ""gce""
   spec:
     tls:
     - hosts:
       - api.foo.io
       secretName: api-tls
     rules:
     - host: api.foo.io
       http:
         paths:
         - path: /*
           backend:
             serviceName: api
             servicePort: 8080
   ```

2. Define the Service to expose the application, ensuring the correct port configuration.  
   ```yaml
   kind: Service
   apiVersion: v1
   metadata:
     name: api
   spec:
     selector:
       app: api
       role: backend
     type: NodePort
     ports:
       - protocol: TCP
         port: 8080
         targetPort: 8080
         name: http
   ```

3. Set up the Deployment with proper liveness and readiness probes, including initial delay settings.  
   ```yaml
   kind: Deployment
   apiVersion: extensions/v1beta1
   metadata:
     name: api-production
   spec:
     replicas: 1
     template:
       metadata:
         name: api
         labels:
           app: api
           role: backend
           env: production
       spec:
         containers:
         - name: api
           image: eu.gcr.io/foobarbar/api:1.0.0
           livenessProbe:
             httpGet:
               path: /readinez
               port: 8080
             initialDelaySeconds: 45
             periodSeconds: 10
           readinessProbe:
             httpGet:
               path: /healthz
               port: 8080
             initialDelaySeconds: 45
           env:
            - name: environment
              value: ""production""
            - name: gin_mode
              value: ""release""
           resources:
             limits:
               memory: ""500Mi""
               cpu: ""100m""
           imagePullPolicy: Always
           ports:
           - containerPort: 8080
   ```"
"### **Key Points:**
1. Specify the exact NGINX ingress controller being used in your Kubernetes environment to aid in troubleshooting, as different controllers may have slight operational differences.
2. Implement a default-backend by creating a deployment and service for it, which will handle traffic that does not match any ingress resource. Example implementation can be found at: ```github.com: uswitch: master: deploy: default-backend.yaml```.
3. Ensure your ingress resource includes the correct YAML manifests for better visibility into potential issues, and consider setting the path to `path: /` to allow all requests to be routed to your service.
4. Use the command ```$ kubectl logs -n namespace ingress-pod-name``` to display logs of the ingress controller for effective troubleshooting."
"### **Key Points:**
1. To enable external access for Keycloak and change the default host, update the Keycloak CRD configuration as follows:
   ```yaml
   externalaccess:
     enabled: true
     host: keycloak.example.org
   ```
2. Ensure that an ingress controller is deployed in your system to access Keycloak using the defined host."
"### **Key Points:**
1. Use a workaround configuration for the Kubernetes dashboard with specific annotations for NGINX ingress, including backend protocol support and configuration snippets.  
   ```yaml
   nginx.ingress.kubernetes.io/backend-protocol: ""https""
   nginx.ingress.kubernetes.io/configuration-snippet: |
     proxy_set_header accept-encoding """";
     sub_filter '<base href=""/"">' '<base href=""/dashboard/"">';
     sub_filter_once on;
   ```
2. Define an ingress rule that rewrites the target path for the dashboard service.  
   ```yaml
   nginx.ingress.kubernetes.io/rewrite-target: /$2
   ```"
"### **Key Points:**
1. Use the `helm template` command to generate Kubernetes manifests from a Helm chart and output them to a specified directory:  
   ```bash
   helm template mychart . --output-dir ./test && mv ./test/mychart/templates/* ./test && rm -r ./test/mychart
   ```"
"### **Key Points:**
1. Helm stores its state in secrets, with each release creating a new secret for its revision. The secret name ending in `.airflow.v29` contains information about revision number 29.
2. By default, Helm retains up to 10 revisions per release, but this can be adjusted using the `--history-max` flag.
3. To minimize the number of secrets created, you can set Helm to keep only one revision by running `helm upgrade --history-max=1`."
"### **Key Points:**
1. Implement source labels based routing in a VirtualService to route traffic based on the source label, using the following configuration:
   ```yaml
   apiVersion: networking.istio.io/v1alpha3
   kind: virtualservice
   metadata:
     name: socks-com
   spec:
     hosts:
     - sock.com
     http:
     - match:
       - sourceLabels:
           ui: v2
       - route:
         - destination:
             host: api
             subset: v2
     - route:
       - destination:
           host: api
           subset: v1
   ```
2. Update the DestinationRule to define two subsets for the service, ensuring proper routing based on the source labels."
"### **Key Points:**
1. Use a Docker image with Newman installed (e.g., `postman/newman`) and manage your JSON configuration files by either using a ConfigMap or copying them into the container.
2. Ensure your system is ready before running tests by either adding a sleep command in your script or actively checking the pod status with `kubectl`."
"### **Key Points:**
1. Enable debug mode and set host and port in the application configuration: ```app.run(debug=true,host='0.0.0.0',port=5000)```."
"### **Key Points:**
1. Initialize Kubernetes with the correct pod network CIDR for Calico:  
   ```sudo kubeadm init --pod-network-cidr=192.168.0.0/16```.
2. Set up kubeconfig for kubectl access:  
   ```mkdir -p $HOME/.kube```  
   ```sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config```  
   ```sudo chown $(id -u):$(id -g) $HOME/.kube/config```.
3. Apply the Calico RBAC configuration:  
   ```sudo kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml```."
"### **Key Points:**
1. **Create an NFS Server:** Install NFS utilities, start and enable the NFS server, create a storage directory, set permissions, and configure exports for the Kubernetes nodes.
   ```bash
   dnf install nfs-utils -y
   systemctl start nfs-server.service
   systemctl enable nfs-server.service
   mkdir /mnt/storage
   chown -r nobody: /mnt/storage
   chmod -r 777 /mnt/storage
   vi /etc/exports
   /mnt/storage    8.8.8.8(rw,sync,no_all_squash,root_squash)
   /mnt/storage    9.9.9.9(rw,sync,no_all_squash,root_squash)
   exportfs -arv
   ```

2. **Bind Kubernetes Nodes to the NFS Server:** Install NFS utilities on the client nodes, create a mount point, mount the NFS share, and configure it to mount on reboot.
   ```bash
   dnf install nfs-utils nfs4-acl-tools -y
   mkdir /mnt/cstorage
   mount -t nfs 1.1.1.1:/mnt/storage /mnt/cstorage
   vi /etc/fstab
   1.1.1.1:/mnt/storage /mnt/cstorage  nfs  defaults  0  0
   ```

3. **Add NFS Subdir External Provisioner Helm Repository:** Access the Rancher UI to add the NFS subdir external provisioner repository.
   ```
   helm repository url: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
   ```

4. **Install NFS Subdir External Provisioner via Charts:** Use the Rancher UI to find and install the NFS subdir external provisioner chart, setting the necessary parameters.
   ```

   ```

5. **Add Bitnami Helm Repository to Rancher:** Access the Rancher UI to add the Bitnami Helm repository.
   ```
   bitnami helm url: https://charts.bitnami.com/bitnami
   ```

6. **Install MongoDB Sharded via Rancher Charts:** Use the Rancher UI to find and install the MongoDB sharded chart, specifying the storage class.
   ```yaml
   global.storageclassname: nfs-client
   ```"
"### **Key Points:**
1. To disable the limitation on client request body size for a service exposed via an NGINX ingress, use the annotation: ```nginx.ingress.kubernetes.io/proxy-body-size: ""0""```."
"### **Key Points:**
1. Reserve a regional external IPv6 address using the command:  
   ```gcloud compute addresses create <your-ipv6-address-name> --global --ip-version=ipv6```.
   
2. Specify the global IP address in the Ingress resource YAML file with the annotation:  
   ```kubernetes.io/ingress.global-static-ip-name: <your-ipv6-address-name>```.

3. For using a Load Balancer, set the reserved static IP as the load balancer IP in the service YAML:  
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: my-lb-service
   spec:
     type: LoadBalancer
     loadBalancerIP: <ip>
   ```"
"### **Key Points:**
1. Ensure the corresponding storage class exists before creating a persistent volume. Check available storage classes with the command: ```kubectl get storageclasses --all-namespaces```.
2. Create a storage class with the appropriate provisioner to map to actual storage. Example definition: 
   ```yaml
   kind: StorageClass
   apiVersion: storage.k8s.io/v1
   metadata:
     name: fast
   provisioner: kubernetes.io/gce-pd  
   parameters:
     type: pd-ssd
     zones: us-central1-a, us-central1-b
   ```"
"### **Key Points:**
1. Use `resourcenames` instead of `resources` to specify object names, e.g., `resourcenames: [my-deployment-name]`.
2. Segment permissions by namespace for better management, limiting them to one or a few namespaces."
"### **Key Points:**
1. Set up a reverse proxy in Nginx and create necessary secrets and a ConfigMap for Nginx configuration. 
   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: nginxsecret
   ...
   apiVersion: v1
   kind: ConfigMap
   metadata:
     name: nginx-config
   ...
   ```

2. Configure Nginx with a specific configuration file that includes SSL settings and proxy settings. 
   ```nginx
   server {
       listen 80;
       server_name example.com;
       return 301 https://$host$request_uri;
   }

   server {
       listen 443;
       server_name example.com;
       ssl_certificate /certs/tls.crt;
       ssl_certificate_key /certs/tls.key;
       ...
       proxy_pass http://deployment-name:8080/;
   }
   ```

3. Create a Kubernetes deployment and service for the Nginx reverse proxy, ensuring to specify the correct ports and volume mounts for certificates and configuration files.
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: puntdoctor-lb
   spec:
     ports:
       - name: https
         port: 443
         targetPort: 443
       - name: http
         port: 80
         targetPort: 80
     selector:
       app: puntdoctor-nginx-deployment
     type: LoadBalancer
   ---
   apiVersion: extensions/v1beta1
   kind: Deployment
   metadata:
     name: puntdoctor-nginx-deployment
   spec:
     replicas: 2
     template:
       metadata:
         labels:
           app: puntdoctor-nginx-deployment
       spec:
         containers:
           - name: adcelerate-nginx-proxy
             image: nginx:1.13
             volumeMounts:
               - name: certs
                 mountPath: /certs/
               - name: site-config
                 mountPath: /etc/site-config/
               - name: default-config
                 mountPath: /etc/nginx/
             ports:
               - containerPort: 80
                 name: http
               - containerPort: 443
                 name: https
         volumes:
           - name: certs
             secret:
               secretName: nginxsecret
           - name: site-config
             configMap:
               name: nginx-config
           - name: default-config
             configMap:
               name: default
   ```"
"### **Key Points:**
1. Replace the current `kubectl` binary on Linux by downloading the previous version and moving it to the appropriate directory: 
   ```bash
   curl -lo https://storage.googleapis.com/kubernetes-release/release/v1.5.2/bin/linux/amd64/kubectl
   chmod +x ./kubectl
   sudo mv ./kubectl /usr/local/bin/kubectl
   ```
2. Replace the current `kubectl` binary on macOS by downloading the previous version and moving it to the appropriate directory: 
   ```bash
   curl -lo https://storage.googleapis.com/kubernetes-release/release/v1.5.2/bin/darwin/amd64/kubectl
   chmod +x ./kubectl
   sudo mv ./kubectl /usr/local/bin/kubectl
   ```
3. Download the previous version of `kubectl` for Windows and ensure it is added to the system path: 
   ```bash
   curl -lo https://storage.googleapis.com/kubernetes-release/release/v1.5.2/bin/windows/amd64/kubectl.exe
   ```"
"### **Key Points:**
1. To resolve the issue with `--dry-run` not generating resources using CRDs in Helm v3, manually install the CRDs first. This allows `--dry-run` to work correctly. 
2. The stable Prometheus operator has been renamed to `prometheus-community/kube-prometheus-stack`, and the Bitnami Prometheus operator is now `bitnami/kube-prometheus`."
"### **Key Points:**
1. Namespace deployed resources by `.release.name` to allow parallel deployment of charts alongside existing software, facilitating a smooth transition to on-chart services.
2. Update off-chart services to point to on-chart pods, enabling both on-chart and off-chart services to operate with distinct ELBs, simplifying the eventual migration process."
"### **Key Points:**
1. To prevent k3s from using the default Traefik ingress controller, deploy k3s with the `--no-deploy traefik` option and store your modified version in the `k3s/server/manifests` directory.
2. To disable the Traefik ingress controller, start each server with the `--disable traefik` option.
3. If deploying the NGINX ingress controller, use the annotation `kubernetes.io/ingress.class: ""nginx""` to ensure it is prioritized when multiple ingress controllers are present."
"### **Key Points:**
1. Create a Kubernetes secret from a certificate using the command line with the following command: ```kubectl create secret generic secret-db-ca --from-file=.tls.ca=digitalocean-db.cer```.
2. Alternatively, create a secret from a manifest by providing a base64 encoded string in the data field, using type opaque: 
   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: secret-db-ca
   type: opaque
   data:
     .tls.ca: |
        ls0tls1crudjtibdrvj..........
   ```"
"### **Key Points:**
1. Add `amazoneksclusterpolicy` to the EKS cluster role.  
2. Add `amazoneksservicepolicy` to the EKS cluster role."
"### **Key Points:**
1. CPU usage is measured in nanocores, and the relevant code snippet for adding CPU samples is:  
   ```metrics_pods_cpu.add_sample('kube_metrics_server_pods_cpu', value=int(pod_container_cpu), labels={ 'pod_name': pod_name, 'pod_namespace': pod_namespace, 'pod_container_name': pod_container_name })```.
2. Memory usage is measured in kibibytes, with the relevant metric for nodes being:  
   ```kube_metrics_server_nodes_mem```."
"### **Key Points:**
1. Avoid relying on global state in scripts; explicitly specify context, cluster, user, and namespace for clarity. Example: ```kubectl delete deployment/primary-service --cluster=test-cluster --namespace=test-namespace --user=test-user```.
2. Use variables for configuration files instead of hard-coded values to reduce errors. Example: ```kubeconfig=sam-monday-morning-config.yaml kubectl delete deployment/primary-service```."
"### **Key Points:**
1. Enhance your ClusterRole definition by referring to the example provided in the linked GitHub repository: [2_roles.yml](https://github.com/vencrena-lt/traefik/blob/main/2_roles.yml).
2. Ensure Traefik picks up routes from Kubernetes by specifying the following flags: 
   ``` 
   - --providers.kubernetescrd 
   - --providers.kubernetesingress 
   ```
3. Apply IngressRoutes in the same namespace where Traefik is deployed to ensure proper routing."
"### **Key Points:**
1. Specify a local chart in the `skaffold.yaml` file using the `deploy.helm.release.chartpath` field.
2. Specify a remote chart in the `skaffold.yaml` file using the `deploy.helm.release.remotechart` field."
"### **Key Points:**
1. Downgrade your kubectl client to version 1.12 to resolve compatibility issues with the server. 
2. Migrate Ingress resources from `extensions/v1beta1` to `networking.k8s.io/v1beta1` API, as `extensions/v1beta1` is deprecated in v1.19."
"### **Key Points:**
1. Verify the Kubernetes configuration file location by checking `~/.kube/config` and view its contents with `kubectl config view`.
2. Ensure the current context is set to `microk8s` using the commands:
   ```bash
   kubectl config get-contexts
   kubectl config set-context <context-name>
   kubectl config use-context microk8s
   ```
3. Confirm that you are referencing the correct kubeconfig file and not a private configuration by checking the `KUBECONFIG` environment variable with:
   ```bash
   echo $KUBECONFIG
   ```"
"### **Key Points:**
1. Install RabbitMQ using Helm with specific configurations: 
   ```bash
   helm install --set replicacount=2 \
                --set rabbitmqusername=yourusername \
                --set rabbitmqpassword=yourpassword \
                --set prometheus.operator.enabled=false \
                ha-rabbitmq stable/rabbitmq-ha
   ```
2. Downgrade to RabbitMQ HA chart version 1.44.1 to avoid issues introduced in version 1.44.2."
"### **Key Points:**
1. Implement verification for non-root user by checking if `uid` is nil and `username` is non-empty, returning an error if the user is non-numeric. Code snippet: 
   ```go
   case uid == nil && len(username) > 0:
       return fmt.Errorf(""container has runasnonroot and image has non-numeric user (%s), cannot verify user is non-root"", username)
   ```

2. Set a numeric user value in the pod definition to avoid verification errors, for example, using `runasuser: 999` in the security context. Code snippet:
   ```yaml
   securityContext:
       runAsUser: 999
   ```"
"### **Key Points:**
1. Name the ports in your pod specification for clarity, e.g., ```ports: - name: http containerPort: 8080 - name: https containerPort: 8443```.
2. Refer to the named ports in your service specification using the `targetPort` field, e.g., ```targetPort: http``` and ```targetPort: https```."
"### **Key Points:**
1. Change the storage class name to the default on EKS by using the following configuration:  
   ```storageclassname: {{ .values.storage.class | default ""gp2"" | quote }}```.  
2. Set the storage class in the storage configuration as follows:  
   ```class: ""gp2""```."
"### **Key Points:**
1. To configure MongoDB authentication via Helm, use the `--set` flag with curly braces for multiple values:  
   ```--set auth.usernames={bob,alice},auth.passwords={bobpass,alicepass},auth.databases={bobdb,alicedb}```.
2. To decode the generated MongoDB passwords from the secret, use the following command:  
   ```echo -n ym9icgfzcyxhbgljzxbhc3m= | base64 -d```."
"### **Key Points:**
1. Ensure that ports in the service definition are named when there is more than one port, and the selector should only be defined once per service.  
   ```yaml
   selector:
     app: etools
   ports:
   - name: web
     port: 8080
     targetport: 8080
   - name: other-port-something
     port: 3100
     targetport: 3100
   ```

2. It is recommended to use natural language names for ports in the pod specification to decouple service changes from container port changes.  
   ```yaml
   ports:
   - name: http
     port: 80
     targetport: http-in-my-pod
   ```"
"### **Key Points:**
1. Use a supported driver for Minikube, such as the Docker driver, as it is easier to configure and does not require root access.
2. Downgrade to Minikube v1.11 is an option, but it is not recommended."
"### **Key Points:**
1. Configure the same environment variables for the user to use the IAM role as the root user.
2. Ensure the user has access to the path specified in the `aws_web_identity_token_file` variable."
"### **Key Points:**
1. Use the command to retrieve the appropriate JSON configuration for a Kubernetes deployment: ```kubectl apply --server-dry-run -f deployment.yaml -o json```."
"### **Key Points:**
1. To resolve errors caused by using uws as a websocket engine, switch to the default socket.io configuration: ```app.configure(socketio())```.
2. If using uws, ensure the Docker container is based on Node 8 instead of Node 10 for compatibility: use an Alpine Docker container based on Node 8."
"### **Key Points:**
1. Enable production mode by adding the extra argument `--auto-build` in the configuration.  
   ```yaml
   - name: keycloak_extra_args
     value: ""--auto-build""
   ```"
"### **Key Points:**
1. Verify the credentials used for the connection to the Docker registry to ensure proper authorization with the `--docker-username` and `--docker-password` flags.
2. Consider managing secrets within a GitLab Runner Helm chart template for better handling of sensitive information."
"### **Key Points:**
1. Use the GitHub repository at `https://github.com/kow3ns/kubernetes-zookeeper/tree/master/docker` for the correct Dockerfile and build environment.
2. Ensure the Makefile aligns with the image `k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10`."
"### **Key Points:**
1. Use service port 8080 for API requests, as shown in the code snippet: `var content = await req.getasync(""https://cepserviceone.cep-dev.svc.cluster.local:8080/api/values"");`."
"### **Key Points:**
1. The error originates from a collision between services during deployment due to poor separation in the CI/CD job, not from Kubernetes or Ingress-NGINX.
2. Logs indicate that a service deployment is being overwritten by another environment's deployment, leading to requests being sent to a different namespace, while the ingress routing itself is functioning correctly."
"### **Key Points:**
1. If the metrics-server service is down with a crashloopbackoff error, delete the apiservice using: ```kubectl delete apiservice/""service_name""```, where the service name is typically `v1beta1.metrics.k8s.io`.
2. To resolve issues with the coredns pod, check its status with: ```kubectl describe pod/""pod_name""```, and if the error indicates an unknown directive proxy, replace `proxy` with `forward` in the CoreDNS configuration YAML file."
"### **Key Points:**
1. Set `automount_service_account_token` to `true` in the `template.spec` section of the `kubernetes_deployment` in your Terraform file to resolve deployment issues.  
   ```hcl
   automount_service_account_token = true
   ```"
"### **Key Points:**
1. Restructure your Helm chart directory to follow the ""subchart"" pattern by creating a parent app directory with a new `chart.yaml`, `values.yaml`, and a `templates` folder. 
   ```plaintext
   parent app
   |--chart.yaml
   |--values.yaml
   |--templates
   |----_helpers.tpl
   |--charts
   |----app1
   |------chart.yaml
   |------values.yaml
   |------templates
   |--------deployment.yaml
   |----app2
   |------chart.yaml
   |------values.yaml
   |------templates
   |--------deployment.yaml
   ```
2. Ensure that the `_helpers.tpl` file is included in the `templates` directory to be automatically referenced in the subcharts."
"### **Key Points:**
1. Mount the basic authentication file to the kube-apiserver pod by adding it to the volumes and volumeMounts sections.  
   ```yaml
   volumeMounts:
   - mountPath: /etc/kubernetes/auth.csv
     name: kubernetes-dashboard
     readOnly: true
   volumes:
   - hostPath:
       path: /etc/kubernetes/auth.csv
     name: kubernetes-dashboard
   ```"
"### **Key Points:**
1. Do not change the release name in Helm, as it is crucial for identifying resources. 
2. To fix the immediate problem, update the annotation for the problematic resource using the command: ```kubectl annotate --overwrite service app-svc meta.helm.sh/release-name=rel-124```."
"### **Key Points:**
1. Use NodePort service type for MariaDB primary service to expose it externally, with a specified NodePort of 32036.  
   ```yaml
   primary:
     service:
       type: nodeport
       nodeport: 32036
   ```
2. Configure MariaDB with replication architecture and set root and replication passwords.  
   ```yaml
   mariadb:
     clusterdomain: a4b-kube.local
     auth:
       rootpassword: ""password""
       replicationpassword: ""password""
     architecture: replication
     secondary:
       replicacount: 2
   ```"
"### **Key Points:**
1. Use the `helm install` command without the `--name` flag to install a Helm chart, as shown:  
   ```bash
   helm install --version 1.7.0 kong kong/kong --namespace kong --set ingresscontroller.enabled=true --set image.tag=1.4 --set admin.usetls=false
   ```
2. Add the Kong repository and update it before installation:  
   ```bash
   helm repo add kong https://charts.konghq.com
   helm repo update
   ```"
"### **Key Points:**
1. Install the `glusterfs-fuse` package on Kubernetes nodes to enable mounting of GlusterFS volumes.  
   ```bash
   sudo apt-get install glusterfs-fuse
   ```"
"### **Key Points:**
1. Change single quotes to double quotes in the command to retrieve the server URL for the 'dev' cluster:  
   ```kubectl config view -o jsonpath=""{.clusters[?(@.name == 'dev')].cluster.server}""```"
"### **Key Points:**
1. Apply all files in a folder using the command: ```kubectl apply -f <folder>```.
2. Use Kustomize for parameterization of manifest files and apply them with: ```kubectl apply -k <folder>```."
"### **Key Points:**
1. Create a ConfigMap for appsettings with Helm values to provide the appsettings file.  
   ```yaml
   apiVersion: v1
   kind: ConfigMap
   metadata:
     name: appsettings
   data:
     appsettings.dev.json: |-
       {
         ""logging"": {
           ""loglevel"": {
             ""default"": {{my__helmvalue}},
           }
         }
       }
   ```

2. Specify volumes and volumeMounts in your Pod definition to mount the appsettings file at the desired location.  
   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: examplepod
   spec:
     containers:
       - name: test-container
         image: myimage
         volumeMounts:
         - name: config-volume
           mountPath: /app ## specify your path to overwrite the appsettings file!
     volumes:
       - name: config-volume
         configMap:
           name: appsettings
     restartPolicy: Never
   ```"
"### **Key Points:**
1. Remove unnecessary commands from the Dockerfile for log directory creation and permissions: 
   ```dockerfile
   run mkdir -p $log_dir/boot $log_dir/access
   run chmod -r 0777 $log_dir/*
   ```
2. Use an init container to set up log directories and permissions in a Kubernetes pod:
   ```yaml
   initcontainers:
   - name: setup
     image: busybox
     command: [""bin/ash"", ""-c""]
     args:
     - >
       mkdir -p /logs/boot /logs/access;
       chmod -r 777 /logs
   volumemounts:
   - name: logs
     mountpath: /logs
   ```
3. Configure the main application container to log to the created directories:
   ```yaml
   containers:
   - name: app
     image: busybox
     command: [""bin/ash"", ""-c""]
     args:
     - >
       while :; do echo ""$(date): $(uname -r)"" | tee -a /logs/boot/boot.log /logs/access/access.log; sleep 1; done
   volumemounts:
   - name: logs
     mountpath: /logs
   ```
4. Set up a logger container to tail the application logs:
   ```yaml
   - name: logger
     image: busybox
     command: [""bin/ash"", ""-c""]
     args:
     - >
       sleep 5;
       tail -f /logs/boot/boot.log /logs/access/access.log
   volumemounts:
   - name: logs
     mountpath: /logs
   ```"
"### **Key Points:**
1. Set the `serviceAccountName` in the pod spec to associate the pod with a specific service account. The code snippet is:
   ```yaml
   serviceAccountName: my-k8s-svc-acct
   ```"
"### **Key Points:**
1. GCE persistent disks only support `ReadWriteOnce` (RWO) and `ReadOnlyMany` (ROX) access modes; attempting to use `ReadWriteMany` (RWX) will result in failure. 
2. For `ReadWriteMany` access, consider using NFS storage, as it allows all nodes to read and write to the storage device. 
3. To implement NFS, refer to Googleâ€™s guide on using Filestore, which provides fully managed NFS file servers."
"### **Key Points:**
1. Change your StorageClass to limit availability to only two zones for regional persistent disks in Google Cloud. 
   ```yaml
   apiVersion: storage.k8s.io/v1
   kind: StorageClass
   metadata:
     name: regional-pd
   provisioner: pd.csi.storage.gke.io
   parameters:
     type: pd-standard
     replication-type: regional-pd
     zones: zone1,zone2
   ```"
"### **Key Points:**
1. Use the `-o yaml` option with `kubectl taint` to generate a YAML file for the taint configuration, ensuring to remove the status and unnecessary details for later use.  
   Code snippet: `kubectl taint node minikube dedicated=foo:prefernoschedule -o yaml`
   
2. Apply the generated YAML file using `kubectl apply` to implement the taint on the node.  
   Code snippet: 
   ```yaml
   apiVersion: v1
   kind: node
   metadata:
     name: minikube
   spec:
     taints:
     - effect: prefernoschedule
       key: dedicated
       value: foo
   ```"
"### **Key Points:**
1. Correct the hostname to `rabbitmq-rabbitmq-svc.rabbitmq.svc.cluster.local`."
"### **Key Points:**
1. Add the remote host key to the `~/.ssh/known_hosts` file in the container to establish trust.
2. Disable host key checking (not recommended for security reasons) using the command: ```ssh -o ""StrictHostKeyChecking=no"" user@host```."
"### **Key Points:**
1. Ensure that liveness and readiness probes are correctly mapped to the same port as the start probe to avoid deployment failures. 
2. Update the `deployment.yml` file to align the port configurations for probes."
"### **Key Points:**
1. Construct a label selector using a range expression to support arbitrary label selectors from a values file.  
   Code snippet: 
   ```yaml
   labelselector:
     matchexpressions:
     {{- range $key, $value := .values.selectorlabels }}
     - key: {{ $key | quote }}
       operator: in
       values: {{ $value | quote }}
     {{- end }}
   ```"
"### **Key Points:**
1. Create a NetworkPolicy to allow internet access for specific pods by tagging them, using the following code snippet:
   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: NetworkPolicy
   metadata:
     name: internet-egress
   spec:
     podSelector:
       matchLabels:
         networking/allow-internet-egress: ""true""
     egress:
     - {}
     policyTypes:
     - egress
   ```

2. Allow internet access by defining IP blocks in a NetworkPolicy, which permits access to all IPs except for a specified block, using the following code snippet:
   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: NetworkPolicy
   metadata:
     name: allow-internet-only
   spec:
     podSelector: {}
     policyTypes:
       - egress
     egress:
       - to:
         - ipBlock:
             cidr: 0.0.0.0/0
             except:
               - 10.0.0.0/8
   ```"
"### **Key Points:**
1. Set public and private IPs for nodes in a custom cluster before installation to ensure the correct xip.io hostname mapping. This can be done in the advanced options during the setup. 
   ```yaml
   # Example of setting public and private IPs (not actual code)
   public_ip: <public-ip>
   private_ip: <private-ip>
   ```

2. Use a Layer 4 load balancer in front of all worker nodes to handle traffic on ports 80 and 443, allowing the ingress controller to listen on all nodes and route traffic internally. 
   ```plaintext
   domain.com -> load balancer -> ingress controller (on all nodes) -> service -> pods
   ```

3. Change your service type to ClusterIP when using an ingress controller, as traffic will be routed through the ingress rather than directly to node ports. 
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: my-service
   spec:
     type: ClusterIP
   ```

4. As a workaround, if you don't want to deploy a load balancer, delete and recreate the ingress with a manually set xip.io hostname to ensure it points to the correct public IP. 
   ```plaintext
   # Example of setting a manual hostname
   hostname: any.thing.you.want.1.2.3.4.xip.io
   ```"
"### **Key Points:**
1. Use a headless service for round-robin access to pods, referenced as `my-svc.my-namespace.svc.cluster.local`.
2. For fixed network identities, set up a StatefulSet and reference pods as `app-0.my-svc.my-namespace.svc.cluster.local`, `app-1.my-svc.my-namespace.svc.cluster.local`, etc.
3. Consider using the Memcached Helm chart for a StatefulSet configuration: [Memcached Helm Chart](https://github.com/kubernetes/charts/tree/master/stable/memcached)."
"### **Key Points:**
1. Create files in a non-root user owned directory, such as `/tmp`.
2. Create a custom Docker image by removing the non-root user instruction (user 1001) from the Dockerfile and host it in your own repository for use in your cluster."
"### **Key Points:**
1. Create a CronJob with a specified schedule and a container using the curl image to execute a POST request. 
   ```yaml
   apiVersion: batch/v1beta1
   kind: cronjob
   metadata:
     name: demo
   spec:
     schedule: ""*/15 * * * *""
     jobTemplate:
       spec:
         template:
           spec:
             nodeSelector:
               env: demo
             containers:
             - name: demo
               image: curlimages/curl
               command: [""/bin/sh""]
               args: [""-c"", 'curl -x post ""https://x.x.x/api/v1/cal/addy_link"" -h ""accept: application/json"" -d """" >/dev/null 2>&1']
             restartPolicy: Never
   ```"
"### **Key Points:**
1. Retrieve a specific pod's UID using jsonpath: ```$ kubectl get pods -n <namespace> <pod-name> -o jsonpath='{.metadata.uid}'```.
2. List all pod names along with their UIDs in a namespace using custom-columns: ```$ kubectl get pods -n <namespace> -o custom-columns=podname:.metadata.name,poduid:.metadata.uid```.
3. Use grep to filter the UID from the YAML output of a specific pod: ```$ kubectl get pods -n <namespace> <pod-name> -o yaml | grep uid```."
"### **Key Points:**
1. Use the `kubectl delete` command to remove an ingress resource by specifying its name and namespace: ```kubectl delete ingress abc-ingress -n default```."
"### **Key Points:**
1. To retrieve the image's digest from the Docker registry API, use the command to list all tags in your repository: ```$ gcloud container images list-tags us.gcr.io/my-project-37111/mysolution.host```.
2. For the full digest of the image, use the command with the JSON format flag: ```$ gcloud container images list-tags --format=json us.gcr.io/my-project-37111/mysolution.host```.
3. If you know the specific tag, you can directly call the registry API to get the digest: ```$ curl -h ""accept: *"" -h ""authorization: bearer $(gcloud auth print-access-token)"" -i https://us.gcr.io/v2/my-project-37111/mysolution.host/manifests/0.0.5-linux | grep ""digest""```."
"### **Key Points:**
1. List all accessible resources with appropriate operations by creating a ClusterRole that includes necessary resources and verbs. Code snippet: 
   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: clusterrole
   metadata:
     annotations:
       rbac.authorization.kubernetes.io/autoupdate: ""true""
     labels:
       kubernetes.io/bootstrapping: rbac-defaults
     name: cluster-admin
   rules: 
   - apiGroups: [""""] 
     resources: [""pods"",""services"",""namespaces"",""deployments"",""jobs""] 
     verbs: [""get"", ""watch"", ""list""]
   ```
2. Avoid modifying the existing cluster-admin role; instead, create a new role and assign users to it for better security practices."
"### **Key Points:**
1. Add the `linkerd-await` binary to the Flink image and override the entrypoint for the job manager to prevent issues during job upgrades. Code snippets:
   ```dockerfile
   RUN apt-get update && apt-get install -y wget
   RUN wget https://github.com/linkerd/linkerd-await/releases/download/release%2fv0.2.7/linkerd-await-v0.2.7-amd64 -O ./linkerd-await && chmod +x ./linkerd-await
   COPY scripts/flink/linkerd-entrypoint.sh ./linkerd-entrypoint.sh
   ```
   ```yaml
   spec:
     containers:
       - name: flink-jobmanager
         command:
          - linkerd-entrypoint.sh
   ```

2. The annotation `config.linkerd.io/proxy-await: enabled` is redundant if you are manually running `linkerd-await --shutdown -- ""$@""`, and can be safely removed."
"### **Key Points:**
1. Create Jenkins secrets for kubeconfig files for different Kubernetes clusters and load the appropriate kubeconfig based on the Git branch. 
   ```groovy
   withcredentials([file(credentialsid: 'kubeconfig-dev', variable: 'config')]) {
       sh """"""
       export kubeconfig=\${config}
       helm upgrade --install -f values.dev.yaml --namespace ${namespace}
       """"""
   }
   ```
2. Use conditional statements to set the namespace and kubeconfig for different environments (dev, stage, prod) based on the Git branch.
   ```groovy
   if (env.git_branch == ""origin/master"") {
       def namespace=""dev""
       // Load dev kubeconfig
   } else if (env.git_branch ==""origin/test"") {
       def namespace=""stage""
       // Load stage kubeconfig
   } else {
       def namespace=""prod""
       // Load prod kubeconfig
   }
   ```"
"### **Key Points:**
1. Locate the resource name in the Azure portal."
"### **Key Points:**
1. Use the list form for commands in Kubernetes to avoid shell features and explicitly list each word: 
   ```yaml
   command: 
     - the_command
     - --an-argument
     - --another
     - value
   ```
2. If shell features are needed, explicitly invoke a shell with the command: 
   ```yaml
   command:
     - sh
     - -c
     - the_command --an-argument --another value
   ```"
"### **Key Points:**
1. Use a liveness probe to trigger pod restarts instead of a readiness probe. Configure it as follows:
   ```yaml
   livenessProbe:
     exec:
       command:
       - /opt/fissile/readiness-probe.sh
     initialDelaySeconds: 20
     periodSeconds: 10
     failureThreshold: 3
   ```
2. Adjust the `failureThreshold` setting to control how many failures are allowed before the pod is restarted, preventing immediate restarts after the first failure."
"### **Key Points:**
1. To change the image of a deployment without downtime, use the `set-image` command:  
   ```kubectl set image deployment deployment_name deployment_name=image_name --record -n namespace```.
   
2. To specify the cluster and user while changing the image, use:  
   ```kubectl set image deployment deployment_name deployment_name=image_name_ecr -n namespace --cluster eks_cluster_nprod --user eks_cluster --record```.
   
3. To rollback to the previous image, use the rollback command:  
   ```kubectl rollout undo deployment.v1.apps/nginx-dep```."
"### **Key Points:**
1. Use a DaemonSet to ensure one replica of the Ingress controller's pod is running on each node. 
   ```yaml
   # DaemonSet configuration not provided in the text, but ensure to use DaemonSet for Ingress controller.
   ```
2. Set the correct load balancer IP for the Ingress controller's service to fix errors in the Ingress description.
   ```yaml
   # Load balancer IP configuration not provided in the text, ensure to set it correctly.
   ```
3. Configure the external traffic policy to ""local"" to route traffic to local endpoints only, preserving the client source IP.
   ```yaml
   apiVersion: v1
   kind: service
   metadata:
     name: example-service
   spec:
     selector:
       app: example
     ports:
       - port: 8765
         targetPort: 9376
     externalTrafficPolicy: Local
     type: LoadBalancer
   ```
4. Ensure the service name of the Ingress backend also uses the external traffic policy set to ""local"".
   ```yaml
   # Ingress backend service configuration not provided in the text, ensure to set externalTrafficPolicy: Local.
   ```"
"### **Key Points:**
1. Create a custom script using the `kubectl get clusterrole` command to list default user groups in a Kubernetes cluster.
2. Install the Rakkess plugin to get a complete overview of access rights for the current user and all server resources, using the command `rakkess`."
"### **Key Points:**
1. Correct the Ingress manifest by ensuring each rule has a defined host and paths. Example configuration:
   ```yaml
   rules:
   - host: example.com
     http:
       paths:
   ```
2. For local development, edit the `/etc/hosts` file to map the application IP to the desired hostname. Example entry:
   ```plaintext
   192.168.49.2    example.com
   ```
3. Use the `curl --resolve` option to test the Ingress with a specific hostname. Example command:
   ```bash
   curl --resolve example.com:443:192.168.49.2 -kv https://example.com
   ```
4. Deploy an Ingress resource with a custom SSL certificate and ensure the secret is in the same namespace as the application. Example configuration:
   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: Ingress
   metadata:
     name: whoami
   spec:
     tls:
       - secretName: myssl
         hosts:
           - example.com
     rules:
       - host: example.com
         http:
           paths:
             - path: /
               pathType: Prefix
               backend:
                 service:
                   name: whoami
                   port:
                     name: http
   ```"
"### **Key Points:**
1. Deploy two headless services using the Helm chart stable/mongodb-replicaset: `<release name>-mongodb-replicaset` and `<release name>-mongodb-replicaset-client`.
2. Use the connection string to connect to the replicaset: `""mongodb+srv://<release name>-mongodb-replicaset.namespace.svc.cluster.local/?tls=false&ssl=false""` (set TLS and SSL to false for testing)."
"### **Key Points:**
1. To expose your cluster using a LoadBalancer service type, ensure that your ingress controller creates a LoadBalancer service, which will have an external IP in a public cloud environment. In a bare metal setup, the external IP will remain in a `<pending>` state until configured with MetalLB.
   ```bash
   kubectl get svc istio-ingressgateway -n istio-system
   ```
2. Use MetalLB to assign an external IP to your LoadBalancer service, allowing access via `http://<ip>` instead of `http://<node-ip>:<node-port>`. MetalLB can also announce the IP via BGP for traffic routing.
3. To access your application running in a pod, kube-proxy manages the routing through iptables rules. You can inspect these rules by running:
   ```bash
   iptables-save | less
   ```
4. For DNS resolution, consider using ExternalDNS to create DNS records, enabling access to your service via a domain name like `http://example.com` instead of just an IP address."
"### **Key Points:**
1. Apply a configuration to a resource by filename using the command: ```kubectl apply -f [.yaml file] --force```. Ensure the resource is initially created with either 'apply' or 'create --save-config'.
2. Force replace a resource by deleting and re-creating it with the command: ```kubectl replace -f grav-deployment.yml```. Use this command only when `grace-period=0` to avoid graceful deletion."
"### **Key Points:**
1. Use the annotation field `kubectl.kubernetes.io/last-applied-configuration` to access the resource's initial applied configuration.  
   Code snippet: `kubectl get daemonset mydaemonset -o yaml | yq r - 'metadata.annotations.""kubectl.kubernetes.io/last-applied-configuration""'`"
"### **Key Points:**
1. To resolve DNS lookup failures in Kubernetes, install the Node Local DNS Cache add-on, which improves DNS performance by running a caching agent on cluster nodes as a DaemonSet.
2. The local caching agent queries the kube-dns service for cache misses, reducing latency and avoiding connection tracking issues associated with iptables DNAT rules."
"### **Key Points:**
1. Adjust the `horizontal-pod-autoscaler-sync-period` to change how often the autoscaler checks resource status, with a default of 30 seconds. This can be modified via the controller-manager flag.
2. The `upscaleforbiddenwindow` defines the minimum time between scale-up actions, set to a default of 3 minutes and currently not adjustable."
"### **Key Points:**
1. Delete the old Kubernetes configuration from the user's home directory: ```rm -rf $HOME/.kube/```.
2. Re-initialize the Kubernetes configuration after logging in with Azure: ```az login```."
"### **Key Points:**
1. Use `kubectl port-forward` to forward traffic from localhost to a specific pod for direct access. 
   ```bash
   kubectl port-forward <pod-name> <local-port>:<pod-port>
   ```
2. Set the readiness probe to ""ok"" after accessing the pod directly, and configure the liveness probe to trigger after more attempts if the host is not ready for too long. 
   ```yaml
   readinessProbe:
     exec:
       command: [""<command-to-check-readiness>""]
     initialDelaySeconds: <delay>
     periodSeconds: <period>

   livenessProbe:
     exec:
       command: [""<command-to-check-liveness>""]
     initialDelaySeconds: <delay>
     periodSeconds: <period>
   ```"
"### **Key Points:**
1. Use `helmfile apply` to check for changes before upgrading the Helm chart, preventing unnecessary chart revision increments. 
   ```bash
   helmfile apply
   ```
2. Avoid using `helmfile sync` directly in a frequent job to prevent incrementing the chart revision every time it runs. 
   ```bash
   helm upgrade --install httpd bitnami/apache > /dev/null
   ```"
"### **Key Points:**
1. For Kubernetes 1.4, replace `pod.alpha.kubernetes.io/init-containers` with `pod.beta.kubernetes.io/init-containers` to ensure proper deployment updates.  
   Code snippet: 
   ```yaml
   pod.beta.kubernetes.io/init-containers: '[
       {
           ""name"": ""install"",
           ""image"": ""busybox"",
           ""command"": [""/bin/sh"", ""-c"", ""echo foo > /work-dir/index.html""],
           ""volumemounts"": [
             {
               ""name"": ""workdir"",
               ""mountpath"": ""/work-dir""
               }
           ]
       }
   ]'
   ```

2. Test the change by modifying the content from ""foo"" to ""bar"" in the deployment YAML and applying it to see the updated output.  
   Code snippet: 
   ```bash
   $ cat nginx.yaml | sed -e 's/foo/bar/g' | kubectl apply -f -
   ```"
"### **Key Points:**
1. Ensure all worker pods have the same environment variables configured, including AWS connection details and logging settings. Relevant code snippet:
   ```yaml
   airflow__kubernetes_environment_variables__airflow__core__remote_logging: true
   airflow__kubernetes_environment_variables__airflow__core__remote_log_conn_id: my_aws
   airflow__kubernetes_environment_variables__airflow__core__remote_base_log_folder: s3://airflow.logs
   airflow__kubernetes_environment_variables__airflow__core__encrypt_s3_logs: false
   ```
   
2. Set the fernet key for the workers to avoid invalid token errors. Relevant code snippet:
   ```yaml
   airflow__kubernetes_environment_variables__airflow__core__fernet_key: ""abcdefghijkl1234567890zxcvbnmasdfghyrewsdsddfd=""
   ```"
"### **Key Points:**
1. Open port 8443 in Google Cloud firewall rules by editing the existing rules that include tcp:80,443 and adding 8443.
2. Create an Ingress resource with specific annotations and paths for routing, using the following configuration:
   ```yaml
   apiVersion: networking.k8s.io/v1beta1
   kind: ingress
   metadata:
     name: ingress-resource
     annotations:
       kubernetes.io/ingress.class: ""nginx""
       nginx.ingress.kubernetes.io/use-regex: ""true""
       nginx.ingress.kubernetes.io/ssl-redirect: ""false""
       nginx.ingress.kubernetes.io/rewrite-target: /$1
   spec:
     rules:
       - http:
           paths:
             - path: /?(.*)
               backend:
                 serviceName: client-cluster-ip
                 servicePort: 3000
             - path: /api/?(.*)
               backend:
                 serviceName: server-cluster-ip
                 servicePort: 5000
   ```"
"### **Key Points:**
1. Use the `kubernetes_namespace` resource to get the name property for the `kubernetes_secret` resource, ensuring the namespace is dynamically assigned. Code snippet: 
   ```hcl
   namespace = kubernetes_namespace.namespace.metadata[0].name
   ```
2. Prefer using the `_v1` version of resources for better compatibility and support. Code snippet reference: 
   ```hcl
   resource ""kubernetes_secret_v1"" ""api-env"" { ... }
   ```"
"### **Key Points:**
1. Use pre-upgrade hooks in Helm to delete existing jobs before applying an upgrade to ensure no conflicts with running jobs. 
   ```yaml
   hooks:
     pre-upgrade:
       - exec:
           command: [""kubectl"", ""delete"", ""job"", ""--all"", ""-n"", ""your-namespace""]
   ```
2. To trace pods and identify their controlling jobs, use the command `kubectl describe {pod}` and check the ""controlled by"" field to see which job is managing the pods.
   ```bash
   kubectl describe pod {pod-name}
   ```
3. If multiple pods are spawned by a job, it indicates that the job considered the first pod as failed. Check the job's ""controlled by"" field to ensure it is not being triggered by another job or cronjob.
   ```bash
   kubectl describe job {job-name}
   ```
4. Monitor the nodes in your cluster for any terminations that may affect job execution, especially if using hosted solutions like AKS, EKS, or GKE.
   ```bash
   watch kubectl get nodes
   ```"
"### **Key Points:**
1. Serialize values to JSON in Helm templates using `toprettyjson` for ConfigMap content: 
   ```yaml
   content: ""{{ .values.content1 | toprettyjson }}""
   ```
2. Directly define known ConfigMap keys in the template to avoid unnecessary complexity and remove the need for `tpl`:
   ```yaml
   data:
     file1.json: |
       {{ .values.content1 | toprettyjson | indent 4 }}
   ```
3. Embed core JSON structure directly in the template for clarity, using `tojson` for specific configuration values:
   ```yaml
   ""field2"": {{ .values.frobnicationlevel | tojson }}
   ```"
"### **Key Points:**
1. Check logs for the metrics server to identify issues with resource allocation and configuration: 
   ```bash
   kubectl logs metrics-server-v0.3.6-547dc87f5f-jrnjt -c metrics-server-nanny -n kube-system
   ```
2. If issues persist and no deployments are present, consider reporting the problem to GCP's issue tracker due to potential system-level logging and monitoring plugin issues in GKE."
"### **Key Points:**
1. Use the `tpl` function in Helm to expand a string as a Go template within a ConfigMap. Code snippet: 
   ```yaml
   data:
   {{ tpl (.files.glob ""foo/*"").asconfig . | indent 2 }}
   ```
2. Invoke arbitrary template code in your configuration, such as generating a service name based on the chart name. Code snippet:
   ```yaml
   foo:
     service:
       name: {{ template ""mychart.name"" . }}-service
   ```"
"### **Key Points:**
1. Use `readwriteonce` access mode for PVCs to ensure each pod gets its own volume: 
   ```yaml
   accessmodes:
     - readwriteonce
   ```
2. For shared volumes across replicas, utilize `readwritemany` access mode with NFS:
   ```yaml
   accessmodes:
     - readwritemany
   ```
3. Consider using MinIO, GlusterFS, or managed services like GCP Filestore to create NFS for your pods."
"### **Key Points:**
1. Change the service type to load balancer in `values.yaml` for Eureka server availability:  
   ```yaml
   service:
     type: loadbalancer
     port: 8761
   ```
2. Ensure the Eureka server is accessible via NodePort by configuring `service.yaml` appropriately, which will make it available at:  
   ```http
   http://....:30066
   ```"
"### **Key Points:**
1. Set an environment variable to expose the node name using the Downward API:  
   ```yaml
   env:
   - name: my_node_name
     valueFrom:
       fieldRef:
         fieldPath: spec.nodeName
   ```"
"### **Key Points:**
1. Use the Azure CLI to obtain AKS credentials by running the command: ```az aks get-credentials```."
"### **Key Points:**
1. Use Helm to upgrade charts with the command: ```gcr.io/$project_id/cloud-builders-helm upgrade --install filebeat --namespace filebeat stable/filebeat```.
2. Implement the `--atomic` flag during Helm upgrades to enable automatic rollback on failure: ```helm upgrade --install --atomic```.
3. Utilize environment variables for dynamic GCR image names in Cloud Build: ```gcr.io/$project_id/$repo_name/$branch_name:$short_sha```.
4. Add GKE cluster details to Cloud Build with environment variables: 
   ```yaml
   env:
   - 'cloudsdk_compute_zone=${_cloudsdk_compute_zone}'
   - 'cloudsdk_container_cluster=${_cloudsdk_container_cluster}'
   ```
5. Update deployment YAML files dynamically using `sed`: ```sed -i ""s,test_image_name,gcr.io/$project_id/$repo_name/$branch_name:$short_sha,"" deployment.yaml```."
"### **Key Points:**
1. Use the `--accept-hosts` option for hostname access control in `kubectl proxy`, ensuring it does not start with a slash. The command is:  
   ```bash
   kubectl proxy --address 0.0.0.0 --accept-hosts '.*'
   ```  
   (Remember to shell escape the `.*` to avoid matching files in the current directory.)"
"### **Key Points:**
1. Use the internal IP of your node as the public IP in your service definition to ensure proper routing. Update your `service.yaml` file accordingly. 
   ```yaml
   publicIPs:
     - 10.240.121.42
   ```"
"### **Key Points:**
1. Add egress rules for DNS traffic on port 53 to ensure proper functionality. The code snippet is:
   ```yaml
   egress:
     - ports:
       - port: 53
         protocol: udp
       - port: 53
         protocol: tcp
   ```"
"### **Key Points:**
1. To create a deployment in YAML format, use the command with `-o yaml --dry-run` to generate the YAML definition. Example command: 
   ```bash
   kubectl create deployment first-k8s-deploy --image=""laxman/nodejs/express-app"" -o yaml --dry-run
   ```
2. Add the `imagePullPolicy` property to the container definition in the YAML file to control image pulling behavior. Example configuration:
   ```yaml
   spec:
     containers:
     - image: laxman/nodejs/express-app
       name: express-app
       resources: {}
       imagePullPolicy: Never
   ```
3. Deploy the created YAML file using the command:
   ```bash
   kubectl apply -f <filename>
   ```"
"### **Key Points:**
1. Use environment variables in Fluent Bit configuration to set the S3 bucket name, e.g., ```bucket ${s3_bucket_name}```.
2. Define the environment variable in your Helm values file (e.g., values-prod.yml) as follows: 
   ```yaml
   env:
   - name: s3_bucket_name
     value: ""bucket/prefix/random123/test""
   ```
3. In ArgoCD, either update the values-prod.yml in your Git repository or directly modify the values in the ArgoCD application definition to set the environment variable:
   ```yaml
   values: |
     env:
     - name: s3_bucket_name
       value: ""bucket/prefix/random123/test""
   ```"
"### **Key Points:**
1. Allocate sufficient CPU and memory to pods, with a minimum of 100m CPU and 200Mi memory, depending on the application and number of replicas.
2. Avoid deploying 100 pods per node to prevent resource exhaustion.
3. Ensure that production clusters consist of multiple nodes to enhance reliability and performance."
"### **Key Points:**
1. Correct the service name and port in `ingress.yaml` to ensure proper routing: 
   ```yaml
   name: aks-helloworld-one  
   port:
     number: 8080
   ```
2. Use the command to confirm if the ingress has any endpoints:
   ```bash
   kubectl describe ingress hello-world-ingress -n ingress-basic
   ```"
"### **Key Points:**
1. Remove all unused Docker networks using the command: ```$ docker network prune```.
2. Set the bridge IP option in the Docker daemon configuration by adding `""bip"": ""xxx.yyy.zzz.vvv/ww""` to the `daemon.json` file.
3. Restart Docker to apply the changes with the command: ```$ docker restart```."
"### **Key Points:**
1. To ensure pods are evenly distributed across nodes, create your service object before the deployment to leverage the `selectorspreadpriority`. 
2. Add inter-pod anti-affinity constraints to your pods to enforce spreading across nodes.
3. Specify resource requests and limits for your containers to activate the `leastrequestedpriority`, which aids in pod distribution."
"### **Key Points:**
1. Use the correct syntax for creating a ConfigMap object: ```kubectl create configmap name [--from-file=[key=]source]```.
2. Example of creating a ConfigMap with a specific file: ```kubectl create configmap mongodb-config-file --from-file=conf=mongodb.cfg```."
"### **Key Points:**
1. Identify the process using port 80 by running the command: ```netstat -ano | findstr "":80"" | findstr ""listening""```.
2. Locate the process in Task Manager by sorting by PID and checking the details; if itâ€™s the ""World Wide Web Publishing Service"", stop it.
3. After resolving the port conflict, reapply your service with: ```kubectl apply -f ingress-srv.yaml```."
"### **Key Points:**
1. Create a new node group with the autoscaler option set to true and relevant IAM policies.  
   ```yaml
   nodegroup:
     iam:
       withaddonpolicies:
         autoscaler: true
   ```
2. Install the autoscaler after creating the new node group, as existing node groups cannot be edited.  
   (Refer to: https://eksctl.io/usage/managing-nodegroups/#nodegroup-immutability)"
"### **Key Points:**
1. Modify the CloudFormation template to specify a spot price in the launch configuration. Code snippet: 
   ```yaml
   spotprice: ""20""
   ```"
"### **Key Points:**
1. Use an init container to modify `config.json` during pod startup by adding the following configuration in `deployment.yaml`:
   ```yaml
   initcontainers:
     - name: init-myconfig
       image: busybox:1.28
       command: ['sh', '-c', 'cat /usr/share/nginx/html/config.json | sed -e ""s#\$authenticationendpoint#$authenticationendpoint#g"" > /tmp/config.json && cp /tmp/config.json /usr/share/nginx/html/config.json']
       env:
         - name: authenticationendpoint
           value: ""http://localhost:8080/security/auth""
   ```
   
2. Create a `/mnt/data.json` file on the host with the following content, which will be used by the init container:
   ```json
   {
     ""authenticationendpoint"": ""$authenticationendpoint"",
     ""authenticationclientid"": ""my-project"",
     ""baseapiurl"": ""http://localhost:8080/"",
     ""homeurl"": ""http://localhost:4300/""
   }
   ```"
"### **Key Points:**
1. Remove unused Docker data using the command: ```docker system prune```.
2. Clear Minikube's local state with: ```minikube delete```.
3. Start the Minikube cluster with the specified driver: ```minikube start --driver=<driver_name>``` (replace `<driver_name>` with `docker`).
4. Check the status of the Minikube cluster using: ```minikube status```."
"### **Key Points:**
1. Use the `kubectl describe node` command combined with `awk` to merge the `topology.kubernetes.io/zone` label with the names of pods scheduled on each node. Adjust the label key case as necessary.  
   Code snippet:  
   ```bash
   kubectl describe node | awk '/topology.kubernetes.io\/zone/{zone=$1;next} /^  namespace/{flag=1; getline; next} /^allocated resources:/{flag=0} flag{print  $2, zone}' | column -t
   ```
2. To filter based on namespace, you can modify the `awk` command to print the namespace along with the pod names and their zones.  
   Code snippet:  
   ```bash
   kubectl describe node | awk '/topology.kubernetes.io\/zone/{zone=$1;next} /^  namespace/{flag=1; getline; next} /^allocated resources:/{flag=0} flag{print $1, $2, zone}' | column -t
   ```"
"### **Key Points:**
1. Use a regex pattern that allows for partial matches to route correctly, specifically `.*v1.*`.  
   ```yaml
   match:
     - uri:
         regex: .*v1.*
   route:
     - destination:
         host: productpage
         port:
           number: 9080
   ```  
2. The previous regex pattern `v1` does not support partial matches, which caused routing issues.  
   ```yaml
   match:
     - uri:
         regex: v1
   route:
     - destination:
         host: productpage
         port:
           number: 9080
   ```"
"### **Key Points:**
1. Create a NodePort service to expose your application, which listens on a specific port on all Kubernetes nodes. 
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: jenkins
   spec:
     type: NodePort
     ports:
       - port: 8080
         targetPort: 8080
         nodePort: 30000
     selector:
       app: jenkins
   ```

2. Set up an OVH load balancer with a public IP and configure it to point to the NodePort of the service where your ingress is listening.
   ```yaml
   # Example configuration for OVH load balancer
   backend:
     service:
       name: jenkins
       port:
         number: 30000
   ```"
"### **Key Points:**
1. Load environment variables into a container using the `env` field, allowing access via `$demo_greeting`. 
   ```yaml
   env:
   - name: demo_greeting
     value: ""hello from the environment""
   - name: demo_farewell
     value: ""such a sweet sorrow""
   ```
2. Load secrets as environment variables using the `envfrom` field, which can be referenced as `$mysecret` inside the container.
   ```yaml
   envfrom:
   - secretref:
       name: mysecret
   ```"
"### **Key Points:**
1. Add the flag to the start command to set the API server port: ```--apiserver-port=6443```."
"### **Key Points:**
1. Use a single load balancer with an ingress controller to map multiple domains by adding CNAME records in DNS for both domains. 
   ```yaml
   spec:
     rules:
     - host: foobar.com
       http:
         paths:
         - backend:
             servicename: foobar
             serviceport: 80
     - host: api.foobar.com
       http:
         paths:
         - backend:
             servicename: foobar
             serviceport: 80
   ```

2. Alternatively, create two separate ingress resources for different domains, as shown in the following configurations:
   ```yaml
   # ingress-1.yaml
   apiVersion: extensions/v1beta1
   kind: Ingress
   metadata:
     name: kubernetes-dashboard
   spec:
     rules:
     - host: dashboard.test.domain.com
       http:
         paths:
         - path: /
           backend:
             servicename: frontend
             serviceport: 80
   ```

   ```yaml
   # ingress-2.yaml
   apiVersion: extensions/v1beta1
   kind: Ingress
   metadata:
     name: kubernetes-ingress-two
   spec:
     rules:
     - host: dashboard.domain.com
       http:
         paths:
         - path: /api
           backend:
             servicename: backend
             serviceport: 80
   ```"
"### **Key Points:**
1. Use jsonpath filtering to retrieve the `.status.succeeded` field of a job to get the number of succeeded pods. Code snippet: `kubectl get job <jobname> -o jsonpath={.status.succeeded}`."
"### **Key Points:**
1. Use a command to wait for a Persistent Volume Claim (PVC) to be in a ""bound"" state before proceeding. The command is:  
   ```bash
   while [[ $(kubectl get pvc myclaim -o 'jsonpath={..status.phase}') != ""bound"" ]]; do echo ""waiting for pvc status"" && sleep 1; done
   ```"
"### **Key Points:**
1. Configure the Ingress resource to handle SSL passthrough by using the appropriate annotations.  
   Code snippet: 
   ```yaml
   nginx.ingress.kubernetes.io/ssl-passthrough: ""true""
   ```
   
2. Set up a rewrite target in the Ingress resource to redirect traffic to the root path.  
   Code snippet: 
   ```yaml
   nginx.ingress.kubernetes.io/rewrite-target: /
   ```"
"### **Key Points:**
1. Use a PersistentVolumeClaim for MySQL on GKE without needing to create volumes or storage classes manually. Code snippet: 
   ```yaml
   kind: persistentvolumeclaim
   apiVersion: v1
   metadata:
     name: mysql-volumeclaim
   spec:
     accessModes:
       - ReadWriteOnce
     resources:
       requests:
         storage: 20Gi
   ```

2. Deploy MySQL using a service and deployment configuration, ensuring to set the root password and mount the persistent storage. Code snippet:
   ```yaml
   apiVersion: v1
   kind: service
   metadata:
     name: mysql
   spec:
     ports:
     - port: 3306
     selector:
       app: mysql
     clusterIP: none
   ---
   apiVersion: apps/v1
   kind: deployment
   metadata:
     name: mysql
   spec:
     selector:
       matchLabels:
         app: mysql
     strategy:
       type: recreate
     template:
       metadata:
         labels:
           app: mysql
       spec:
         containers:
         - image: mysql:5.6
           name: mysql
           env:
             - name: mysql_root_password
               value: password
           ports:
           - containerPort: 3306
             name: mysql
           volumeMounts:
           - name: mysql-persistent-storage
             mountPath: /var/lib/mysql
         volumes:
         - name: mysql-persistent-storage
           persistentVolumeClaim:
             claimName: mysql-volumeclaim
   ```"
"### **Key Points:**
1. To view logs associated with GCE instances, use the command to list logs: ```$ gcloud beta logging logs list projects/<project>/logs/<log_name>```.
2. To read specific logs for GCE instances, execute: ```$ gcloud beta logging read projects/<project>/logs/<log_name>``` and check for the resource type labeled as `gce_instance`."
"### **Key Points:**
1. Specify the exact port number in the targetPort field of your service configuration to resolve issues with GKE Ingress. Example configuration:
   ```yaml
   targetPort: 1088
   ```
2. Upgrade your GKE cluster to version 1.16.0-gke.20 or higher to potentially resolve related issues (not tested)."
"### **Key Points:**
1. Ensure the correct format for the attestor reference by including the project ID: ```projects/my-project/attestors/vulnz-attestor```.
2. Update the GCR image links to include the project ID: ```gcr.io/my-project/hello-app:e1479a4```.
3. Verify that the project ID variable (e.g., `$project_id`) is set correctly in your terminal session to avoid issues."
"### **Key Points:**
1. Define a service with a specified port and target port for the container, ```port: 8080``` and ```targetport: bananaport```."
"### **Key Points:**
1. Use a shell script to compute the total CPU limits of all pods across all namespaces, with the following code snippet:
   ```bash
   res=$(kubectl get pods -o=jsonpath='{.items[*]..resources.limits.cpu}' -a)
   let tot=0
   for i in $res
   do
      if [[ $i =~ &quot;m&quot; ]]; then
         i=$(echo $i | sed 's/[^0-9]*//g')
         tot=$(( tot + i ))
      else
         tot=$(( tot + i*1000 ))
      fi
   done
   echo $tot
   ```
2. Extend the shell script to also compute CPU requests and memory requests and limits values."
"### **Key Points:**
1. To fix the redirect issue, add a configuration snippet to rewrite requests from 'foo.com' to 'www.foo.com':  
   ```nginx.ingress.kubernetes.io/configuration-snippet: | if ($host = 'foo.com' ) { rewrite ^ https://www.foo.com$request_uri permanent; }```"
"### **Key Points:**
1. Add jpg files as binary data using the `binarydata` field:  
   ```yaml
   binarydata:
     {{ .files.get ""/path/to/file.jpg"" }}
   ```
2. Encode jpg files in the `binarydata` field with base64:  
   ```yaml
   {{ .files.get ""/path/to/file.jpg"" | b64enc }}
   ```
3. Ensure proper indentation when encoding jpg files:  
   ```yaml
   {{ .files.get ""/path/to/file.jpg"" | b64enc | nindent 4 }}
   ```"
"### **Key Points:**
1. Update the ingress values to place hosts under `.values.ingress.hosts` and paths directly under hosts, like this:
   ```yaml
   ingress:
     enabled: true
     hosts:
       - host: localhost
         paths:
           - ""/questdb""
           - ""/influxdb""
   ```
2. Modify the ingress template to use `networking.k8s.io/v1` format, ensuring rules are structured correctly:
   ```yaml
   rules:
   {{- range .values.ingress.hosts }}
   - host: {{ .host | quote }}
     http:
       paths:
         {{- range .paths }}
         - path: {{ .path }}
           backend:
             service:
               name: {{ .svc }}
               port:
                 number: {{ .port }}
         {{- end }}
   {{- end }}
   ```
3. Remove unnecessary declarations like `{{- $fullname := include ""questdb.fullname"" . -}}` and `{{- $svcport := .values.service.port -}}` as they are no longer needed."
"### **Key Points:**
1. **Install Ingress Controller:** Ensure the ingress controller is installed by applying the mandatory configuration.  
   ```bash
   $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml
   ```

2. **Create LoadBalancer Service:** Download the NodePort template and modify it to create an ingress-nginx service of type LoadBalancer with external IPs.  
   ```bash
   $ curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/baremetal/service-nodeport.yaml > svc-ingress-nginx-lb.yaml
   # Changes in svc-ingress-nginx-lb.yaml
   type: loadbalancer
   externalips:
     - 192.168.121.110
     - 192.168.121.111
     - 192.168.121.112
   externaltrafficpolicy: local
   $ kubectl apply -f svc-ingress-nginx-lb.yaml
   ```

3. **Verify Ingress Setup:** Check that the ingress-nginx service and deployment are created and running properly.  
   ```bash
   $ kubectl get svc -n ingress-nginx
   $ kubectl get deploy -n ingress-nginx
   $ kubectl get pods --all-namespaces -l app.kubernetes.io/name=ingress-nginx
   ```

4. **Test Ingress Functionality:** Execute a curl command to test that requests are being routed through the ingress load balancer.  
   ```bash
   $ curl myminikube.info
   ```"
"### **Key Points:**
1. Delete a job without cascading deletions using the command: ```kubectl delete job/jobname -n namespace --cascade=false```.
2. Delete any existing pod with the command: ```kubectl delete pod/podname -n namespace```."
"### **Key Points:**
1. Create a proxy Nginx server to expose the API service globally, allowing external access. 
   ```nginx
   location /<your_requested_url> {
       proxy_pass http://service_name:port;
   }
   ```"
"### **Key Points:**
1. Call your backend service directly using the URL format: ```http://login:5555/login``` if the frontend and backend are in the same Kubernetes namespace.
2. If the frontend and backend are in different namespaces, use the format: ```http://login.<namespace>.svc.cluster.local:5555/login``` to access the backend service.
3. To access the backend from outside the cluster, expose it using a LoadBalancer service or configure a Kubernetes Ingress with an Ingress controller."
"### **Key Points:**
1. Provision nodes with more CPU to meet the requirement of 2 CPU cores free. 
2. Reduce the CPU requests in the pod specification to fit the available resources."
"### **Key Points:**
1. Remove the `volumes:` block from your `docker-compose.yml` file to prevent local directory contents from overriding the container's filesystem."
"### **Key Points:**
1. Expose your backend service using Ingress for access, e.g., `https://backend.example.com/action`.
2. Update your frontend code to point to the new backend URL: `https://backend.example.com/action`."
"### **Key Points:**
1. Ensure lines with indentation start at the beginning of the line to avoid YAML parsing errors. Use the following code snippet for proper indentation:
   ```yaml
   data:
     {{ .values.env.datafile }}: |-
     {{ .files.get .values.env.datafile | indent 4}}
   ```

2. For a more aesthetic output, use `nindent` to include a newline before the first line's indentation and consume preceding whitespace. Example code:
   ```yaml
   data:
     {{ .values.env.datafile }}: |-
     {{- .files.get .values.env.datafile | nindent 4}}
   ```

3. When including labels and annotations, use `nindent` to maintain proper indentation:
   ```yaml
   metadata:
     labels: {{- include ""common.labels"" . | nindent 4 }}
     annotations: {{- include ""common.annotations"" . | nindent 4 }}
   ```"
"### **Key Points:**
1. Retrieve admin password for GKE from the Google Cloud Console: navigate to **Container Engine** and select **Show Credentials**.
2. Use the command `kubectl config view` to view your current Kubernetes configuration, but note that it may not display the admin password."
"### **Key Points:**
1. Bare pods are created by the `kubernetes-admin` user, who has access to all Pod Security Policies (PSPs), allowing their successful creation. 
2. Managed pods (created by deployments, replica sets, stateful sets, daemon sets) require their service accounts to have access to PSPs via a cluster role or role for successful creation.
3. To prevent users from creating privileged and dangerous pods, identify the user and group from the kubeconfig or certificate, and ensure they do not have access to PSPs through any cluster role or role."
"### **Key Points:**
1. Mount a volume for heap dumps by defining a volume mount and volume in your Kubernetes configuration.  
   ```yaml
   volumemounts:
       - name: heap-dumps
         mountpath: /dumps
     volumes:
     - name: heap-dumps
       emptydir: {}
   ```"
"### **Key Points:**
1. To serve all pages securely under `/ghost`, use a wildcard path in the Ingress configuration: 
   ```yaml
   path: /ghost/*
   ```
2. For secure and unsecured paths, create separate host rules in the Ingress, such as `ghost.example.com` for secured and `main.example.com` for unsecured:
   ```yaml
   host: ghost.example.com
   ```
3. Ensure to configure TLS settings in the Ingress with the appropriate annotations and secret names:
   ```yaml
   nginx.ingress.kubernetes.io/auth-tls-verify-client: ""on""
   secretName: mysite-tls
   ```
4. To verify the NGINX configuration, you can access the ingress controller pod and check the `nginx.conf`:
   ```bash
   kubectl exec -it nginx-ingress-controller-xxxxxxxxx-xxxxx bash
   cat nginx.conf
   ```"
"### **Key Points:**
1. The issue of the LoadBalancer service being incorrectly listed as external instead of internal has been reported and is under investigation by Google Cloud Platform (GCP) support.
2. Users are advised to follow the public issue tracker for updates regarding the resolution of this problem."
"### **Key Points:**
1. Set a numeric user value in the pod definition to resolve the error when `uid == nil`. Use the following configuration:  
   ```yaml
   securityContext:
     runAsUser: 1000
   ```
2. Configure the security context to include `fsgroup`, `runAsNonRoot`, and `runAsUser` for enhanced security:  
   ```yaml
   securityContext:
     fsGroup: 2000
     runAsNonRoot: true
     runAsUser: 1000
   ```"
"### **Key Points:**
1. Create two separate Ingress resources for different rewrite rules, ensuring that annotations are applied per Ingress. 
   ```yaml
   metadata:
     name: thesis-frontend
     namespace: thesis
     annotations:
       kubernetes.io/ingress.class: ""nginx""
       nginx.ingress.kubernetes.io/add-base-url: ""true""
       nginx.ingress.kubernetes.io/service-upstream: ""true""
   spec:
     tls:
       - hosts:
           - thesis
         secretname: ingress-tls
     rules:
       - host: thesis.info
         http:
           paths:
           - path: /
             pathtype: prefix
             backend:
               service:
                 name: frontend
                 port:
                   number: 3000
   ```

2. For the backend Ingress, use a rewrite annotation to transform the path `/backend/(.+)` to `/$1`.
   ```yaml
   metadata:
     name: thesis-backend
     namespace: thesis
     annotations:
       kubernetes.io/ingress.class: ""nginx""
       nginx.ingress.kubernetes.io/add-base-url: ""true""
       nginx.ingress.kubernetes.io/rewrite-target: /$1
       nginx.ingress.kubernetes.io/service-upstream: ""true""
   spec:
     tls:
       - hosts:
           - thesis
         secretname: ingress-tls
     rules:
       - host: thesis.info
         http:
           paths:
           - path: /backend/(.+)
             backend:
               service:
                 name: backend
                 port:
                   number: 5000
   ```"
"### **Key Points:**
1. To enable bash completion for kubectl, run the command: ```source /etc/bash_completion```.
2. Additionally, you can load kubectl completion with: ```source <(kubectl completion bash)```."
"### **Key Points:**
1. Expose the NGINX ingress controller to enable internal and external communication by running:
   ```bash
   kubectl expose deployment ingress-nginx-controller --target-port=80 --type=nodeport -n kube-system
   ```
2. Create an Ingress resource with the appropriate backend services and paths, ensuring to use the correct service names and ports:
   ```yaml
   apiVersion: networking.k8s.io/v1beta1
   kind: Ingress
   metadata:
     name: ingress-service
     annotations:
       kubernetes.io/ingress.class: nginx
       nginx.ingress.kubernetes.io/use-regex: ""true""
   spec:
     rules:
       - host: ticketing.dev
         http:
           paths:
             - path: /api/users/?(.*)
               backend:
                 serviceName: hello1-svc
                 servicePort: 8080
             - path: /?(.*)
               backend:
                 serviceName: hello2-svc
                 servicePort: 8080
   ```
3. Ensure to specify the host header when making requests to the ingress from within the cluster:
   ```bash
   curl -h ""host: ticketing.dev"" ingress-nginx-controller.kube-system.svc.cluster.local/?foo
   ```"
"### **Key Points:**
1. Set up Helm with appropriate authorization settings for your cluster, particularly in an RBAC-enabled environment. Refer to the Helm documentation for detailed instructions. 
   ```plaintext
   https://v2.helm.sh/docs/using_helm/#role-based-access-control
   ```
2. Create a dedicated service account for Tiller with the cluster-admin role to grant necessary permissions, especially if the cluster is solely for Weaviate.
   ```yaml
   apiVersion: v1
   kind: ServiceAccount
   metadata:
     name: tiller
     namespace: kube-system
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRoleBinding
   metadata:
     name: tiller-cluster-role-binding
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: cluster-admin
   subjects:
   - kind: ServiceAccount
     name: tiller
     namespace: kube-system
   ```"
"### **Key Points:**
1. When running Python code in a Kubernetes pod, use external broker DNS addresses instead of 'localhost:9092'. 
2. To access the Kafka service from outside the Kubernetes cluster, expose it using a ClusterIP, NodePort, or Ingress service.
3. Set `advertised.listeners` to the appropriate service address (e.g., `kafka.svc.cluster.local`) to ensure clients can connect correctly, and configure `bootstrap.servers` accordingly, e.g., `bootstrap.servers = kafka.svc.cluster.local:9092`."
"### **Key Points:**
1. Use a higher-level controller like a deployment instead of directly creating pods, as pods cannot be modified after creation. 
   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: example-deployment
   spec:
     replicas: 1
     template:
       spec:
         containers:
         - name: example-container
           image: example-image
           command: [""new-command""]
   ```

2. To change the command of a container in a deployment, update the deployment's pod template, which will create a new pod with the updated command and delete the old one.
   ```yaml
   spec:
     template:
       spec:
         containers:
         - name: example-container
           command: [""updated-command""]
   ```

3. For short-lived single-command containers, consider using a Job instead of a pod, as jobs are designed for one-time execution.
   ```yaml
   apiVersion: batch/v1
   kind: Job
   metadata:
     name: example-job
   spec:
     template:
       spec:
         containers:
         - name: example-container
           image: example-image
           command: [""job-command""]
         restartPolicy: Never
   ```"
"### **Key Points:**
1. The `-o | --output` flag is not a universal flag in kubectl and is not included in the default flags for version 1.18.
2. The `kubectl describe` command does not support the `--output` (or shorthand `-o`) flag."
"### **Key Points:**
1. Use the revision flag to retrieve details about a specific deployment revision:  
   ```kubectl rollout history deployment/<deployment-name> --revision=<revision-number>```
2. To find the deployment date of a specific revision, add the -o yaml flag to check the creation timestamp:  
   ```kubectl rollout history deployment/<deployment-name> --revision=<revision-number> -o yaml```"
"### **Key Points:**
1. To run a pod on a specific node, use a node selector in the pod specification. 
   ```yaml
   nodeSelector:
     disktype: ssd
   ```
2. For running a pod on every node in GKE before version 1.2, create a replication controller with replicas greater than the number of nodes and specify a host port in the container spec.
   ```yaml
   apiVersion: v1
   kind: ReplicationController
   metadata:
     name: my-rc
   spec:
     replicas: 3
     template:
       spec:
         containers:
         - name: my-container
           image: my-image
           ports:
           - containerPort: 80
             hostPort: 8080
   ```"
"### **Key Points:**
1. **Liveness Probe**: Use a liveness probe to ensure the container is running and not in a deadlock state. If the probe fails, Kubernetes will restart the pod. Example configuration:
   ```yaml
   livenessProbe:
     exec:
       command: [""cat"", ""/tmp/healthy""]
     initialDelaySeconds: 5
     periodSeconds: 10
     timeoutSeconds: 1
     failureThreshold: 3
   ```

2. **Readiness Probe**: Implement a readiness probe to check if the container can handle traffic. If it fails, Kubernetes will stop sending traffic to the pod until it passes. Example configuration:
   ```yaml
   readinessProbe:
     httpGet:
       path: /healthz
       port: 8080
     initialDelaySeconds: 5
     periodSeconds: 10
     timeoutSeconds: 1
     failureThreshold: 3
   ```

3. **Startup Probe**: Optionally use a startup probe for slow-starting containers to prevent premature checks. Example configuration:
   ```yaml
   startupProbe:
     tcpSocket:
       port: 8080
     initialDelaySeconds: 5
     periodSeconds: 10
     timeoutSeconds: 1
     failureThreshold: 3
   ``` 

4. **Probe Configuration Parameters**: Define parameters such as `initialDelaySeconds`, `periodSeconds`, `timeoutSeconds`, `successThreshold`, and `failureThreshold` to customize the behavior of the probes based on application needs. Example:
   ```yaml
   readinessProbe:
     exec:
       command: [""cat"", ""/tmp/healthy""]
     initialDelaySeconds: 10
     periodSeconds: 5
     timeoutSeconds: 2
     successThreshold: 1
     failureThreshold: 3
   ```"
"### **Key Points:**
1. Developed a Go application that monitors Ingress resources and dynamically adds rewrite rules to the Corefile used by CoreDNS. 
2. The tool can be made open-source if there is sufficient interest."
"### **Key Points:**
1. Ensure SSL is not terminated by the NGINX ingress by setting the backend protocol to HTTPS and enabling SSL passthrough:  
   ```nginx.ingress.kubernetes.io/backend-protocol: ""https""```  
   ```nginx.ingress.kubernetes.io/ssl-passthrough: ""true""```
2. Consider changing the port to 6443 for direct connections to the service."
"### **Key Points:**
1. Adjust application configuration to listen on all interfaces by setting it to `0.0.0.0`."
"### **Key Points:**
1. To create a deployment that manages pods, use the default behavior of `kubectl run`. 
2. To run a single pod without a deployment, add the `--restart=never` flag to the `kubectl run` command: ```kubectl run mypod --restart=never --image=imagex```."
"### **Key Points:**
1. To check DNS entries of a service, run `nslookup` from inside a pod instead of localhost. Use the following command to create a temporary pod for this purpose:
   ```bash
   kubectl run -it --rm --restart=never dnsutils2 --image=tutum/dnsutils --command -- bash
   ```
2. After entering the pod, execute the `nslookup` command to verify the service's DNS resolution:
   ```bash
   nslookup sfs-svc.default.svc.cluster.local
   ```"
"### **Key Points:**
1. Retrieve node information with kubelet version using JSONPath:  
   ```bash
   kubectl get node -o=jsonpath='{range.items[*]}{.metadata.selflink} {""\t""} {.status.nodeinfo.kubeletversion}{""\n""}{end}'
   ```
2. Sort nodes by selflink and kubelet version using custom columns:  
   ```bash
   kubectl get node -o=custom-columns=node:.metadata.selflink
   kubectl get node -o=custom-columns=version:.status.nodeinfo.kubeletversion
   kubectl get node -o=custom-columns=node:.metadata.selflink,version:.status.nodeinfo.kubeletversion
   ```"
"### **Key Points:**
1. Configure the nodeselector for the Alertmanager component:  
   ```yaml
   alertmanager:
      nodeselector:
         nodetype: infra
   ```
2. Configure the nodeselector for the Node Exporter component:  
   ```yaml
   nodeexporter:
      nodeselector:
         nodetype: infra
   ```
3. Configure the nodeselector for the Server component:  
   ```yaml
   server:
      nodeselector:
         nodetype: infra
   ```
4. Configure the nodeselector for the Pushgateway component:  
   ```yaml
   pushgateway:
      nodeselector:
         nodetype: infra
   ```"
"### **Key Points:**
1. Add DNS entries for the namespaces pointing to a single cluster IP for accessing specific applications. For example, use:
   ```
   stage.cloudapp.azure.com
   dev.cloudapp.azure.com
   ```
2. For deploying multiple applications, consider adding wildcard DNS entries:
   ```
   *.stage.cloudapp.azure.com
   *.dev.cloudapp.azure.com
   ```
3. Create an Ingress resource with the following configuration:
   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: Ingress
   metadata:
     name: ingress-example-with-hostname
     namespace: dev
   spec:
     rules:
     - host: dev.cloudapp.azure.com
       http:
         paths:
         - pathType: Prefix
           path: /
           backend:
             service:
               name: service
               port:
                 number: 80
   ```"
"### **Key Points:**
1. Create a custom NGINX template by modifying the `ssl_session_cache` line in the default template to change `builtin:1000` to `builtin:3000`, and save it as `nginx.tmpl`.  
   ```bash
   $ sed -i '343s/builtin:1000/builtin:3000/' nginx.tmpl
   ```

2. Create a ConfigMap from the modified `nginx.tmpl` file to mount it into the NGINX Ingress controller pod.  
   ```bash
   $ kubectl create cm nginx.tmpl --from-file=nginx.tmpl
   ```

3. Edit the NGINX Ingress deployment to add a volume and volume mount for the custom template.  
   ```yaml
   volumemounts:
   - mountpath: /etc/nginx/template
     name: nginx-template-volume
     readonly: true
   volumes:
   - name: nginx-template-volume
     configmap:
       name: nginx.tmpl
       items:
       - key: nginx.tmpl
         path: nginx.tmpl
   ```

4. Create another ConfigMap to set the `ssl-session-cache-size` parameter with the desired value, ensuring the namespace and name match your environment.  
   ```yaml
   kind: configmap
   apiversion: v1
   metadata:
     name: ingress-nginx-controller
     namespace: ingress-nginx
   data:
     ssl-session-cache-size: ""100m""
   ```

5. Verify that the changes have been applied correctly by checking the NGINX configuration for the updated `ssl_session_cache` values.  
   ```bash
   $ kubectl exec -n ingress-nginx $nginx_pod -- cat nginx.conf | grep -n ssl_session_cache
   ```"
"### **Key Points:**
1. Ensure your ingress controller has the node-selector `kubernetes.io/os=linux` by editing the relevant nodes or ingress configuration to match this label.  
   Code snippet: `kubectl label nodes <node-name> <label-key>=<label-value>`.
2. Verify the current labels on your nodes using the command:  
   Code snippet: `kubectl get nodes --show-labels`."
"### **Key Points:**
1. `kubectl port-forward` only forwards connections to a specific pod, not to services without pods behind them, such as ExternalName services.
2. To connect to an ExternalName service or perform other service-related tasks, initiate the connection from inside the cluster using a temporary pod, as shown in the code snippet: 
   ```bash
   kubectl run curl-test --rm --image=curlimages/curl --generator=run-pod/v1 -- \
   http://my-service.dev.svc.cluster.local
   ```"
"### **Key Points:**
1. Use the `-k` (or `--kustomize`) flag with `kubectl apply` for applying configurations with Kustomize. Code snippet: `kubectl apply -k <my-folder-or-file>`."
"### **Key Points:**
1. Configure the Ingress service to include original IPs by setting `externalTrafficPolicy` to `local` in the service configuration.  
   ```yaml
   spec: 
     externalTrafficPolicy: local
   ```

2. Update the ConfigMap associated with the Ingress controller to enable full forwarded headers by adding the following data:  
   ```yaml
   data:
     compute-full-forwarded-for: ""true""
     use-forwarded-headers: ""true""
   ```

3. Restart the Ingress controller pods to apply the changes, which will allow HTTP requests to include `x-forwarded-for` and `x-real-ip` headers."
"### **Key Points:**
1. **Stick to Kubernetes 1.15**: Not recommended as it is significantly outdated.
2. **Manually Update API Version**: Clone your repository and change `apiVersion` to `apps/v1` in all resources.
3. **Use kubectl convert**: Utilize the command `kubectl convert -f deployment.yaml --output-version apps/v1` to automatically change the API version."
"### **Key Points:**
1. Use a kube API client to watch a resource group for events and continue when criteria are met.
2. Implement a loop to handle waiting for a non-existent resource with a timeout, using the following code snippet:
   ```bash
   timeout=$(( $(date +%s) + 60 )) 
   while ! kubectl get job whatever 2>/dev/null; do
     [ $(date +%s) -gt $timeout ] && exit 1
     sleep 5
   done
   ```"
"### **Key Points:**
1. Change the volume type in your Postgres specs from `readwritemany` to `readwriteonce` to ensure compatibility with Google Cloud, then redeploy the database. 
   ```yaml
   volumeType: readwriteonce
   ```
2. To retrieve application logs in a cluster, use the command: 
   ```bash
   kubectl logs $podname
   ```"
"### **Key Points:**
1. To restart a StatefulSet using a patch request, you can use the following curl command to update the annotations:  
   ```bash
   curl -k --data '{""spec"":{""template"":{""metadata"":{""annotations"":{""kubectl.kubernetes.io/restartedat"":""'""$(date +%y-%m-%dt%t%z)""'""}}}}}' -X PATCH -h ""accept: application/json, */*"" -h ""content-type: application/strategic-merge-patch+json"" localhost:8080/apis/apps/v1/namespaces/default/statefulsets/my-statefulset
   ```"
"### **Key Points:**
1. Update the kubeconfig file by copying the admin configuration and setting the correct ownership: 
   ```bash
   mkdir -p $HOME/.kube
   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
   sudo chown $(id -u):$(id -g) $HOME/.kube/config  
   ```"
"### **Key Points:**
1. Set the container to run as root by adding `runasuser: 0` in the security context.  
   ```yaml
   securitycontext:
     privileged: true
     runasuser: 0
   ```"
"### **Key Points:**
1. Define a template that expands to the string `{{` for variable expansion without needing to escape `}}`: 
   ```yaml
   {{- define ""cc"" }}{{ printf ""{{"" }}{{ end -}}
   - name: syslog_hostname
     value: '{{cc}} index .container.config.labels ""io.kubernetes.container.name"" }}'
   ```
2. Create an external file for environment variable fragments to avoid excessive escaping, and include it in your configuration:
   ```yaml
   # i am files/syslog_vars.yaml
   - name: syslog_hostname
     value: '{{ index .container.config.labels ""io.kubernetes.container.name"" }}'
   
   env:
   {{ .files.get ""files/syslog_vars.yaml"" | indent 2 }}
   ```"
"### **Key Points:**
1. Deleting a Persistent Volume (PV) does not automatically create a new one; you must manually create a new PV if needed.
2. When done with a Persistent Volume Claim (PVC), you can delete the PVC, and the action taken on the PV depends on its reclaim policy, which can be set to delete, retain, or recycle."
"### **Key Points:**
1. To modify kube-proxy configuration, edit the kube-proxy daemonset in the kube-system namespace: ```$ kubectl -n kube-system edit daemonset kube-proxy```.
2. Alternatively, you can edit the kube-proxy configmap in the kube-system namespace: ```$ kubectl -n kube-system edit configmap kube-proxy```."
"### **Key Points:**
1. Use a placeholder in the values file for environment variables, e.g., ```database_url=${database_url}``` in `values-env.yaml`.
2. In the CI/CD script, evaluate and create a new values file with resolved variables using: ```eval ""echo \""$(cat values-env.yaml)\"""" >> values-ci.yaml``` followed by the Helm upgrade command: ```helm upgrade chart_name helm --install --values values-ci.yaml```."
"### **Key Points:**
1. Configure CoreDNS with a custom host and forward DNS queries to external servers, using the following configuration:
   ```
   .:53 {
       errors
       health
       ready
       kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
       }
       prometheus :9153
       hosts custom.hosts myapi.local {
           192.168.49.2 myapi.local
           fallthrough
       }
       forward . 8.8.8.8 8.8.4.4
       cache 30
       loop
       reload
       loadbalance
   }
   ```
2. Alternatively, use the hosts plugin for CoreDNS to manage custom hosts, as referenced in the documentation: 
   ```
   https://coredns.io/plugins/hosts/
   ```"
"### **Key Points:**
1. Create a deployment for the application with the specified image and configuration:
   ```yaml
   apiVersion: apps/v1
   kind: deployment
   metadata:
     name: bonsai-onboarding
   spec:
     selector:
       matchLabels:
         app: bonsai-onboarding
     replicas: 2
     template:
       metadata:
         labels:
           app: bonsai-onboarding
       spec:
         containers:
         - name: bonsai-onboarding
           image: nginx:latest
           ports:
           - containerPort: 80
   ```

2. Set up a LoadBalancer service to expose the application:
   ```yaml
   apiVersion: v1
   kind: service
   metadata:
     name: lb-onboarding
   spec:
     type: LoadBalancer
     selector:
       app: bonsai-onboarding
     ports:
     - protocol: TCP
       port: 3000
       targetPort: 80
   ```

3. Verify the deployment and service status using:
   ```bash
   kubectl get pods,svc
   ```

4. Check if the application is reachable using:
   ```bash
   nmap -pn $(kubectl get svc lb-onboarding -o jsonpath='{.status.loadBalancer.ingress[*].ip}')
   ```"
"### **Key Points:**
1. Delete the Persistent Volume Claim (PVC) and re-authenticate to resolve the issue.
2. Use the command to switch to the desired database: ```use mydatabase```.
3. Authenticate with the database using: ```db.auth('user1','password1')```."
"### **Key Points:**
1. Connect to a NATS server using a specified URL instead of the default localhost address. Code snippet: 
   ```go
   nc, err := nats.connect(""nats://nats:4222"")
   ```
2. Establish a connection to the NATS Streaming server using the NATS connection. Code snippet: 
   ```go
   sc, err := stan.connect(""stan"", ""me"", stan.natsconn(nc))
   ```
3. Publish a message to a subject after establishing the connection. Code snippet: 
   ```go
   if err := sc.publish(""test-subject"", []byte(""this is a test-message!"")); err != nil {
       panic(err)
   }
   ```"
"### **Key Points:**
1. Redirect requests from `www.foo.bar` by specifying the hostname in the Ingress configuration, ensuring the wildcard host is the last entry. 
   ```yaml
   nginx.ingress.kubernetes.io/server-alias: www.foo.bar
   ```
2. Use regex for path matching and rewrite the target in the Ingress rules to handle requests appropriately.
   ```yaml
   nginx.ingress.kubernetes.io/rewrite-target: /$1
   ```"
"### **Key Points:**
1. Add the Amazon EBS CSI driver policy to the AWS IAM role attached to all AWS EKS nodes to resolve PVC attachment issues. 
   ```json
   {
     ""Version"": ""2012-10-17"",
     ""Statement"": [
       {
         ""Effect"": ""Allow"",
         ""Action"": ""ebs:AttachVolume"",
         ""Resource"": ""*""
       }
     ]
   }
   ```
2. Install the AWS EBS CSI driver add-on in the AWS EKS cluster to ensure proper functionality of persistent volume claims.
   ```bash
   eksctl create addon --name aws-ebs-csi-driver --cluster <cluster-name> --service-account-role-arn <role-arn>
   ```"
"### **Key Points:**
1. Modify the StatefulSet YAML structure to include necessary metadata, labels, and annotations. Example:
   ```yaml
   apiVersion: apps/v1beta1
   kind: statefulset
   metadata:
     name: ""{{.values.primaryname}}""
     labels:
       name: ""{{.values.primaryname}}""
       app: ""{{.values.primaryname}}""
       chart: ""{{.chart.name}}-{{.chart.version}}""
     annotations:
       ""helm.sh/created"": {{.release.time.seconds | quote }}
   ```

2. Ensure the spec section includes a selector that matches the template labels and define the service name and replicas. Example:
   ```yaml
   spec:
     selector:
       matchLabels:
         app: """" # has to match .spec.template.metadata.labels
     serviceName: """" # put your servicename here
     replicas: 1 # by default is 1
   ```

3. Configure the container specifications, including image, ports, resources, and environment variables. Example:
   ```yaml
   containers:
   - name: {{.values.containername}}
     image: ""{{.values.postgresimage}}""
     ports: 
     - containerPort: 5432
       protocol: tcp
       name: postgres
     resources:
       requests:  
         cpu: {{default ""100m"" .values.cpu}}
         memory: {{default ""100m"" .values.memory}}
     env:
     - name: pghost
       value: /tmp
     - name: pg_primary_user
       value: primaryuser
     - name: pg_primary_password
       value: ""{{.values.primarypassword}}""
   ```

4. Define volume mounts and persistent volume claims for data storage. Example:
   ```yaml
   volumeMounts:
   - name: pgdata
     mountPath: ""/pgdata""
     readOnly: false
   volumes:
   - name: pgdata
     persistentVolumeClaim:
       claimName: {{.values.pvcname}}
   ```"
"### **Key Points:**
1. Configure Weave with `no_masq_local=1` to ensure it respects the `externalTrafficPolicy` property.  
   Code snippet: `no_masq_local=1`"
"### **Key Points:**
1. Use the production Let's Encrypt API endpoint in your issuer specification to avoid certificate warnings: ```https://acme-v02.api.letsencrypt.org/directory```.
2. Ensure that the certificate status is true, indicating successful provisioning by cert-manager."
"### **Key Points:**
1. Install Superset using Helm by adding the repository and running the install command: 
   ```bash
   helm repo add cloudposse-incubator https://charts.cloudposse.com/incubator/
   helm install --name my-release stable/superset
   ```
2. Ensure you are running commands from within the Helm directory, specifically the `stable/superset` directory, or use the relative path when installing:
   ```bash
   cd stable/superset
   helm install stable/superset
   ```
   or 
   ```bash
   helm install superset ./stable/superset
   ```
3. Alternatively, download the Helm chart locally and install it from the extracted directory:
   ```bash
   helm fetch stable/superset --untar
   cd superset
   helm install .
   ```"
"### **Key Points:**
1. Grant the default service account from project A the role of Artifact Registry Reader in project B to enable OAuth and allow pulling Docker images from the registry."
"### **Key Points:**
1. Set `maxunavailable` to 0 in the rolling update strategy to ensure zero unavailable pods during deployment: 
   ```yaml
   strategy:
     type: rollingupdate
     rollingupdate:
       maxunavailable: 0
       maxsurge: 1
   ```
2. Configure a readiness probe to ensure the pod is ready to receive traffic before being used by Kubernetes: 
   ```yaml
   readinessprobe:
     httpget:
       path: /
       port: 80
     initialdelayseconds: 5
     periodseconds: 5
     successthreshold: 1
   ```"
"### **Key Points:**
1. Update the URI in `charts.yaml` or `requirements.yaml` to point to the new stable repository:  
   ```https://charts.helm.sh/stable```.
2. Update the URI in `charts.yaml` or `requirements.yaml` to point to the new incubator repository:  
   ```https://charts.helm.sh/incubator```."
"### **Key Points:**
1. Define a storage class for your EKS cluster to enable persistent storage, and ensure to set a default storage class for persistent volume claims.
2. After adding the storage class, delete and recreate your Prometheus deployment using the command: ```helm delete prometheus --purge``` to ensure all pods function correctly."
"### **Key Points:**
1. Upgrade your Kubernetes cluster to a more recent version (preferably v1.20) to utilize the `--control-plane-endpoint` flag. 
2. Alternatively, create a new cluster from scratch using a recent version of Kubernetes, referencing the `--kubernetes-version` flag in the kubeadm init documentation."
"### **Key Points:**
1. Use the `kubectl patch` command to update the node selector for a deployment, ensuring all pods restart on a specified node. Code snippet: `kubectl patch deployments nginx-deployment -p '{""spec"": {""template"": {""spec"": {""nodeselector"": {""kubernetes.io/hostname"": ""node-2""}}}}}'`."
"### **Key Points:**
1. Start Minikube to avoid errors related to the current context in the config file: ```minikube start```.
2. If Minikube is stopped, restart it to recreate the `.kube/config`: ```minikube stop``` followed by ```minikube start```.
3. For Windows, install Minikube and a hypervisor (e.g., VirtualBox), then run: ```minikube start --vm-driver=virtualbox```.
4. Optionally, add the `kubectl` directory to Windows environment variables for easier access."
"### **Key Points:**
1. Flush the iptables rules to clear any networking state issues. 
   ```bash
   iptables -F
   ```
2. Restart all relevant services, such as Docker, to ensure proper configuration.
   ```bash
   systemctl restart docker
   ```"
"### **Key Points:**
1. Set your IAM account as an admin for learning by selecting the existing policy ""AdministratorAccess"" and adding it to your user. 
   ```plaintext
   select users -> add permissions -> select AdministratorAccess -> next next next!
   ```
2. Create a custom policy with full permissions by using the following JSON and then adding it to your user.
   ```json
   {
       ""version"": ""2012-10-17"",
       ""statement"": [
           {
               ""effect"": ""allow"",
               ""action"": ""*"",
               ""resource"": ""*""
           }
       ]
   }
   ```
   Steps: 
   ```plaintext
   select ""policies"" -> create policy -> replace with your JSON -> next next -> select your user -> add permissions.
   ```"
"### **Key Points:**
1. To view logs for StatefulSets, use the command: ```kubectl get events``` and ```kubectl describe <your stateful-set name>```.
2. To verify the presence of secrets used in your StatefulSet definitions, run: ```kubectl get secrets```."
"### **Key Points:**
1. To run Minikube without a hypervisor, use the Docker driver with the command: ```minikube start --vm-driver=none```. This requires Docker and a Linux environment.
2. For running Minikube with VirtualBox, create a VM using the command: ```minikube start --vm-driver=virtualbox```, ensuring the host-only adapter has a different address than the Minikube instance (e.g., Minikube: 192.168.99.103, Host-only: 192.168.99.1)."
"### **Key Points:**
1. Scaling is not possible without meeting certain conditions. 
2. Use the option `--current-replicas` to specify the current size of the resource for scaling, e.g., `--current-replicas=-1`."
"### **Key Points:**
1. **Update Kubernetes Version**: Upgrade to the latest supported Kubernetes version (v1.23 as of December 2021) to utilize stable features like taint-based evictions. 
   
2. **Modify Kubelet Configuration**: Edit the kubelet config file at `/var/lib/kubelet/config.yaml` to adjust eviction parameters:
   ```yaml
   evictionpressuretransitionperiod: 120s
   evictionsoftgraceperiod: 
     nodefs.available: 60s
   evictionsoft:
     nodefs.available: 15gi 
   ```

3. **Monitor and Manage Disk Pressure**: Regularly monitor disk pressure on nodes and adjust kubelet configurations as necessary to prevent pod evictions. Use the following command to delete unwanted pods:
   ```bash
   kubectl get pods | grep -e ""containerstatusunknown"" -e ""evicted"" -e ""completed"" -e ""error"" | awk '{print $1}' | xargs kubectl delete pod
   ```"
"### **Key Points:**
1. Use a bash script to monitor the external IP of a Kubernetes service until it becomes available. The script checks the service status and waits if the IP is not yet assigned. 
   ```bash
   #!/bin/bash
   ip=""""
   while [ -z $ip ]; do
     echo ""waiting for external ip""
     ip=$(kubectl get svc $1 --namespace cloud-endpoints --template=""{{range .status.loadbalancer.ingress}}{{.ip}}{{end}}"")
     [ -z ""$ip"" ] && sleep 10
   done
   echo 'found external ip: '$ip
   ```"
"### **Key Points:**
1. Use the special `$` variable to reference top-level Helm values instead of `.` to avoid issues with the loop index in templates. Code snippet: `$.values`.
2. Replace the range loop with a StatefulSet to simplify the template and utilize standard `.values` without special handling. Code snippet:
   ```yaml
   apiVersion: apps/v1
   kind: StatefulSet
   metadata: { ... }
   spec:
     replicas: {{ .values.cp.replicas }}
   ```
3. For environment variables based on pod index, use a Docker entrypoint script to set the variable to the hostname if necessary."
"### **Key Points:**
1. Specify CPU requests in your pod specifications to enable Horizontal Pod Autoscaler (HPA) calculations, e.g., ```resources: requests: cpu: 500m```.
2. Upgrade to the latest version of Horizontal Pod Autoscaler (v2beta2) and configure it as follows:
   ```yaml
   apiVersion: autoscaling/v2beta2
   kind: horizontalpodautoscaler
   metadata:
     name: myapi
   spec:
     scaleTargetRef:
       apiVersion: apps/v1
       kind: deployment
       name: myapi
     minReplicas: 2
     maxReplicas: 4
     metrics:
     - type: resource
       resource:
         name: cpu
         target:
           type: utilization
           averageUtilization: 80
   ```"
"### **Key Points:**
1. Add a query parameter to specify the container when retrieving logs from a pod. Use the following command:  
   ```bash
   curl -k -h authorization: bearer my-super-secret-token https://kubernetes/api/v1/namespaces/default/pods/my-app-1/log?container=nginx```"
"### **Key Points:**
1. Define environment variables in `deployment.yaml` using Helm's flow control and dictionary functions to render them correctly. Code snippet: 
   ```yaml
   {{- range $key, $value := .values.env }}
   - name: {{ $key }}
     value: {{ $value | quote }}
   {{- end }}
   ```
2. Specify the environment variables in `values.yaml` to ensure they are rendered in the deployment. Code snippet:
   ```yaml
   env:
     variable_1: value_1
     variable_2: value_2
     variable_3: value_3
   ```"
"### **Key Points:**
1. Update the Ingress specification from v1beta1 to v1, ensuring to define the host and backend service correctly.  
   ```yaml
   spec:
     rules:
     - host: mylocalmongoexpress.com
       http:
         paths:
         - path: /
           pathtype: prefix
           backend:
             service:
               name: mongoexpress-service
               port:
                 number: 8081
   ```"
"### **Key Points:**
1. Create a service that selects pods labeled with `app: myapp` using the following configuration:
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: service-simple-service
   spec:
     selector:
       app: myapp
     ports:
       - protocol: tcp
         port: 80
         targetPort: 9376
   ```"
"### **Key Points:**
1. Disable the `securityContext` feature in `values.yml` of the Helm chart to allow writing of files."
"### **Key Points:**
1. Use a single deployment template with separate value files for each environment to manage environment-specific configurations. Code snippet: 
   ```yaml
   env:
      - name: system_opts
      - value: ""{{ .values.opts }}""
   ```
2. Create environment-specific value files (e.g., `values-dev.yaml` and `values-prod.yaml`) to define configurations for different environments. Code snippets:
   ```yaml
   # values-dev.yaml
   opts: ""-dapp1.url=http://dev.app1.xyz -dapp2.url=http://dev.app2.abc ""
   ```
   ```yaml
   # values-prod.yaml
   opts: ""-dapp1.url=http://prod.app1.xyz -dapp2.url=http://prod.app2.abc ""
   ```
3. Specify the relevant value file during deployment using the Helm command. Code snippet:
   ```bash
   helm install -f values-dev.yaml my-app ./test-chart
   ```"
"### **Key Points:**
1. Use the `--recursive` flag with `kubectl explain` to display all fields of a custom resource definition (CRD) at once without descriptions.  
   Code snippet: `kubectl explain --recursive my-crd`"
"### **Key Points:**
1. Use YAML/JSON config files to define Kubernetes resources, allowing for version control and auditing of changes. 
   ```bash
   kubectl get job/ticketing-job-lifetime-manage --namespace=${namespace} --output=json > ${pwd}/ticketing-job-lifetime-manage.json
   ```
   
2. Mutate the resource file using tools like `jq` for JSON processing to reflect the intended configuration.
   ```bash
   jq '.metadata.annotations=[{""foo"":""x""},{""bar"":""y""}]' ${pwd}/ticketing-job-lifetime-manage.json > ${pwd}/new-job.json
   ```

3. Apply the modified configuration back to Kubernetes using `kubectl create` or `kubectl apply`.
   ```bash
   kubectl create --filename=${pwd}/new-job.json --namespace=${namespace}
   ```"
"### **Key Points:**
1. To patch a deployment and add an environment variable from a secret, use the following command:
   ```bash
   kubectl patch deploy/example --patch='
   {
     ""spec"": {
       ""template"": {
         ""spec"": {
           ""containers"": [
             {
               ""name"": ""darkhttpd"",
               ""env"": [
                 {
                   ""name"": ""storage_password"",
                   ""valueFrom"": {
                     ""secretKeyRef"": {
                       ""name"": ""redis"",
                       ""key"": ""redis-password""
                     }
                   }
                 }
               ]
             }
           ]
         }
       }
     }
   }'
   ```

2. Alternatively, you can use a JSON patch style to add the environment variable:
   ```bash
   kubectl patch --type json deploy/example --patch='
   [
     {
       ""op"": ""add"",
       ""path"": ""/spec/template/spec/containers/0/env/-"",
       ""value"": {
         ""name"": ""storage_password"",
         ""valueFrom"": {
           ""secretKeyRef"": {
             ""name"": ""redis"",
             ""key"": ""redis-password""
           }
         }
       }
     }
   ]'
   ```"
"### **Key Points:**
1. Use `kubectl get` and `jq` to change all occurrences of a service name in an Ingress object, but be aware of potential race conditions:  
   ```bash
   kubectl get ing/main-ingress -o json \ 
     | jq '(.spec.rules[].http.paths[].backend.servicename | select(. == ""be-srvc"")) |= ""some-srvc""' \
     | kubectl apply -f -
   ```

2. Directly patch specific backend service names in an Ingress object using known indexes in the arrays:  
   ```bash
   kubectl patch ing/main-ingress --type=json \
     -p='[{""op"": ""replace"", ""path"": ""/spec/rules/0/http/paths/1/backend/servicename"", ""value"":""some-srvc""}]'
   kubectl patch ing/main-ingress --type=json \
     -p='[{""op"": ""replace"", ""path"": ""/spec/rules/1/http/paths/1/backend/servicename"", ""value"":""some-srvc""}]'
   ```

3. Combine multiple patch operations into a single command to ensure atomicity and avoid race conditions:  
   ```bash
   kubectl patch ing/main-ingress --type=json \
     -p='[{""op"": ""replace"", ""path"": ""/spec/rules/0/http/paths/1/backend/servicename"", ""value"":""some-srvc""}, {""op"": ""replace"", ""path"": ""/spec/rules/1/http/paths/1/backend/servicename"", ""value"":""some-srvc""}]'
   ```"
"### **Key Points:**
1. Set the restart policy to never to prevent the pod from restarting after deletion, ```restartPolicy: Never```."
"### **Key Points:**
1. To use Traefik as an ingress controller, expose the Traefik service with type `LoadBalancer`.  
   Code snippet: 
   ```yaml
   apiVersion: v1
   kind: service
   metadata:
     name: traefik
   spec:
     type: LoadBalancer
     selector:
       k8s-app: traefik-ingress-lb
     ports:
     - port: 80
       targetPort: 80
   ```"
"### **Key Points:**
1. Automate the update of master authorized networks in a GKE cluster using `gcloud`, `jq`, and `tr`. The code snippet to retrieve existing CIDR blocks and update the cluster is:
   ```bash
   new_cidr=8.8.4.4/32
   export cluster=test-psp
   old_cidr=$(gcloud container clusters describe $cluster --format json | jq -r '.masterauthorizednetworksconfig.cidrblocks[] | .cidrblock' | tr '\n' ',')
   echo ""the existing master authorized networks were $old_cidr""
   gcloud container clusters update $cluster --master-authorized-networks ""$old_cidr$new_cidr"" --enable-master-authorized-networks
   ```"
"### **Key Points:**
1. Change the path for persistent volumes to avoid using the `/tmp` directory, as it gets automatically cleared. 
   - Reference: [Minikube Persistent Volumes Documentation](https://minikube.sigs.k8s.io/docs/handbook/persistent_volumes/#a-note-on-mounts-persistence-and-minikube-hosts).
2. Use a safer directory for persistent volume claims, such as `/data`, instead of the default temporary path. 
   - Workaround: [GitHub Issue #7511](https://github.com/kubernetes/minikube/issues/7511#issuecomment-612099413)."
"### **Key Points:**
1. Ensure the service is linked to the deployment by setting the correct selector in the service definition: 
   ```yaml
   selector:
     app: your-deployment-name
   ```
2. Set the corresponding label in the deployment to match the service selector:
   ```yaml
   labels:
     app: your-deployment-name
   ```"
"### **Key Points:**
1. Include the authentication command in your probe by formatting it as: `""auth ....\r\nping\r\n""`.
2. If the response from the server is not critical, you can test for no authentication by simply checking for noauth."
"### **Key Points:**
1. Update API version from `apiextensions.k8s.io/v1beta1` to `apiextensions.k8s.io/v1` due to deprecation in Kubernetes 1.19.
2. Use `kubectl` with high verbosity level for debugging commands effectively."
"### **Key Points:**
1. To grant additional AWS users access to an Amazon EKS cluster, edit the `aws-auth` configmap and add a new `mapusers` entry under `data`. Ensure the section exists or create it if necessary.
   ```yaml
   mapusers: |
     - userarn: arn:aws:iam::555555555555:user/my-new-admin-user
       username: my-new-admin-user
       groups:
         - system:masters
   ```
2. Each `mapusers` entry requires the following parameters: `userarn` (ARN of the IAM user), `username` (Kubernetes username), and `groups` (list of Kubernetes groups the user belongs to).
   ```yaml
   userarn: the arn of the iam user to add.
   username: the user name within kubernetes to map to the iam user.
   groups: a list of groups within kubernetes to which the user is mapped to.
   ```"
"### **Key Points:**
1. Ensure to specify the `type: LoadBalancer` in your service configuration to create a load balancer.  
   ```yaml
   spec:
     type: LoadBalancer
   ```"
"### **Key Points:**
1. Manually scale an app horizontally by changing the number of replicas in the deployment.
2. Use a Horizontal Pod Autoscaler to automate horizontal scaling by configuring metric thresholds."
"### **Key Points:**
1. Update the cron job schedule syntax to run every 5 hours at the 15th minute from Monday to Sunday: ```15 */5 * * 1-7```.
2. Use the crontab.guru tool to verify the syntax of the scheduled job according to requirements."
"### **Key Points:**
1. Use one Helm chart per service and make dependent charts for databases to avoid shared resources. Code snippet: `helm upgrade --install --name $* -f values.yaml ./charts/$*`.
2. To manage deployment order and dependencies, utilize a Makefile to ensure services are deployed in the correct sequence. Code snippet: 
   ```makefile
   all: api-proxy.deployed

   %.deployed:
           helm upgrade --install --name $* -f values.yaml ./charts/$*
           touch $@

   api-proxy.deployed: a.deployed c.deployed
   a.deployed: b.deployed
   c.deployed: b.deployed d.deployed
   ```"
"### **Key Points:**
1. Define values for a sub-chart in the parent `values.yaml` under a property named after the sub-chart, e.g., `subchart1:` followed by its properties. 
   ```yaml
   subchart1:
     subchartprop1: value
     subchartprop2: value
   ```
2. Access global properties in both the parent and sub-chart using the syntax `{{ values.global.globalprop1 }}`.
3. Access parent properties only within the parent chart, e.g., `values.parentprop1` is not accessible in the sub-chart.
4. Access sub-chart properties in the parent chart using `values.subchart1.subchartprop1` and directly in the sub-chart using `values.subchartprop1`."
"### **Key Points:**
1. Use `kubectl get pod` with `-oname` to list pod names and filter by specific names using `grep`. The command is:  
   ```bash
   kubectl get pod -oname | grep -e 'app2|app3|hello' | xargs kubectl describe
   ```"
"### **Key Points:**
1. Change the current context to a specific user and cluster using the command: ```kubectl config use-context my-context-name```."
"### **Key Points:**
1. Create a default deny-all NetworkPolicy to restrict all ingress and egress traffic: 
   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: NetworkPolicy
   metadata:
     name: default-deny-all
   spec:
     podSelector: {}
     policyTypes:
     - Ingress
     - Egress
   ```
   
2. Reference the NetworkPolicy in your Ingress settings to allow traffic from a specific IP block:
   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: NetworkPolicy
   metadata:
     name: ingress-access
   spec:
     podSelector:
       matchLabels:
         app.kubernetes.io/name: ingress-nginx
     ingress:
     - from:
       - ipBlock:
           cidr: 192.168.1.0/24
   ```

3. Create an additional NetworkPolicy to allow ingress traffic to backend pods from the Ingress controller:
   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: NetworkPolicy
   metadata:
     name: ingress-to-backends
   spec:
     podSelector:
       matchLabels:
         app: myapp
     ingress:
     - from:
       - namespaceSelector:
           matchLabels:
             ingress: ""true""
         podSelector:
           matchLabels:
             app.kubernetes.io/name: ingress-nginx
   ```"
"### **Key Points:**
1. Scaling the nginx-ingress-controller from 1 to 3 replicas may increase pressure on the Kubernetes API server; implement monitoring to observe any impact.
2. Increasing the replica count of the nginx-ingress-controller will not create new AWS ELBs; a single ELB will continue to route traffic to the service, which balances it across the 3 replicas."
"### **Key Points:**
1. Patch the default service account in the correct namespace to include the image pull secret."
"### **Key Points:**
1. Change the `create_iam_role` argument to `false` to avoid creating the same role twice and manage the role with `aws_iam_role.fluent_bit_logger`. Update the IAM role name accordingly: ```iam_role_name = aws_iam_role.fluent_bit_logger.name```.
2. Alternatively, manage the IAM role within the EKS module declaration using the provided configuration."
"### **Key Points:**
1. Use a log query to match pod names based on deployment names, utilizing regular expressions for precise matching:  
   ```jsonpayload.metadata.name =~ ""<workload-name>\s*""```.
2. Alternatively, employ a substring comparison operator to query pod names that contain the deployment name:  
   ```jsonpayload.metadata.name : ""workload-name""```."
"### **Key Points:**
1. To route database connections based on usernames, consider implementing a database proxy service that acts like an ingress controller, allowing routing to different servers based on the username format (e.g., user1@server1 to server1, user1@server2 to server2). 
2. This solution leverages Layer 7 (L7) characteristics, which require support for username-based routing, depending on the specific database capabilities."
"### **Key Points:**
1. Create the storage class ""ebs-sc"" after installing the EBS CSI driver. Use the following code snippet:
   ```yaml
   apiVersion: storage.k8s.io/v1
   kind: StorageClass
   metadata:
     name: ebs-sc
   provisioner: ebs.csi.aws.com
   parameters:
     type: gp3
   reclaimPolicy: Retain
   volumeBindingMode: WaitForFirstConsumer
   ```"
"### **Key Points:**
1. Authenticate with your Google account using the command: ```gcloud auth login```.
2. Set up application default credentials with the command: ```gcloud auth application-default login```."
"### **Key Points:**
1. Reference the target port by name in the deployment.spec and service.spec for Kubernetes services, using a custom port name like `mycustomport`. 
   ```yaml
   targetPort: mycustomport
   ```
2. Ensure that the target port is defined in the pod's container ports and adheres to the IANA service name format or is a valid number between 1 and 65535.
   ```yaml
   ports:
     - name: mycustomport
       containerPort: 8080
   ```"
"### **Key Points:**
1. Set the `http2maxstreamsperconnection` in the cluster spec for the kube-apiserver configuration: 
   ```yaml
   spec:
     kubeapiserver:
       http2maxstreamsperconnection: <value>
   ```"
"### **Key Points:**
1. Enable LDAP authentication in `grafana.ini` by setting `auth.ldap.enabled` to true and specifying the configuration file as `ldap.toml`:  
   ```yaml
   auth.ldap:
     enabled: true
     allow_sign_up: true
     config_file: /etc/grafana/ldap.toml
   ```

2. Configure LDAP settings in the `ldap` section, ensuring to update the `host`, `port`, and `bind_dn` as per your LDAP server configurations:  
   ```yaml
   ldap:
     enabled: true
     existingsecret: """"
     config: |-
       verbose_logging = true
       [[servers]]
       host = ""my-ldap-server""
       port = 636
       use_ssl = true
       start_tls = false
       ssl_skip_verify = false
       bind_dn = ""uid=%s,ou=users,dc=myorg,dc=com""
   ```"
"### **Key Points:**
1. Remove the `-t` flag from the `kubectl exec` command to avoid tty allocation issues in Jenkins. Use the following command instead:  
   ```bash
   kubectl exec -i kafkacat-5f8fcfcc57-2txhc -- kafkacat -b cord-kafka -c -t bbsim-olt-0-events -o s@1585031458
   ```"
"### **Key Points:**
1. Set the Datadog destination site to `datadoghq.eu` in the Helm chart using the command:  
   ```bash
   helm upgrade --install datadog datadog/datadog \
     --namespace monitoring \
     --create-namespace \
     --atomic \
     --set datadog.apikey=<my-datadog-api-key> \
     --set targetsystem=linux \
     --set datadog.site=datadoghq.eu \
     --values values.yaml
   ```
2. Alternatively, configure the Datadog site in the values file:  
   ```yaml
   datadog:
     site: datadoghq.eu
     kubelet:
       host:
         valuefrom:
           fieldref:
             fieldpath: spec.nodename
       hostcapath: /etc/kubernetes/certs/kubeletserver.crt
       tlsverify: false # required as of agent 7.35
   ```"
"### **Key Points:**
1. Use `kubectl get pods` with `-o json` and `jq` to filter pods not in a ready state, excluding specific namespaces: 
   ```bash
   kubectl get pods --all-namespaces --field-selector=metadata.namespace!=kube-system,metadata.namespace!=monitoring,metadata.namespace!=rtf -ojson | jq '.items[] | select(.status.containerstatuses[].ready==false) | .metadata.namespace + ""/"" + .metadata.name'
   ```
2. To filter pods that are in a ready state, change `ready==false` to `ready==true` in the `jq` command."
"### **Key Points:**
1. Create a deployment for a redirector using darkhttpd to forward requests:
   ```yaml
   apiVersion: apps/v1
   kind: deployment
   metadata:
     name: redirector
   spec:
     replicas: 1
     template:
       spec:
         containers:
           - name: redirector
             image: docker.io/alpinelinux/darkhttpd
             ports:
               - containerPort: 8080
             args:
               - --forward-all
               - http://monitoring.internal/grafana
   ```

2. Define a corresponding service for the redirector:
   ```yaml
   apiVersion: v1
   kind: service
   metadata:
     name: redirector
   spec:
     ports:
       - port: 8080
         protocol: tcp
         targetPort: 8080
   ```

3. Set up an Ingress object to route traffic to the redirector service:
   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: ingress
   metadata:
     name: redirector
     annotations:
       kubernetes.io/ingress.class: traefik
   spec:
     rules:
       - host: monitoring.internal
         http:
           paths:
             - path: /
               pathType: Prefix
               backend:
                 service:
                   name: redirector
                   port:
                     number: 8080
   ```

4. Use Kustomize to manage the deployment, service, and ingress configurations:
   ```yaml
   apiVersion: kustomize.config.k8s.io/v1beta1
   kind: kustomization
   resources:
   - deployment.yaml
   - ingress.yaml
   - service.yaml
   commonLabels:
     component: redirector
   ```"
"### **Key Points:**
1. Install a specific version of kubectl to resolve issues: ```apt-get install -y kubectl=1.10.5-00```."
"### **Key Points:**
1. Ensure that the SSL certificate specified in `service.beta.kubernetes.io/aws-load-balancer-ssl-cert` is signed in the same region where the AWS load balancer is created to avoid provisioning issues.
2. If encountering load balancer provisioning limits, request a quota increase from AWS or consider moving your Kubernetes cluster to a different AWS region to resolve the issue."
"### **Key Points:**
1. Use a postStart handler in the Kubernetes StatefulSet to copy configuration files from a ConfigMap to the NiFi configuration directory.  
   Code snippet: 
   ```yaml
   lifecycle:
     poststart:
       exec:
         command:
           - bash
           - -c
           - |
             cp -a /opt/nifi/nifi-1.6.0/kubeconfig/. /opt/nifi/nifi-1.6.0/conf
   ```
2. Define the container and volume mounts in the StatefulSet to ensure the NiFi application has access to the necessary configuration files.  
   Code snippet: 
   ```yaml
   containers:
   - name: 'myname'
     image: 'apache/nifi:latest'
     ports:
       - name: http
         containerPort: 8080
         protocol: tcp
       - name: http-2
         containerPort: 1337
         protocol: tcp
     volumeMounts:
       - name: 'nifi-config'
         mountPath: /opt/nifi/nifi-1.6.0/kubeconfig
   volumes:
   - name: 'nifi-config'
     configMap:
       name: 'nifi-config'
   ```"
"### **Key Points:**
1. Ensure that the specified pod network subnet does not overlap with the cluster nodes subnet during `kubeadm init`. Use the command:  
   ```bash
   sudo kubeadm init --pod-network-cidr=10.16.96.0/24 --apiserver-advertise-address=<master-node-ip_addr>```."
"### **Key Points:**
1. Use the `:workloads` command to view all resources across all namespaces, or `:workloads your-namespace` to filter resources from a specific namespace.
2. Note that the workloads view does not display Custom Resource Definitions (CRDs), which require switching to a different view to access."
"### **Key Points:**
1. To resolve the error related to invalid base64 values, use the base64 representation of the value. For example, for the value `true`, use: ```new_var: dhj1zq==```."
"### **Key Points:**
1. To list all secrets currently in use by a pod, use the command:  
   ```kubectl get pods -o json | jq '.items[].spec.containers[].env[]?.valuefrom.secretkeyref.name' | grep -v null | sort | uniq```.
2. Enable encryption at rest for secrets and configure RBAC rules to restrict access to secrets, ensuring only authorized principals can read or create them."
"### **Key Points:**
1. To assign a static IP to your service, add `clusterIP: 1.2.3.3` under the `spec:` section of the service definition."
"### **Key Points:**
1. Use the Helm template command to render chart templates locally, which allows you to see the output without deploying to a cluster. Code snippet: ```$ helm template --help```."
"### **Key Points:**
1. Use a simple path with type prefix for Grafana Ingress, as shown in the manifest: 
   ```yaml
   path: /grafana
   pathtype: prefix
   ```
2. Ensure that the Grafana deployment, service, and other resources are in the same namespace (monitoring)."
"### **Key Points:**
1. A bug in Skaffold has been fixed and will be released in version 2.10.0. 
2. Include a `cluster: {}` configuration inside the build section instead of using `local: usebuildkit: true`."
"### **Key Points:**
1. **Backup existing resources:** Use the command to back up cert-manager resources before deletion.  
   ```bash
   kubectl get -o yaml --all-namespaces issuer,clusterissuer,certificates,orders,challenges > cert-manager-backup.yaml
   ```

2. **Delete existing cert-manager installation:** Uninstall the helm chart and delete custom resource definitions.  
   ```bash
   helm delete --purge <your release name here>
   kubectl delete crd certificates.certmanager.k8s.io issuers.certmanager.k8s.io clusterissuers.certmanager.k8s.io
   ```

3. **Install a fresh version of cert-manager:** Apply CRDs, create a namespace, label it, add the helm repository, and install the helm chart.  
   ```bash
   kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.9/deploy/manifests/00-crds.yaml
   kubectl create namespace cert-manager
   kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true
   helm repo add jetstack https://charts.jetstack.io
   helm repo update
   helm install --name <your release name here> --namespace cert-manager --version v0.9.1 jetstack/cert-manager
   ```

4. **Restore resources:** Attempt to restore from the backup, but note that some resources may need to be recreated manually due to errors.  
   ```bash
   kubectl apply -f cert-manager-backup.yaml
   ```"
"### **Key Points:**
1. Use the `kubectl convert` plugin to update Ingress resources from `extensions/v1beta1` to `networking.k8s.io/v1` with the following command:  
   ```bash
   find . -name ""ingress.yaml"" | while read file; do kubectl convert -f ""$file"" --output-version networking.k8s.io/v1 > ""$file"".bak && mv ""$file"".bak ""$file""; done
   ```
2. Always back up your files before running conversion commands to prevent data loss."
"### **Key Points:**
1. Ensure the correct project ID is used in the image pull command: `gcr.io/{project id}/hello-app:v1`.
2. Create the service account and generate a key, then create the secret with: `kubectl create secret docker-registry gcr-json-key...`.
3. Update the Kubernetes spec with `imagePullSecrets` to allow access to the private image.
4. Verify the service account permissions; use `roles/storage.objectViewer` for pulling images instead of overly broad permissions like `project viewer`."
"### **Key Points:**
1. Ensure the ServiceMonitor's port matches the service port by either using `targetport: 8080` in ServiceMonitor or naming the service port as ""web"".  
   Code snippet: 
   ```yaml
   servicemonitor:
   endpoints:
     - interval: 10s
       path: /prometheus/
       port: ""web""
   ```

2. Define the service with a named port to avoid mismatches, using the name ""web"" for clarity.  
   Code snippet: 
   ```yaml
   service:
   spec:
     type: clusterip
     ports:
       - name: ""web""
         port: 8080
         targetport: 8080
   ```"
"### **Key Points:**
1. Use JSON to patch a service in OpenShift to update its ports configuration. Code snippet: 
   ```bash
   oc patch service/simple-server -p '{ ""spec"": { ""ports"": [ { ""name"": ""gw"", ""protocol"": ""tcp"", ""port"": 1234, ""targetport"": 1234 } ] } }'
   ```"
"### **Key Points:**
1. Use `sudo` before the `kubectl port-forward` command to bind to privileged ports, as elevated permissions are required.
2. Instead of using `kubectl port-forward`, create a LoadBalancer service for GKE with the following configuration:
   ```yaml
   apiVersion: v1
   kind: service
   metadata:
     name: traefik
   spec:
     ports:
       - protocol: tcp
         name: web
         port: 80
         targetPort: web
       - protocol: tcp
         name: websecure
         port: 443
         targetPort: websecure
     selector:
       app: traefik
     type: LoadBalancer
   ```
3. After creating the LoadBalancer service, find the external IP using `kubectl get svc` and update your DNS records accordingly."
"### **Key Points:**
1. The ""ready"" field in the output indicates the number of pods that are currently ready compared to the desired number of pods, shown as ""4/5"" (4 pods ready, 5 pods desired)."
"### **Key Points:**
1. Use semantic versioning (semver) to append values after the last number, such as using `0.1.0-latest` for versioning.
2. Overwrite the version in ChartMuseum during uploads to maintain the latest version."
"### **Key Points:**
1. Remove filters to access pgAdmin from the Kubernetes cluster by using the command: ```kubectl proxy --address='0.0.0.0' --disable-filter=true```."
"### **Key Points:**
1. Create a security group using the AWS CLI with the command: ```$ aws ec2 create-security-group --group-name my-sg --description ""my security group"" --vpc-id vpc-1a2b3c4d```.
2. Authorize ingress for RDP on port 3389 with the command: ```$ aws ec2 authorize-security-group-ingress --group-id sg-903004f8 --protocol tcp --port 3389 --cidr 203.0.113.0/24```.
3. Authorize ingress for SSH on port 22 with the command: ```$ aws ec2 authorize-security-group-ingress --group-id sg-903004f8 --protocol tcp --port 22 --cidr 203.0.113.0/24```.
4. View the security group details using the command: ```$ aws ec2 describe-security-groups --group-ids sg-903004f8```."
"### **Key Points:**
1. Create a PersistentVolume (PV) using a statically provisioned SSD disk with the following configuration:
   ```yaml
   apiVersion: v1
   kind: persistentvolume
   metadata:
     name: ssd-for-k8s-volume
   spec:
     capacity:
       storage: 10Gi
     volumeMode: Filesystem
     accessModes:
       - ReadWriteOnce
     gcePersistentDisk:
       pdName: already-created-ssd-disk
       fsType: ext4
   ```

2. Create a PersistentVolumeClaim (PVC) that requests an empty storage class to bind to the statically provisioned PV:
   ```yaml
   apiVersion: v1
   kind: persistentvolumeclaim
   metadata:
     name: pvc-ssd-demo
   spec:
     storageClassName: """"
     accessModes:
       - ReadWriteOnce
     resources:
       requests:
         storage: 10Gi
   ```

3. Manually provision the SSD disk using the following command:
   ```bash
   gcloud compute disks create --size=10Gi --zone=[your zone] --type=pd-ssd already-created-ssd-disk
   ```"
"### **Key Points:**
1. Add the `kubernetes.io/ingress.class: haproxy` annotation to the Ingress resource to enable HAProxy ingress functionality.  
   ```yaml
   kubernetes.io/ingress.class: haproxy
   ```
2. Define the Ingress rules for multiple hosts and their corresponding backend services and ports.  
   ```yaml
   rules:
   - host: a.raven.aedas-prev.inercya.com
     http:
       paths:
       - backend:
           servicename: ravendb-0
           serviceport: 443
         path: /
   ```  
   (Repeat for other hosts as shown in the original configuration.)"
"### **Key Points:**
1. Create a Role to allow access to service accounts in a specific namespace:
   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: Role
   metadata:
     name: default
     namespace: your_namespace
   rules:
   - apiGroups:
     - """"
     resources:
     - serviceaccounts
     verbs:
     - get
   ```

2. Create a RoleBinding to bind the Role to the default service account:
   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: RoleBinding
   metadata:
     name: default
     namespace: your_namespace
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: Role
     name: default
   subjects:
   - kind: ServiceAccount
     name: default
     namespace: your_namespace
   ```

3. Create a Job with a pre-install hook to patch the default service account with an image pull secret:
   ```yaml
   apiVersion: batch/v1
   kind: Job
   metadata:
     name: create-image-pull-secret
     annotations:
       ""helm.sh/hook"": pre-install
       ""helm.sh/hook-delete-policy"": hook-succeeded
   spec:
     template:
       spec:
         restartPolicy: Never
         containers:
         - name: k8s
           image: google/cloud-sdk
           imagePullPolicy: IfNotPresent
           command: [""/bin/sh"", ""-c"", ""kubectl patch serviceaccount default -p '{\""imagePullSecrets\"": [{\""name\"": \""your_secret_name\""}]}'""]
   ```"
"### **Key Points:**
1. Define a PodDisruptionBudget (PDB) to control the number of pods that can be evicted during voluntary disruptions, ensuring minimal downtime for your application.
2. Use the `kubectl drain` command to safely evict pods from nodes while respecting the PDB, allowing for graceful evictions."
"### **Key Points:**
1. Change the service configuration to refer to `targetport` instead of `hostport`.  
   ```yaml
   ports:
     - port: 2379
       targetport: 2379
       nodeport: 32379
   ```"
"### **Key Points:**
1. Modify the selector in your service.yaml to bind the service to the deployment: 
   ```yaml
   selector:
     app: simple-server-app
   ```
2. Use the `kubectl expose` command to create a service and avoid binding mistakes."
"### **Key Points:**
1. Add the `logback.xml` file to the root folder of the Helm chart template, ensuring the correct folder structure is maintained.
2. Update the `configmap.yml` file to import the `logback.xml` file as follows:
   ```yaml
   apiVersion: v1
   kind: configmap
   metadata:
     name: {{ .values.app.name }}-configmap
     namespace: {{ .values.namespace }}
   data:
     application.yml : |+
       server:
         ssl:
           enabled: false
     logback.xml : |+
   {{ .Files.Get ""base-logback.xml"" | indent 4 }}
   ```"
"### **Key Points:**
1. The `startupprobe` feature is only available in Kubernetes version 1.16 and later; it cannot be used with version 1.12.
2. To resolve issues related to liveness probes, consider removing the liveness probe, as many applications may not require it."
"### **Key Points:**
1. External monitoring tools require restricted access; consider using supported solutions from Google Cloud partners for autopilot clusters, as custom tools cannot be installed. 
2. Host options are limited in GKE Autopilot: HostPort and HostNetwork are not allowed, HostPath volumes can only be used in read mode for specific paths, and HostNamespaces are prohibited.
3. To access Docker container logs in GKE Autopilot, consider switching to a standard GKE cluster or using Cloud Operations with Slack notifications for alerts.
4. For exporting logs to Elasticsearch from Cloud Logging, refer to the guide on Google Cloud's architecture documentation."
"### **Key Points:**
1. Use `helm delete` with `awk` to remove Helm releases matching a specific chart name pattern. Replace ""search"" with your desired pattern: 
   ```bash
   helm delete $(helm ls | awk '$9 ~ /search/ { print $1 }')
   ```"
"### **Key Points:**
1. Use a single argument for `sh -c` commands by specifying them in a list format without outer double quotes, allowing for alternate YAML syntaxes.  
   Code snippet: 
   ```yaml
   args:
     - /bin/sh
     - -ec
     - >-
         current_time=$(date +â€œ%y-%m-%dt%h:%m:%s.%szâ€);
   ```

2. For dynamic environment variables in commands, embed or compute them directly in the command string, avoiding command substitution in `env:`.  
   Code snippet: 
   ```yaml
   -d â€œ{â€œlabelâ€:â€œmyservicebuslabelâ€,â€œdataâ€:â€œ{'timestamp':'$current_time'}â€,â€œqueuenameâ€:â€œmyservicebusqueueâ€}â€
   ```

3. When using JSON in shell commands, ensure to use double quotes and escape them properly to allow for variable expansion.  
   Code snippet: 
   ```yaml
   -d â€œ{â€œdataâ€:â€œ{\â€œtimestamp\â€:â€œ$current_timeâ€}â€}â€
   ```

4. Consider using tools like `jq` for JSON manipulation or templates in a configmap with `sed` or `envsubst` for dynamic values to simplify the process.  
   Code snippet: 
   ```bash
   jq '.data.timestamp = $current_time'
   ```"
"### **Key Points:**
1. In Helm version 2, the release name is determined by the `--name` parameter during installation or automatically generated if absent. 
   ```bash
   helm install --name <release-name>
   ```
2. In Helm version 3, the release name is the first parameter of the `helm install` command, and it can be generated automatically with the `--generate-name` flag.
   ```bash
   helm install <release-name>
   helm install --generate-name
   ```
3. To specify a name explicitly in Helm 3, use the `--name-template` flag.
   ```bash
   helm template --name-template=dummy
   ```"
"### **Key Points:**
1. Use a configuration snippet to rewrite requests to a specific URL permanently: 
   ```nginx.ingress.kubernetes.io/configuration-snippet: | rewrite /preview https://test.app.example.com$uri permanent;```
2. Define Ingress rules for two hosts, directing traffic to different services based on the path: 
   ```yaml
   - host: test.example.io
     http:
       paths:
       - path: /
         backend:
           servicename: service-1
           serviceport: 80
   - host: test.app.example.io
     http:
       paths:
       - path: /preview/*
         backend:
           servicename: service-2
           serviceport: 80
   ```"
"### **Key Points:**
1. To use a token with `kubectl`, ensure the kubeconfig file does not include a client certificate, allowing the token to be overridden with the `--token` flag. 
2. To view the bearer token being sent in a `kubectl` command, use the command: ```kubectl get pods --v=10 2>&1 | grep -i bearer```."
"### **Key Points:**
1. Ensure the Ingress is correctly configured to point to port 80 of the service, which should match the service's target port. Code snippet: 
   ```yaml
   ports:
     - port: 80
       targetport: 3001
   ```
2. Verify that you can directly access the service using `curl -l echo.example.com`. If it fails, check the service configuration for correctness."
"### **Key Points:**
1. To use an image from Google Container Registry (GCR), specify the full address format: ```asia.gcr.io/project-name/repo-name```."
"### **Key Points:**
1. Create a single CronJob resource with one crontab time and multiple containers to run. 
2. Create several CronJob resources using the same container images but with different commands and arguments for each job."
"### **Key Points:**
1. Create an Endpoints object with the service IP address to point to the pods, using the following code snippet:
   ```yaml
   apiVersion: v1
   kind: endpoints
   metadata:
     name: my-headless-service
   subsets:
   - addresses:
     - ip: 10.0.0.10
     ports:
     - port: 80
       protocol: tcp
   ```
2. Create a headless Service with the same name as the Endpoints object, ensuring it has no label selectors, using the following code snippet:
   ```yaml
   apiVersion: v1
   kind: service
   metadata:
     name: my-headless-service
   spec:
     clusterIP: none
     ports:
     - name: http
       port: 80
       targetPort: 80
   ```"
"### **Key Points:**
1. Add the CRT file to the `/etc/ssl/certs` directory on the master node to enable functionality. 
2. Newly added rows in the `containerd-template.tom` file are not necessary for the solution."
"### **Key Points:**
1. Ensure the directory `/efs-data` exists in your EFS drive by creating it with the command: 
   ```bash
   cd /efs-data
   mkdir efs-data
   ```
2. To mount the EFS file system correctly, use the following command, replacing `$efs_file_system_dns_name` with your actual EFS DNS name:
   ```bash
   sudo mount -t nfs -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport $efs_file_system_dns_name:/ /efs-data
   ```"
"### **Key Points:**
1. Install `kops` and `kubectl` on your CI server, and configure AWS access credentials to access the S3 state store. 
   ```bash
   export name=${your_cluster_name}
   export kops_state_store=s3://${your_cluster_kops_state_store}
   ```
2. Use the `kops export` command to generate the necessary kubeconfig for `kubectl`.
   ```bash
   kops export kubecfg ${your_cluster_name}
   ```
3. For enhanced security, create a service account with specific permissions and use its credentials on your CI server instead of the default admin account."
"### **Key Points:**
1. Create a Kubernetes service object for Elasticsearch to provide a stable endpoint for other pods, using the following configuration:
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: elasticsearch
     labels:
       app: elasticsearch
   spec:
     selector:
       app: elasticsearch
     ports:
       - protocol: tcp
         port: 9200
         targetPort: 9200
   ```
2. Access the Elasticsearch API from other pods using the endpoint `http://elasticsearch:9200`."
"### **Key Points:**
1. Change the `write.replicas` parameter to 1 to schedule a single replica of the loki-write statefulset."
"### **Key Points:**
1. Use service topology to route traffic based on node labels, allowing preference for local pods. Example configuration:
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: my-service
   spec:
     selector:
       app: my-app
     ports:
       - protocol: tcp
         port: 80
         targetPort: 9376
     topologyKeys:
       - ""kubernetes.io/hostname""
       - ""*""
   ```
2. Utilize hostPort to ensure requests to a specific node's port are handled on that node. Example configuration:
   ```yaml
   kind: Pod
   apiVersion: v1
   metadata:
     name: test-api
     labels:
       app: test-api
   spec:
     containers:
     - name: testapicontainer
       image: myprivaterepo/testapi:latest
       ports:
       - name: web
         hostPort: 55555
         containerPort: 80      
         protocol: tcp
   ```"
"### **Key Points:**
1. Use a global values file to define key-value pairs for readability, e.g., in `values.yaml`:
   ```yaml
   global:
     somemap:
       coffee: robusta
       origin: angola
       crema: yes
   ```

2. Define a helper template in `_helpers.tpl` to format the key-value pairs:
   ```go
   {{- define ""mychart.labels.standard""}}
   {{- $global := default (dict) .values.global.somemap -}}
   release: {{ .release.name | quote }}
   chart: {{ .chart.name }}
   values:
   {{- range $key, $value := $global }}
     {{ $key }}: {{ $value }}
   {{- end }}
   {{- end -}}
   ```

3. Include the helper template in another template to render the data:
   ```go
   helm_data:
     {{- $global := default (dict) .values.global -}}
     {{- range $key, $value := $global }}
       {{ $key }}: {{ $value }}
     {{- end }}
     {{ include ""mychart.labels.standard"" . | nindent 0 -}}
   ```

4. Render the template to verify the output using:
   ```bash
   helm template --name dict-chart .
   ```"
"### **Key Points:**
1. To mount a file from a ConfigMap without hiding existing files in the mount point, use the `subPath` option in your volume mount configuration.  
   Code snippet: 
   ```yaml
   - mountPath: /mnt/configmap-file.txt
     subPath: configmap-file.txt
     name: task-cm-file
   ```

2. Ensure that you define both the ConfigMap and the deployment correctly to utilize the mounted files.  
   Code snippets: 
   ```yaml
   apiVersion: v1
   kind: configmap
   metadata:
     name: test-pd-plus-cfgmap
   data:
     file-from-cfgmap: file data
   ```

   ```yaml
   apiVersion: apps/v1
   kind: deployment
   metadata:
     name: test-pv
   spec:
     replicas: 3
     selector:
       matchLabels:
         app: test-pv
     template:
       metadata:
         labels:
           app: test-pv
       spec:
         containers:
         - image: nginx
           name: nginx
           volumeMounts:
           - mountPath: /mnt
             name: task-pv-storage
           - mountPath: /mnt/configmap-file.txt
             subPath: configmap-file.txt
             name: task-cm-file
         volumes:
           - name: task-pv-storage
             persistentVolumeClaim:
               claimName: task-pv-claim
           - name: task-cm-file
             configMap:
               name: test-pd-plus-cfgmap
   ```"
"### **Key Points:**
1. Ensure that the pod's `containerPort` is defined and the service's `targetPort` points to the pod's `containerPort` value or name for readiness probes to work correctly. 
   ```yaml
   containerPort: 8080
   ```
   
2. Create a default backend service with a health check on `/healthz` to handle requests that don't match a host/path. This service is automatically created when the first ingress is created.
   ```yaml
   apiVersion: extensions/v1beta1
   kind: deployment
   metadata:
     name: l7-default-backend
     namespace: kube-system
   spec:
     replicas: 1
     template:
       spec:
         containers:
         - name: default-http-backend
           image: k8s.gcr.io/defaultbackend-amd64:1.5
           livenessProbe:
             httpGet:
               path: /healthz
               port: 8080
   ```

3. Define the default backend service as a NodePort type to comply with GCE requirements, ensuring it matches the `--default-backend-service` argument of the L7 load balancer controller.
   ```yaml
   apiVersion: v1
   kind: service
   metadata:
     name: default-http-backend
     namespace: kube-system
   spec:
     type: NodePort
     ports:
     - port: 80
       targetPort: 8080
   ```"
"### **Key Points:**
1. Verify your kubeconfig file by running the command: ```kubectl config view --kubeconfig <path_to_your_config_file>```.
2. Ensure your kubeconfig is either empty or points to the default location: ```~/.kube/config```.
3. If installed via snap, check for potential sandboxing issues."
"### **Key Points:**
1. The Horizontal Pod Autoscaler (HPA) calculates utilization as a percentage of actual memory usage to resource requests, which is essential for scaling decisions.
2. To avoid conflicts, attach multiple metrics to a single HPA instead of having multiple HPAs managing the same deployment."
"### **Key Points:**
1. Access NGINX logs for troubleshooting by using the following commands:
   ```bash
   kubectl -n <ingress-namespace> get pods 
   kubectl -n <ingress-namespace> logs <nginx-pod>
   ```
2. Ensure that the host in the TLS configuration matches the host in the rules explicitly:
   ```yaml
   spec:
     tls:
       - hosts:
           - pod1out.ie
         secretName: ingress-tls
     rules:
       - host: pod1out.ie
   ```
3. Test the HTTP route functionality by creating another ingress resource before configuring TLS."
"### **Key Points:**
1. Create a deployment for the `transfer-sh` application with specified metadata and labels.  
   ```yaml
   apiVersion: apps/v1
   kind: deployment
   metadata:
     name: transfer-sh
     namespace: transfer-sh
     labels:
       app: transfer-sh
   ```

2. Configure the deployment to use the `dutchcoders/transfer.sh:latest` image with specific arguments and expose port 8080.  
   ```yaml
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: transfer-sh
     template:
       spec:
         containers:
         - name: transfer-sh
           image: dutchcoders/transfer.sh:latest
           args:
           - --provider
           - local
           - --basedir
           - /tmp
           ports:
           - containerPort: 8080
   ```"
"### **Key Points:**
1. Specify the image value directly in the Kubernetes manifest, as Kubernetes does not support lookup or substitution for this value. Example: `image: busybox:{{ .values.dynamictag }}`.
2. Use Helm's templating language to inject the image tag at deployment time, and set the value via command line: `helm upgrade my-app . --set-string dynamictag=20230710`.
3. Kustomize allows modification of the image value at deploy time by writing out a kustomization bundle or using the `kustomize edit` CLI tool."
"### **Key Points:**
1. Add a default property to the `shoulddefault` object in the CustomResourceDefinition to resolve issues. The code snippet is:
   ```yaml
   default: {}   #  &lt;-- this was added
   ```"
"### **Key Points:**
1. Include a `retries` field in the route of the virtual service configuration.  
   Code snippet: `retries: {}`  
2. Ensure the destination is defined as an array within the virtual service.  
   Code snippet: `destination: [{ ... }]`"
"### **Key Points:**
1. Access service CIDR range information through etcd if you have the necessary permissions, using the command: ```etcdctl --endpoints=<your etcd> --<any authentication flags> get ""/registry/ranges/serviceips"" --prefix=true```."
"### **Key Points:**
1. To correctly override the container image's command without affecting the entrypoint in Kubernetes, use the `args` field instead of `command`. Example configuration:
   ```yaml
   containers:
   - name: mongo
     args:
     - mongod
     - --wiredtigercachesizegb=2
   ```
2. When using Podman or Docker, to override the command correctly, use the following command structure:
   - To override `cmd`:
   ```bash
   podman run \
   --interactive --tty --rm \
   docker.io/mongo:5.0 \
     mongod \
     --wiredtigercachesizegb=2
   ```
   - To override `entrypoint` with `mongod`:
   ```bash
   podman run \
   --interactive --tty --rm \
   --entrypoint=mongod \
   docker.io/mongo:5.0 \
     --wiredtigercachesizegb=2
   ```"
"### **Key Points:**
1. Use the `statesave` option in the DataTable to maintain the table state on re-rendering, ensuring that the order and pagination are preserved. Code snippet: 
   ```R
   options = list(statesave = true)
   ```
2. Implement `observeEvent` to update the DataTable when a button is pressed, modifying the selected row's new column. Code snippet:
   ```R
   observeEvent(input$button, {
       df$iris[input$table_rows_selected, c('new_col')] <- 'changed!'
   })
   ```
3. Utilize `updateSearch` and `selectPage` functions to manage search and pagination based on the current state of the DataTable. Code snippet:
   ```R
   updateSearch(proxy, keywords = list(global = input$table_state$search$search, columns = NULL))
   selectPage(proxy, page = input$table_state$start/input$table_state$length + 1)
   ```"
"### **Key Points:**
1. Use `xargs -n1` to call `kubectl logs` for each pod individually, ensuring each pod name is passed as a single argument. Code snippet: ```kubectl get pods | grep running | awk '{print $1}' | xargs -n1 kubectl logs```.
2. Add the `--prefix` argument to `kubectl logs` to identify which pod generated the output. Code snippet: ```kubectl get pods | grep running | awk '{print $1}' | xargs -n1 kubectl logs --prefix```.
3. Simplify the command by using `awk` to filter running pods directly, eliminating the need for `grep`. Code snippet: ```kubectl get pods --field-selector=status.phase==running -o name | xargs -n1 kubectl logs --prefix```."
"### **Key Points:**
1. Identify secrets not referenced in container environment variables using the following command:
   ```bash
   diff \
   <(kubectl get pods -o jsonpath='{.items[*].spec.containers[*].env[*].valuefrom.secretkeyref.name}' | xargs -n1 | sort | uniq) \
   <(kubectl get secrets -o jsonpath='{.items[*].metadata.name}' | xargs -n1 | sort | uniq)
   ```
2. Find all unused secrets in the current namespace, including those referenced in volumes, ingress TLS, and image pull secrets, with this command:
   ```bash
   envsecrets=$(kubectl get pods -o jsonpath='{.items[*].spec.containers[*].env[*].valuefrom.secretkeyref.name}' | xargs -n1)
   envsecrets2=$(kubectl get pods -o jsonpath='{.items[*].spec.containers[*].envfrom[*].secretref.name}' | xargs -n1)
   volumesecrets=$(kubectl get pods -o jsonpath='{.items[*].spec.volumes[*].secret.secretname}' | xargs -n1)
   pullsecrets=$(kubectl get pods -o jsonpath='{.items[*].spec.imagepullsecrets[*].name}' | xargs -n1)
   tlssecrets=$(kubectl get ingress -o jsonpath='{.items[*].spec.tls[*].secretname}' | xargs -n1)
   sasecrets=$(kubectl get secrets --field-selector=type=kubernetes.io/service-account-token -o jsonpath='{range .items[*]}{.metadata.name}{&quot;\n&quot;}{end}' | xargs -n1)

   diff \
   <(echo ""$envsecrets\n$envsecrets2\n$volumesecrets\n$pullsecrets\n$tlssecrets\n$sasecrets"" | sort | uniq) \
   <(kubectl get secrets -o jsonpath='{.items[*].metadata.name}' | xargs -n1 | sort | uniq)
   ```"
"### **Key Points:**
1. Helm 3 does not include a server version, unlike Helm v2. Use the command `helm version` for Helm v2.
2. Refer to the Tiller removal documentation for guidance on transitioning from Helm v2 to Helm 3."
"### **Key Points:**
1. The labeling of the master node is deprecated; it now shows the role as ""control-plane"" instead of ""control-plane,master""."
"### **Key Points:**
1. Pass an array of values using flow syntax: ```elasticsearch: uri: [&quot;127.0.0.1:9200&quot;, &quot;127.0.0.2:9200&quot;]```.
2. Pass an array of values using block syntax: ```elasticsearch: uri: - 127.0.0.1:9200 - 127.0.0.2:9200```.
3. Access the values in Helm templates using range: ```uris:{{- range .values.elasticsearch.uri }}{{.}}{{- end }}```."
"### **Key Points:**
1. Query to show cronjobs where the last finished job has failed: 
   ``` 
   (max by(owner_name, namespace) (kube_job_status_start_time * on(job_name) group_left(owner_name) ((kube_job_status_failed / kube_job_status_failed == 1) + on(job_name) group_left(owner_name) (0 * kube_job_owner{owner_is_controller=""true"",owner_kind=""cronjob""})))) == 1 
   ``` 
2. Query to show cronjobs where the last finished job has succeeded: 
   ``` 
   (max by(owner_name, namespace) (kube_job_status_start_time * on(job_name) group_left(owner_name) ((kube_job_status_succeeded / kube_job_status_succeeded == 1) + on(job_name) group_left(owner_name) (0 * kube_job_owner{owner_is_controller=""true"",owner_kind=""cronjob""})))) 
   ```"
"### **Key Points:**
1. Define an ExternalName service to link to another service using its DNS name, ensuring to use `svc` instead of `service`.  
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: db-service
     namespace: app-layer
   spec:
     type: ExternalName
     externalName: db-service.data-layer.svc.cluster.local
     ports:
       - port: 3306
   ```"
"### **Key Points:**
1. Configure a volume mount for a secret file by specifying the mount path and subpath, ensuring it is read-only: 
   ```yaml
   volumemounts:
     - name: app-secret
       mountpath: /app/appsettings.secret.json
       subpath: appsettings.secret.json
       readonly: true
   ```
2. Define the volume using a Kubernetes secret, specifying the secret name and the key to be used: 
   ```yaml
   volumes:
   - name: app-secret
     secret:
       secretname: app-secret
       items:
       - key: app-secret.json
         path: appsettings.secret.json
   ```"
"### **Key Points:**
1. Google managed certificates can only be used with Ingress for external HTTP(S) load balancing in GKE, not with other ingress controllers like Kong. 
   - Relevant code snippet: `kubernetes.io/ingress.class: gce`
   
2. To establish a secure connection between the client and Kong, use cert-manager to provision certificates or create TLS secrets for the ingress controller.
   - Relevant code snippets: 
     - For cert-manager: `cert-manager.io: docs`
     - For TLS secrets: `kubernetes.io: secret: tls secrets`
   
3. Customize the load balancer configuration within Kong's service by modifying the `values.yml` file, specifically under the `proxy` section.
   - Relevant code snippet: 
     ```yaml
     proxy:
       type: loadbalancer
       annotations: {}
     ```"
"### **Key Points:**
1. Define a multiline environment variable for NGINX configuration to check Azure Front Door ID and return a 403 status if it doesn't match: 
   ```bash
   read -d '' conf << eof
   if ($http_x_azure_fdid !~* ""55ce4ed1-4b06-4bf1-b40e-4638452104da"" ) {
          return 403;
   }
   eof
   ```
2. Use the defined environment variable in the Helm upgrade command for the NGINX Ingress Controller: 
   ```bash
   helm upgrade --install nginx-ingress-controller ingress-nginx/ingress-nginx \
       --namespace ""${namespace}"" \
       --version ""${chart_version}"" \
       --set controller.replicacount=""${replicas}"" \
       --set-string controller.config.use-forwarded-headers=true \
       --set-string controller.config.server-snippet=$conf \
       --debug
   ```"
"### **Key Points:**
1. Build and test a custom Docker image for a React app using the following Dockerfile:
   ```dockerfile
   FROM bitnami/apache:latest
   COPY build /app
   ```
2. Publish the Docker image to Docker Hub with:
   ```bash
   docker login
   docker push username/react-app
   ```
3. Deploy the application on Kubernetes using Helm with the command:
   ```bash
   helm install apache bitnami/apache \
       --set image.repository=username/react-app \
       --set image.tag=latest \
       --set image.pullpolicy=always
   ```
4. For external access via a domain name with NGINX Ingress, use:
   ```bash
   helm install apache bitnami/apache \
       --set image.repository=username/react-app \
       --set image.tag=latest \
       --set image.pullpolicy=always \
       --set ingress.enabled=true \
       --set ingress.hosts[0].name=domain
   ```"
"### **Key Points:**
1. To prepare your container, run a new shell session using: ```kubectl exec my-app -c my-app -it -- bash```.
2. If your pod spec has `stdin: true` and `tty: true`, detach from the container using the escape sequence: ```ctrl+p``` followed by ```ctrl+q``` after executing ```kubectl attach -it``` to the container."
"### **Key Points:**
1. Update the allowed IP address for master authorized networks in Google Kubernetes Engine to avoid access issues when the Cloud Shell IP changes. Use the following command:
   ```bash
   gcloud container clusters update tia-test-cluster \
       --region europe-north1 \
       --enable-master-authorized-networks \
       --master-authorized-networks [existing_auth_nets],[shell_ip]/32
   ```"
"### **Key Points:**
1. Ensure that `cpu` and `memory` properties are nested within `requests` in the resource configuration. The correct structure is:
   ```yaml
   resources:
     requests:
       cpu: ""0.3""
       memory: ""500Mi""
   ```
2. Use the command `kubectl apply -f test.yaml` to validate the configuration, and if errors occur, consider using `--validate=false` to bypass validation checks."
"### **Key Points:**
1. To restrict client IP access to a network load balancer, add the `loadbalancersourceranges` parameter in your service manifest.  
   Code snippet:  
   ```yaml
   spec:
     loadbalancersourceranges:
     - ""143.231.0.0/16""
   ```"
"### **Key Points:**
1. Copy the `~/.kube` and `~/.minikube` folders into the Docker image and configure the Dockerfile accordingly. Code snippet: 
   ```dockerfile
   COPY .kube /root/.kube
   COPY .minikube /root/.minikube
   ```
2. Build and run the Docker image to use `kubectl` commands. Code snippets: 
   ```bash
   docker build . -t username/kubectl:latest
   docker run username/kubectl:latest ./kubectl get pods
   ```
3. Modify the `.kube/config` file to point to the `.minikube` folder in the container after copying."
"### **Key Points:**
1. Configure the ingress controller to allow access via its rules. Refer to the documentation for detailed instructions: [Kubernetes Ingress Documentation](https://kubernetes.io/docs/user-guide/ingress/)."
"### **Key Points:**
1. Set the pod ID of the StatefulSet as an environment variable using `fieldRef` to access the pod's metadata name.
   ```yaml
   env:
     - name: pod_id_of_statefulset
       valueFrom:
         fieldRef:
           fieldPath: metadata.name
   ```
2. Use a post-start lifecycle hook to write the pod ID to the `myid` file in the ZooKeeper data directory.
   ```yaml
   lifecycle:
     postStart:
       exec:
         command: [""/bin/sh"", ""-c"", ""echo ${pod_id_of_statefulset##*-} > ${zoo_data_dir}/myid""]
   ```"
"### **Key Points:**
1. Use `kubectl --v=8` for detailed output, which combines results from the pod and events APIs.
2. Retrieve pod details with the following API call: `get /api/v1/namespaces/default/pods/xyz`.
3. Access related events using this API call: `get /api/v1/namespaces/default/events?fieldselector=involvedobject.name%3dxyz%2cinvolvedobject.namespace%3ddefault%2cinvolvedobject.uid%3dd4605fd6-b152-11e6-a208-02e9103bab42`."
"### **Key Points:**
1. Create a custom Docker image based on `php:7-fpm` and install required PHP extensions using the command: ```RUN apt-get install php-pdo php-mysql```.
2. Build the custom image with the command: ```docker build -t php:7-fpm-mysql .```.
3. Push the built image to a Docker registry using: ```docker push php:7-fpm-mysql```."
"### **Key Points:**
1. To change an existing GKE cluster to a private cluster, you must create a new private cluster or duplicate the existing one via the GCP Cloud Console. This will clone the configuration but not the workloads. 
   - **Action:** Use GCP Cloud Console to duplicate the cluster.
   
2. For creating a private GKE cluster with Terraform, configure the `google_container_cluster` resource with the `private_cluster_config` block to enable private nodes and set the master IP range.
   - **Code Snippet:**
   ```hcl
   resource ""google_container_cluster"" ""primary-cluster"" {
     name               = ""gke-private""
     location           = ""europe-west3-c""
     initial_node_count = 1

     private_cluster_config {
       enable_private_nodes = ""true""
       enable_private_endpoint = ""false""
       master_ipv4_cidr_block = ""172.16.0.0/28""
     }

     node_config {
       machine_type = ""e2-medium""
     }
   }
   ```"
"### **Key Points:**
1. Use a Bitnami image for kubectl in a busybox pod to avoid missing kubectl installation: ```image: bitnami/kubectl:latest```.
2. Set up proper RBAC permissions by creating a role and role binding for the service account: 
   ```yaml
   kind: role
   apiVersion: rbac.authorization.k8s.io/v1
   metadata:
     namespace: jp-test
     name: jp-runner
   rules:
   - apiGroups:
     - extensions
     - apps
     resources:
     - deployments
     verbs:
     - 'patch'
   ```
3. Create a service account for the job to use: 
   ```yaml
   apiVersion: v1
   kind: serviceaccount
   metadata:
     name: sa-jp-runner
     namespace: jp-test
   ```
4. Define a cron job that uses the service account and runs a kubectl patch command: 
   ```yaml
   apiVersion: batch/v1beta1
   kind: cronjob
   metadata:
     name: hello
   spec:
     schedule: ""*/5 * * * *""
     jobTemplate:
       spec:
         template:
           spec:
             serviceAccountName: sa-jp-runner
             containers:
             - name: hello
               image: bitnami/kubectl:latest
               command:
               - /bin/sh
               - -c
               - kubectl patch deployment runners -p '{""spec"":{""template"":{""spec"":{""containers"":[{""name"":""jp-runner"",""env"":[{""name"":""start_time"",""value"":""'$(date +%s)'""}]}]}}}}' -n jp-test
             restartPolicy: OnFailure
   ```"
"### **Key Points:**
1. To resolve volume node affinity conflicts in AWS, recreate the volume in the correct availability zone that matches your worker nodes, or manually create a new volume and copy the data into the appropriate availability zone. 
2. Verify node taints, as the scheduler indicates that there are nodes with taints, which may prevent pods from being scheduled."
"### **Key Points:**
1. Ensure you are using bash to execute commands and import environment variables from `/home/airflow/airflow_env.sh` for Airflow connection setup. Code snippet: `kubectl exec -ti airflow-scheduler-nnn-nnn -- /bin/bash`.
2. Alternatively, you can manually import the environment variables using a shell command. Code snippet: `kubectl exec -ti airflow-scheduler-nnn-nnn -- sh -c "". /home/airflow/airflow_env.sh && airflow list_dags""`."
"### **Key Points:**
1. Use `--dry-run --debug` options with the `helm install` command to display the executed YAML. 
   ```bash
   helm install <release-name> <chart-name> --dry-run --debug
   ```
2. Override the default configmap for the NGINX Ingress controller by creating your own with the necessary configurations.
   ```yaml
   kind: ConfigMap
   apiVersion: v1
   metadata:
     name: {name-of-the-helm-chart}-nginx-ingress-controller
     namespace: {namespace-where-the-nginx-ingress-is-deployed}
   data:
     proxy-read-timeout: ""86400""
     proxy-body-size: ""2g""
     use-http2: ""false""
   ```"
"### **Key Points:**
1. Create a kubeconfig for Amazon EKS using `aws eks get-token` and set the environment variable:  
   ```export kubeconfig=$kubeconfig:~/.kube/config-aws```.
2. For convenience, add the kubeconfig export command to your `.bash_profile`:  
   ```echo 'export kubeconfig=$kubeconfig:~/.kube/config-aws' >> ~/.bash_profile```."
"### **Key Points:**
1. Deploy a job queue system like RabbitMQ with two long-running containers: one for the REST server that writes requests to a queue, and another that listens to the queue and processes messages. 
2. Use the Kubernetes API to create a job that can restart on failure, is not node-bound, and does not require root permissions, avoiding running Docker commands from within a Kubernetes pod."
"### **Key Points:**
1. Track resource consumption trends using Prometheus and Grafana to identify spikes in daemonsets, allowing for better resource allocation.
2. Adjust eviction thresholds to prevent node crashes by following the guidelines provided in the Kubernetes documentation: [Eviction Thresholds](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#eviction-thresholds)."
"### **Key Points:**
1. Disable Grafana installation by setting `grafana.enable: false` in `values.yaml`. 
   ```yaml
   grafana:
     enabled: false
   ```
2. Alternatively, pull the chart locally and remove the `template.grafana` directory before installation. 
   ```bash
   helm pull <chart-name>
   # Delete the template.grafana directory
   helm install <name> ./prometheus-stack
   ```"
"### **Key Points:**
1. Create a function to convert kube-apiserver, kube-controller-manager, and kube-scheduler logs to JSON format, including a timestamp and log group. 
   ```python
   def convert_text_logs_to_json_and_add_loggroup(message, loggroup):
       ...
   ```
2. Develop a function to handle authenticator logs by wrapping them into JSON format with key-value pairs, ensuring the timestamp is correctly labeled and including the log group.
   ```python
   def wrap_text_to_json_and_add_loggroup(message, loggroup):
       ...
   ```"
"### **Key Points:**
1. Register the alertclient in `main.go` by adding it to the nrqlconditionreconciler setup. Code snippet: 
   ```go
   alertclient: interfaces.initnewclient,
   ```"
"### **Key Points:**
1. Add a rewrite target annotation to the Ingress configuration to ensure proper routing, ```nginx.ingress.kubernetes.io/rewrite-target: /```.
2. Update the API path in the backend configuration to match the .NET Core application, changing it to ```path = ""/(api.*)""``` and setting the backend service name to ```service_name = ""olc-api""```."
"### **Key Points:**
1. Add the label `namespace: kube-system` to the `kube-system` namespace to resolve the issue with the namespace selector. 
   ```yaml
   namespaceselector:
       matchlabels:
         namespace: kube-system
   ```"
"### **Key Points:**
1. Deploy a PostgreSQL instance without a subpath by using the following YAML configuration:
   ```yaml
   apiVersion: apps/v1
   kind: deployment
   metadata:
     name: postgres-deployment
   spec:
     replicas: 1
     selector:
       matchLabels:
         component: postgres
     template:
       metadata:
         labels:
           component: postgres
       spec:
         volumes:
           - name: postgres-storage
             persistentVolumeClaim:
               claimName: database-persistent-volume-claim
         containers:
         - name: postgres
           image: postgres
           ports:
           - containerPort: 5432
           volumeMounts:
             - mountPath: /var/lib/postgresql/data
               name: postgres-storage
   ```
2. Verify the deployment and pod status using the following commands:
   ```bash
   kubectl get deploy
   kubectl get po
   ```"
"### **Key Points:**
1. Create a service account in the `my-data` project with necessary GCP roles/permissions (e.g., `roles/bigquery.dataviewer`).
2. Generate a service account key and download the `.json` file containing the credentials.
3. Create a Kubernetes secret for the service account credentials:
   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: my-data-service-account-credentials
   type: opaque
   data:
     sa_json: <contents of running 'base64 the-downloaded-sa-credentials.json'>
   ```
4. Mount the credentials in the container:
   ```yaml
   spec:
     containers:
     - name: my-container
       volumeMounts:
       - name: service-account-credentials-volume
         mountPath: /etc/gcp
         readOnly: true
     volumes:
     - name: service-account-credentials-volume
       secret:
         secretName: my-data-service-account-credentials
         items:
         - key: sa_json
           path: sa_credentials.json
   ```
5. Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable in the container:
   ```yaml
   spec:
     containers:
     - name: my-container
       env:
       - name: GOOGLE_APPLICATION_CREDENTIALS
         value: /etc/gcp/sa_credentials.json
   ```"
"### **Key Points:**
1. Generate Helm chart templates locally to check for unauthorized issues using the command: ```helm template mychart```."
"### **Key Points:**
1. Pods are classified into three Quality of Service (QoS) classes based on resource requests and limits: Guaranteed (all containers have limits), Burstable (at least one container has requests or limits), and Best Effort (no requests or limits set). 
2. Requests are used for scheduling decisions, while limits are not considered during scheduling but can prevent excessive resource usage. 
3. CPU is a compressible resource, meaning it can be throttled if needed, while memory is not compressible; if a memory limit is exceeded, the process will be killed. 
4. In memory exhaustion scenarios, Best Effort class containers are killed first, followed by Burstable, and then Guaranteed class containers. 
5. If a container (like c2) has no memory limit set, it can use as much memory as available, but it will be the first to be killed if the node runs out of memory."
"### **Key Points:**
1. Wait for a Kubernetes namespace to become active by using a loop that checks the namespace status. The code snippet is:  
   ```bash
   while ! [ ""$(kubectl get ns <change to your namespace> -o jsonpath='{.status.phase}')"" == ""active"" ]; do echo 'waiting for namespace to come online. ctrl-c to exit.'; sleep 1; done
   ```"
"### **Key Points:**
1. Specify the namespace for a service account in the subjects section of a configuration: 
   ```yaml
   subjects:
   - kind: serviceaccount
     name: kube-state-metrics
     namespace: <namespace-of-kube-state-metrics-serviceaccount>
   ```"
"### **Key Points:**
1. For private clusters, add a firewall rule to allow master nodes access to port 8443/tcp on worker nodes. Code snippet: 
   ```bash
   gcloud compute firewall-rules create firewall-rule-name \
       --action allow \
       --direction ingress \
       --source-ranges master-cidr-block \
       --rules tcp:8443 \
       --target-tags target
   ```
2. Alternatively, modify the existing firewall rule to include port 8443/tcp along with ports 80/tcp, 443/tcp, and 10254/tcp."
"### **Key Points:**
1. Add a regex annotation to the Kibana Ingress to enable regex path interpretation: ```nginx.ingress.kubernetes.io/use-regex: ""true""```.
2. Set the environment variable in the Kibana pod/deployment to inform it of the base path: ```- name: server_basepath value: /kibana```."
"### **Key Points:**
1. Configure Kibana to run behind a reverse proxy by setting `server.basepath: /kibana` in the `kibana.yml` config file.
2. Maintain the existing Ingress configuration to ensure the application is reachable at the root of the server while responses include the `/kibana/` prefix."
"### **Key Points:**
1. Correct the typo in the cluster name from `kuberntes-dev` to `kubernetes-dev` to match the reference in the context."
"### **Key Points:**
1. Minimum permissions required to restart a deployment include specifying the API groups, resources, resource names, and verbs. The relevant code snippet is:
   ```yaml
   rules:
   - apigroups: [""apps"", ""extensions""]
     resources: [""deployments""]
     resourcenames: [$deployment]
     verbs: [""get"", ""patch""]
   ```"
"### **Key Points:**
1. Create a Kubernetes Job with Helm annotations for testing, using the specified image and command to perform a curl request. 
   ```yaml
   apiVersion: batch/v1
   kind: job
   metadata:
     name: ""{{ .release.name }}-test""
     labels:
       app: {{ .release.name }}
       release: {{ .release.name }}
     annotations:
       ""helm.sh/hook"": test-success
   spec:
     ttlSecondsAfterFinished: 0
     template:
       spec:
         containers:
           - name: test-curl
             image: target-image:1.2.3
             imagePullPolicy: ""IfNotPresent""
             command:
               - /bin/bash
               - -ec
               - |
                 curl --connect-timeout 5 --max-time 10 --retry 5 --retry-delay 5 --retry-max-time 30 --retry-all-errors http://{{ .release.name }}:{{ .values.service.port }}/v1/rest -x post -h ""content-type: application/json"" -d ""{\""foo\"":[\""bar\""]}""
         restartPolicy: Never
   ```
2. Ensure compatibility with Kubernetes API server version 1.20+ and curl version 7.71+ for the job to function correctly."
"### **Key Points:**
1. Create a Certificate Signing Request (CSR) configuration file with the necessary details, including common name and extended key usage:
   ```plaintext
   cat > csr.conf <<eof
   [req]
   req_extensions = v3_req
   distinguished_name = req_distinguished_name
   prompt = no
   [req_distinguished_name]
   cn = s-controller.ns-controller.svc
   [ v3_req ]
   basicconstraints = ca:false
   keyusage = nonrepudiation, digitalsignature, keyencipherment
   extendedkeyusage = clientauth, serverauth
   subjectaltname = @alt_names
   [alt_names]
   dns.1 = s-controller.ns-controller
   dns.2 = s-controller.ns-controller.svc
   eof
   ```

2. Generate a CA certificate and a TLS key and certificate, then create a Kubernetes TLS secret for the webhook:
   ```plaintext
   openssl genrsa -out ca.key 2048
   openssl req -x509 -new -nodes -key ca.key -days 365 -out ca.crt -subj ""/cn=admission_ca""
   openssl genrsa -out server.key 2048
   openssl req -new -key server.key -out server.csr -config csr.conf
   openssl x509 -req -in server.csr -ca ca.crt -cakey ca.key -cacreateserial -out server.crt -days 365 -extensions v3_req -extfile csr.conf
   kubectl create secret tls webhook-tls --cert=server.crt --key=server.key
   ```

3. Set the CA bundle and configure the webhook with volume mounts for the TLS certificates:
   ```plaintext
   export ca_bundle=$(cat ca.crt | base64 | tr -d '\n')
   volume:
   volumes:
   - name: tls-vol
       secret:
         secretname: webhook-tls
   volumemount:
   volumemounts:
   - name: tls-vol
     mountpath: /etc/webhook/certs
     readonly: true
   args:
   - -tlscertfile=/etc/webhook/certs/tls.crt
   - -tlskeyfile=/etc/webhook/certs/tls.key
   ```"
"### **Key Points:**
1. Check the configuration of the ingress resource to ensure it is set up correctly for your application. Use the command: ```kubectl get ingress ingress-fibonacci-service -o=yaml```.
2. Review the logs of the ingress controller to diagnose any issues. Use the command: ```kubectl logs <ingress-controller-pod>```.
3. If using an NGINX ingress controller, verify the NGINX configuration by executing: ```kubectl exec -t <ingress-pod> cat nginx.conf```."
"### **Key Points:**
1. Set up a reverse proxy with basic authentication using NGINX in front of Loki to enforce authentication. 
2. Check the logs of the NGINX ingress pod for authentication errors to troubleshoot issues.
3. Ensure the Kubernetes secret contains the expected credentials and is correctly referenced in the ingress configuration.
4. Use the correct annotations in the ingress object to enable basic authentication."
"### **Key Points:**
1. Move DNS to Azure and use DNS validation for generating the certificate, and configure the Istio gateway as follows:
   ```yaml
   apiVersion: networking.istio.io/v1alpha3
   kind: Gateway
   metadata:
     name: mygateway
   spec:
     selector:
       istio: ingressgateway
     servers:
     - hosts:
       - 'mydomain.com'
       port:
         name: http-bookinfo
         number: 80
         protocol: http
       tls:
         httpsRedirect: true
     - hosts:
       - 'mydomain.com'
       port:
         name: https-bookinfo
         number: 443
         protocol: https
       tls:      
         mode: simple
         serverCertificate: ""use sds""
         privateKey: ""use sds""
         credentialName: ""istio-bookinfo-certs-staging""
   ```

2. Enable SDS at the ingress gateway by using the following Helm command:
   ```bash
   helm template install/kubernetes/helm/istio/ --name istio \
   --namespace istio-system -x charts/gateways/templates/deployment.yaml \
   --set gateways.istio-egressgateway.enabled=false \
   --set gateways.istio-ingressgateway.sds.enabled=true > \
   $HOME/istio-ingressgateway.yaml
   kubectl apply -f $HOME/istio-ingressgateway.yaml
   ```"
"### **Key Points:**
1. Amazon EKS provides high availability for the control plane by automatically scaling across multiple AWS availability zones, managing instances based on load, and handling unhealthy instances without requiring manual installation of the cluster autoscaler.
2. The Kubernetes cluster autoscaler automatically adjusts the number of worker nodes in the cluster based on pod requirements, ensuring optimal resource allocation and can be installed as a deployment in the cluster."
"### **Key Points:**
1. Save the service account file inside a secret and mount it into the deployment volume for pod access.  
   Code snippet: 
   ```yaml
   apiVersion: v1
   kind: deployment
   metadata:
     name: mypod
   spec:
     containers:
     - name: mypod
       image: nginx
       volumeMounts:
       - name: foo
         mountPath: ""/etc/foo""
         readOnly: true
     volumes:
     - name: foo
       secret:
         secretName: mysecret
   ```"
"### **Key Points:**
1. Enable the HTTP load balancing add-on in your GKE cluster to use Google Cloud Load Balancer with Kubernetes Ingress. This can be done via the UI or command line.  
   **UI Steps:** Navigate to clusters > [cluster-name] > details > edit > add-ons > enable HTTP load balancing.  
   **Command:** ```gcloud beta container clusters update <clustername> --update-addons=httploadbalancing=enabled --zone=<your-zone>```
   
2. After enabling the add-on, verify the creation of the BackendConfig CRD by running:  
   ```kubectl get crd | grep backend```  
   and check the API versions with:  
   ```kubectl api-versions | grep cloud```  
   This ensures that the backend configuration can be created successfully."
"### **Key Points:**
1. A Pod can run multiple containers, which is why the containers field is defined as a list. 
   ```yaml
   spec:
     containers:
     - name: busybox
       image: busybox:latest
     - name: nginx
       image: nginx:1.7.9
     - name: redis
       image: redis:latest
   ```"
"### **Key Points:**
1. Create a new service account for the Kubernetes dashboard with admin privileges using a ClusterRoleBinding:
   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1beta1
   kind: clusterrolebinding
   metadata:
     name: kubernetes-dashboard
     labels:
       k8s-app: kubernetes-dashboard
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: cluster-admin
   subjects:
   - kind: serviceaccount
     name: kubernetes-dashboard
     namespace: kube-system
   ```

2. If a cluster-admin service account does not exist, create one with the following configuration:
   ```yaml
   apiVersion: v1
   kind: serviceaccount
   metadata:
     name: admin
     namespace: kube-system
     labels:
       kubernetes.io/cluster-service: ""true""
       addonmanager.kubernetes.io/mode: reconcile
   ```

3. Define an admin ClusterRole to grant all permissions:
   ```yaml
   kind: clusterrole
   apiVersion: rbac.authorization.k8s.io/v1alpha1
   metadata:
     name: admin
   rules:
     - apiGroups: [""*""]
       resources: [""*""]
       verbs: [""*""]
       nonResourceURLs: [""*""]
   ```"
"### **Key Points:**
1. Specify the desired machine type in the cluster configuration by adding `machine_type` in the `node_config` section.  
   ```hcl
   machine_type = ""${var.machine_type}""
   ```
2. Declare the machine type in the `variable.tf` file, setting a default value such as `n1-standard-4`.  
   ```hcl
   variable ""machine_type"" {
       type = ""string""
       default = ""n1-standard-4""
   }
   ```"
"### **Key Points:**
1. For quick tests, use a command to run a container that automatically exits after a set time: ```docker run -d debian sleep 300```.
2. To keep a container running indefinitely, run an application like `top` or use a while loop."
"### **Key Points:**
1. Create a background thread to refresh AWS credentials and write them to a file, then set the `aws_shared_credentials_file` environment variable to point to this file. 
   ```go
   func updatecredentials(ctx context.context) {
       creds, err := c.credentialsprovider.retrieve(ctx)
       s := fmt.sprintf(`[default]
   aws_access_key_id=%s
   aws_secret_access_key=%s
   aws_session_token=%s`, creds.accesskeyid, creds.secretaccesskey, creds.sessiontoken)
       err = os.writefile(credentialsfile.name(), []byte(s), 0666)
       return nil
   }

   func updatecredentialsloop(ctx context.context) {
       for {
           err := updatecredentials(ctx)
           time.sleep(5*time.minute)
       }
   }
   ```

2. Configure the Kubernetes client to use the AWS IAM authenticator with the credentials file created in the first step.
   ```go
   config := clientcmdapi.newconfig()
   config.authinfos[""eks""] = &clientcmdapi.authinfo{
       exec: &clientcmdapi.execconfig{
           command: ""aws-iam-authenticator"",
           args: []string{
               ""token"",
               ""-i"",
               clustername,
           },
           env: []clientcmdapi.execenvvar{
               {
                   name:  ""aws_shared_credentials_file"",
                   value: credentialsfile.name(),
               },
           },
           apiversion:      ""client.authentication.k8s.io/v1beta1"",
           interactivemode: clientcmdapi.neverexecinteractivemode,
       },
   }
   restconfig, err := config.clientconfig()
   clientset, err = kubernetes.newforconfig(restconfig)
   ```"
"### **Key Points:**
1. Use the `join` function to handle environment values containing the `=` symbol, ensuring they are processed correctly. Code snippet: 
   ```go
   {{ $file := .files.get "".env"" | trimsuffix ""\n"" }}
   {{- range $line := splitlist ""\n"" $file -}}
   {{- $kv := splitlist ""="" $line -}}
       - name: {{ first $kv }}
         value: {{ join ""="" (slice $kv 1) | quote }}
   {{- end }}
   ```"
"### **Key Points:**
1. To access the Kubernetes master from within the cluster, use the `--internal-ip` flag to get the internal IP address.  
   Code snippet: ```gcloud container clusters get-credentials --internal-ip [cluster_name]```."
"### **Key Points:**
1. Use custom configuration for the NGINX Ingress Controller to tailor its behavior as needed. 
2. If the paths in your ingress rule match the backend service, no rewrite rule is necessary; simply specify the path for the backend service.
3. To handle different paths between the frontend and backend, use the `nginx.ingress.kubernetes.io/rewrite-target` annotation to avoid 404 errors. Example configuration:
   ```yaml
   nginx.ingress.kubernetes.io/rewrite-target: /different-path
   ```
4. Example ingress rule to redirect requests from `http://example.com/something` to the backend service:
   ```yaml
   apiVersion: extensions/v1beta1
   kind: Ingress
   metadata:
     name: gpg-app-ingress
     annotations:
       kubernetes.io/ingress.class: nginx
   spec:
     rules:
     - host: example.com
       http:
         paths:
           - path: /something
             backend:
               serviceName: example-com
               servicePort: 80
   ```
5. Check the logs of the NGINX Ingress Controller pod for troubleshooting:
   ```bash
   kubectl logs nginx-ingress-controller-xxxxx
   ```"
"### **Key Points:**
1. Fix your system path to ensure that your kubectl path is referenced before the Docker path. 
2. Replace the Docker version of kubectl with your own version.
3. Restart your PC after making changes to the path to ensure kubectl updates its configuration correctly with the appropriate Kubernetes version using the `minikube start` command.
4. Create a script to dynamically change your path based on whether you need to use Docker's kubectl or your own."
"### **Key Points:**
1. Obtain the Kubernetes configuration file by accessing the Google Cloud Platform Kubernetes Engine panel, selecting the desired cluster, and running the command: ```$ gcloud container clusters get-credentials ...```.
2. Load the Kubernetes configuration in your application by setting the environment variable for Google application credentials and using the `kubernetes.config.load_kube_config` function with the path to your YAML file: 
   ```python
   os.environ['google_application_credentials'] = os.path.abspath('credentials.json')
   config.load_kube_config(os.path.abspath('config.yaml'))
   ```"
"### **Key Points:**
1. Regenerate the Kubernetes API server certificate with the correct values: use `127.0.0.1,localhost` instead of `127.0.0.1.localhost`.
2. After regenerating the certificate, copy the cert files to your control nodes and replace the old files in the correct location."
"### **Key Points:**
1. Use the index function to access map values with dots in them. Code snippet: `{{- print (index $secret.data ""user.password"")}}`."
"### **Key Points:**
1. Specify a static IP address in an annotation for GCE/GKE Ingress by using the annotation `""kubernetes.io/ingress.global-static-ip-name"": my-ip-name`.  
   ```yaml
   apiVersion: extensions/v1beta1
   kind: Ingress
   metadata:
     name: myingress
     annotations:
       ""kubernetes.io/ingress.global-static-ip-name"": my-ip-name
   spec:
     ...
   ```"
"### **Key Points:**
1. Verify your kubectl configuration by checking the config file at `~/.kube/config` and use the admin config for testing:  
   ```bash
   kubectl --kubeconfig /etc/kubernetes/admin.conf get po
   ```
2. For testing, copy the admin config to your home directory and set the appropriate permissions:  
   ```bash
   sudo cp /etc/kubernetes/admin.conf $HOME/
   sudo chown $(id -u):$(id -g) $HOME/admin.conf
   export KUBECONFIG=$HOME/admin.conf
   ```
3. To join a Kubernetes cluster, ensure Kubernetes is installed and use the following command:  
   ```bash
   sudo kubeadm join --token token master_ip:6443
   ```"
"### **Key Points:**
1. Remove the namespace ""test"" from all chart files to ensure compatibility. 
2. Use the command `helm install --namespace=namespace2 ...` to install the charts in the new namespace."
"### **Key Points:**
1. Specify the ingress class by adding `ingressclassname: nginx` to the Ingress resource definition to resolve the issue. 
   ```yaml
   ingressclassname: nginx
   ```"
"### **Key Points:**
1. Specify `externalIPs` in the service specification to allow external access, applicable with any service type."
"### **Key Points:**
1. Use the `kubectl create -f` command to install the PostgreSQL operator from the specified URL. The correct command is:  
   ```bash
   kubectl create -f https://operatorhub.io/install/postgresql-operator-dev4devs-com.yaml
   ```  
2. Avoid duplicating the `create -f` command; ensure to use it only once with the URL."
"### **Key Points:**
1. Create a `properties.yml` file with sensitive data, base64 encode it, and store it in a Kubernetes secret under the key `properties.yml`. 
   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: my-secrets
     labels:
       app: my-app
   data:
     properties.yml: dxnlcm5hbwu=
   ```
2. Mount the secret in your pod, allowing Kubernetes to decrypt and place the value at the specified path."
"### **Key Points:**
1. Use the Alpine version of the Traefik image for compatibility.  
   Code snippet: `kubectl exec -it _podname_ -- sh`"
"### **Key Points:**
1. Use the rewrite annotation to redirect requests to a specific path, with the code snippet: ```nginx.ingress.kubernetes.io/rewrite-target: /foo/bar/$1```.
2. Configure the Ingress resource with the appropriate backend service and path settings, as shown in the following snippet:
   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: Ingress
   metadata:
     name: <ingress-name>
     namespace: <namespace>
     annotations:
       nginx.ingress.kubernetes.io/rewrite-target: /foo/bar/$1
   spec:
     ingressClassName: nginx
     rules:
     - host: fool.example
       http:
         paths:
         - path: /(.*)
           pathType: Prefix
           backend:
             service:
               name: servicea
               port:
                 number: 8999
   ```"
"### **Key Points:**
1. Avoid using environment variables for the container port; specify the container port as an integer in the container port API object.
2. To connect using a different port number, set up a Kubernetes service that forwards the desired external port to the internal pod port, e.g., port 80 on the service to port 8080 in the pod."
"### **Key Points:**
1. Add a volume mount for database initialization by specifying `volumemounts` in your pod configuration: 
   ```yaml
   volumemounts:
     - name: initdb
       mountpath: /docker-entrypoint-initdb.d
   ```
2. Define a volume using a ConfigMap to provide the initialization script: 
   ```yaml
   volumes:
     - name: initdb
       configmap:
         name: initdb-config
   ```
3. Create a ConfigMap containing the SQL initialization script: 
   ```yaml
   apiVersion: v1
   kind: configmap
   metadata:
     name: initdb-config
   data:
     initdb.sql: |
         mysqlquery
   ```"
"### **Key Points:**
1. Copy and modify the `/var/snap/microk8s/current/certs/csr.conf.template` to include additional IP or DNS entries for generated certificates."
"### **Key Points:**
1. Convert both services to type NodePort to use them as backends for GCE Ingress. Verify allocation of NodePort with the command: ```kubectl get service web```.
2. Create an Ingress resource with two host paths to route traffic to the respective services."
"### **Key Points:**
1. Use `batch/v1` API version for cronjobs in GKE version 1.21 and later. 
   ```yaml
   apiVersion: batch/v1
   kind: CronJob
   ...
   ```
2. Check supported Kubernetes API versions with the following commands:
   ```bash
   kubectl api-resources
   kubectl api-versions
   ```
3. Use `kubectl explain` to get details about cronjob resources:
   ```bash
   kubectl explain cronjob
   ```"
"### **Key Points:**
1. Use Python with the Kubernetes library to calculate the total size of all Persistent Volumes (PVs) in bytes by extracting the storage capacity from each PV. 
   ```python
   from kubernetes import client, config
   import re 

   config.load_kube_config() 
   v1 = client.corev1api()
   multiplier_dict = {""k"": 1000, ""ki"": 1024, ""m"": 1000000, ""mi"": 1048576, ""g"": 1000000000, ""gi"": 1073741824}
   size = 0 

   for i in v1.list_persistent_volume(watch=False).items:
       x = i.spec.capacity[""storage""]
       y = re.findall(r'[a-zA-Z]+|\d+', x)
       try: 
           if y[1] in multiplier_dict: 
               size += multiplier_dict.get(y[1]) * int(y[0])
       except IndexError:
           size += int(y[0])
   
   print(""the size in bytes of all pv's is: "" + str(size))
   ```

2. Define a PersistentVolumeClaim (PVC) in YAML format to request storage, ensuring to specify the access modes and resource requests.
   ```yaml
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: pvc
   spec:
     accessModes:
       - ReadWriteOnce
     resources:
       requests:
         storage: 100m
   ```"
"### **Key Points:**
1. Set the image pull policy to always to ensure the latest image is pulled from the registry. Code snippet: ```imagePullPolicy: Always```"
"### **Key Points:**
1. Use the correct network tag instead of the instance group name for VM configurations. 
2. Update the node pool definition with a service account:
   ```hcl
   resource ""google_service_account"" ""sa-node"" {
     account_id   = ""sa-node""
     display_name  = ""sa-node""
   }
   resource ""google_container_node_pool"" ""primary_preemptible_nodes"" {
     name       = ""my-node-pool""
     location   = ""us-central1""
     cluster    = google_container_cluster.primary.name
     node_count = 1
     node_config {
       preemptible  = true
       machine_type = ""n1-standard-1""
       service_account = google_service_account.sa-node.email
   ```
3. Define a firewall rule using the service account as the source:
   ```hcl
   resource ""google_compute_firewall"" ""default"" {
     name    = ""test-firewall""
     network = google_compute_network.default.name
     allow {
       protocol = ""tcp""
       ports    = [""80"", ""8080"", ""1000-2000""]
     }
     source_service_accounts = [google_service_account.sa-node.email]
   }
   ```
4. Use a target service account for Rancher deployment instead of mixing target tags and source service accounts."
"### **Key Points:**
1. Helm dependencies can only install charts under the same release name; partial inclusion of a dependency chart is not possible without a post-renderer.
2. To use a subchart effectively, it must be designed to provide templates that can be conditionally called by the parent chart, which disables the subchart's normal output."
"### **Key Points:**
1. Verify the ingress controller status by running `kubectl describe ingress`. If you see create/add events, the ingress controller is running; otherwise, the GKE ingress controller may be disabled.
2. Check for version compatibility between Kubernetes server and kubectl by executing `kubectl version`.
3. Ensure the annotation for ingress class is set correctly: either remove `kubernetes.io/ingress.class` or set it to `kubernetes.io/ingress.class: gce`.
4. Set the service type for the Jenkins inception service to NodePort as per Google Cloud documentation."
"### **Key Points:**
1. Ensure to provide exactly one namespace when creating a namespace with kubectl, as it requires a name. 
2. Use a loop in bash or zsh to create multiple namespaces efficiently. Example for zsh: `foreach ns (ns1 ns2 ns3); kubectl create ns $ns; end`."
"### **Key Points:**
1. Use `get_namespaced_custom_object` to retrieve a namespace-scoped custom object with a synchronous HTTP request.
2. Replace the retrieved object using `replace_cluster_custom_object`."
"### **Key Points:**
1. Use Helm for templating Kubernetes deployments by creating a Helm chart with YAML files and Golang variable placeholders. Example: 
   ```yaml
   kind: deployment
   metadata:
     name: foo
   spec:
     replicas: {{ .values.replicacount }}
   ```
2. Define default values in a `values.yaml` file and override them using the `--set` command or external answer files. Example:
   ```bash
   helm install foo --set replicacount=42
   helm install foo -f ./dev.yaml
   ```
3. Utilize the Helm Secrets plugin to encrypt sensitive data with PGP keys, allowing for a structured repository setup for charts and environment-specific data. Example command:
   ```bash
   helm secrets upgrade foo --install -f ./values/foo/$environment/values.yaml -f ./values/foo/$environment/secrets.yaml
   ```
4. Use `envsubst` for simple variable substitution in templates. Example:
   ```bash
   goos=amd64 envsubst < mytemplate.tmpl > mydeployment.yaml
   ```
5. Kustomize offers a simpler alternative to Helm for managing Kubernetes configurations with a base and overlays structure, but lacks plugin support for secrets management. Example command:
   ```bash
   kustomize build someapp/overlays/production
   ```"
"### **Key Points:**
1. Use a regex pattern to identify and delete Kubernetes jobs matching the pattern: ```$pattern=""mailmigrationjob-id699""; kubectl get job | % {""$_"" -split "" ""} | select-string -pattern $pattern | %{ kubectl delete job $_ }```."
"### **Key Points:**
1. Use a ServiceMonitor resource to limit monitored services by excluding them from the selector. Example code snippet: 
   ```yaml
   selector:
     matchexpressions:
     - {key: foo, operator: notin}
     - {key: bar, operator: notin}
   ```
2. Specify namespaces to exclude from monitoring using the namespace selector. Example code snippet:
   ```yaml
   namespaceselector:
     matchnames:
     - kube-system
     - monitoring
   ```"
"### **Key Points:**
1. Create a new branch from 'develop', replace non-required files from 'main', and create a pull request from this new branch.
2. Create a new branch from 'main', apply changes of required files from 'develop', and create a pull request from this new branch.
3. To selectively merge files, checkout the target branch and use `git checkout` to pick specific files from the source branch, then push to the remote repository:
   ```bash
   git checkout main
   git checkout dev src/
   git push origin main
   ```"
"### **Key Points:**
1. Use the `-f` flag to specify a user-supplied values file when installing a Helm chart, which merges with the default values to generate the final manifest. Example command: ```helm install -f myvals.yaml ./mychart```.
2. Understand the precedence order for values in Helm: `values.yaml` in the chart > parent chart's `values.yaml` > user-supplied values file with `-f` > individual parameters with `--set`."
"### **Key Points:**
1. Change the context of the `tpl` function when looping over a list of environment variables. Use the following code snippet:
   ```yaml
   {{- range $k, $v := .values.environmentvariables }}
               - name: {{ quote $k }}
                 value: {{ tpl $v $ }}
   {{- end }}
   ```"
"### **Key Points:**
1. A deployment's rollout is triggered only when the pod template is changed, such as updating labels or container images. Other updates, like scaling, do not trigger a rollout.
2. To observe the events of a deployment update after changing the image, use the command: ```kubectl apply -f nginx-deploy.yml```."
"### **Key Points:**
1. Check firewall rules to ensure they are not blocking access to the required port.
2. Access the service using the NodePort of the service created during the ingress controller deployment.
3. Edit the ingress to include TLS and hosts properties with a self-signed certificate for `my.service.com`:
   ```yaml
   apiVersion: extensions/v1beta1
   kind: Ingress
   metadata:
     name: test-ingress
     annotations:
       ingress.kubernetes.io/rewrite-target: /
       kubernetes.io/ingress.class: nginx
   spec:
     tls:
     - hosts:
       - my.service.com
       secretName: tls-secret-for-my-service
     rules:
     - host: my.service.com
       http:
         paths:
         - path: /
           backend:
             serviceName: myservice
             servicePort: 9090
   ```"
"### **Key Points:**
1. Recreate the service by deleting and applying the YAML configuration: 
   ```bash
   kubectl delete -f service.yaml
   kubectl apply -f service.yaml
   ```"
"### **Key Points:**
1. Use `kubectl get` with a label selector to retrieve pod names in a script-friendly format: ```kubectl get pods -l app=guestbook,tier=frontend -o name```."
"### **Key Points:**
1. Use the `printf` command instead of `echo` to avoid newline characters when encoding passwords. Code snippet: `printf admin1234 | base64`."
"### **Key Points:**
1. To retrieve the project name from the instance metadata, use the following curl command:  
   ```bash
   curl ""http://metadata.google.internal/computemetadata/v1/project/project-id"" -h ""metadata-flavor: google""
   ```
2. To obtain the project number from the instance metadata, use this curl command:  
   ```bash
   curl ""http://metadata.google.internal/computemetadata/v1/project/numeric-project-id"" -h ""metadata-flavor: google""
   ```"
"### **Key Points:**
1. Use `imagePullPolicy: Always` to ensure that Kubernetes always pulls the latest image, which prevents using outdated images. 
   ```yaml
   imagePullPolicy: Always
   ```
2. Ensure your CI/CD system assigns a unique tag to each image build (e.g., based on commit ID or timestamp) to maintain image uniqueness and avoid manual image management.
   ```plaintext
   registry.example.com/name/image:tag
   ```"
"### **Key Points:**
1. Always specify matching versions of all Kubernetes components in your `go.mod` file.  
   Code snippet: 
   ```
   require (
       ...
       k8s.io/api v0.19.0
       k8s.io/apimachinery v0.19.0
       k8s.io/client-go v0.19.0
       ...
   )
   ```"
"### **Key Points:**
1. To check pods in a specific namespace, use: ```kubectl get pods -n your_namespace```.
2. To view all pods across all namespaces, use: ```kubectl get pods -a```.
3. To list all available namespaces, use: ```kubectl get namespaces```."
"### **Key Points:**
1. Add a container template to the agent for deploying with Helm, using the specified image and command.  
   ```groovy
   agent {
     kubernetes {
       containertemplate {
         name 'helm'
         image 'lachlanevenson/k8s-helm:v3.1.1'
         ttyenabled true
         command 'cat'
       }
     }
   }
   ```
2. Use the Helm container to execute the upgrade command in the deployment steps.  
   ```groovy
   container('helm') { 
     sh ""helm upgrade full-cover ./helm""
   }
   ```"
"### **Key Points:**
1. Delete the failing job named `kube-prometheus-stack-admission-patch` to resolve the `backofflimitexceeded` error and change the helm release status to deployed. 
2. Ensure to check the error logs for issues related to webhook configurations and resource availability."
"### **Key Points:**
1. Use `jsonpath` to extract specific data from Kubernetes resources by providing a jsonpath expression. Example: ```kubectl get pod my-pod -o jsonpath='{.status.phase}'```.
2. Use `--template` with Go templates for more flexible output formatting and data manipulation. Example: ```kubectl get pod my-pod --template='{{.metadata.name}} is in {{.status.phase}} phase'```."
"### **Key Points:**
1. To find the last cron job run name, use the command: ```kubectl get jobs | tail -1 | awk '{print $1}'```.
2. To check if the last job was successful, run: ```kubectl get job <job-name> -o=jsonpath='{.status.succeeded}'``` which should return a 1 if successful."
"### **Key Points:**
1. Create a single `values-int.yaml` file that includes settings for all sub-charts under top-level keys with the charts' names, as Helm does not support per-environment values files natively. 
   ```yaml
   global:
     env: int

   chart1:
     tickets:
       dynamicevents: { ... }

   chart2: { ... }
   ```
2. Ensure the `values-int.yaml` file is located in the deployment directory, as Helm will only read it from the current directory and not from sub-charts. 
   ```bash
   helm template . -f values-int.yaml
   ```"
"### **Key Points:**
1. To view detailed HTTP requests made by `kubectl`, use the verbose log level with the command: ```kubectl get po --v=7```.
2. To download a file from a Kubernetes API using `curl`, ensure you have proper authentication from your `.kube/config` file.
3. To build Kustomize manifests without applying them, use the command: ```kubectl kustomize github.com/minio/direct-csi```."
"### **Key Points:**
1. Create a ClusterRoleBinding to grant the default service account view permissions.  
   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1beta1
   kind: ClusterRoleBinding
   metadata:
     name: default-view
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: view
   subjects:
     - kind: ServiceAccount
       name: default
       namespace: default
   ```"
"### **Key Points:**
1. Allow UDP traffic for DNS by adding an inbound rule in the worker nodes security group for port 53.  
   ```hcl
   resource ""aws_security_group_rule"" ""eks-node-ingress-cluster-dns"" {
     description = ""allow pods dns""
     from_port                = 53
     protocol                 = 17
     security_group_id        = ""${aws_security_group.sg-eks-workernodes.id}""
     source_security_group_id = ""${aws_security_group.sg-eks-workernodes.id}""  
     to_port                  = 53
     type                     = ""ingress""
   }
   ```"
"### **Key Points:**
1. Use the connection URL for LDAP as `ldap://openldap.default:389`, replacing `default` with the appropriate namespace if OpenLDAP is deployed elsewhere."
"### **Key Points:**
1. Create a service account in the desired namespace by defining it in a YAML file and applying it with kubectl:
   ```yaml
   apiVersion: v1
   kind: ServiceAccount
   metadata:
     name: <name_of_service_account>
   ```
   ```bash
   kubectl create -f <path_to_file> --namespace=<namespace_name>
   ```

2. Retrieve the bearer token linked to the service account by finding the secret name and describing it:
   ```bash
   kubectl get secrets --namespace=<namespace_name>
   ```
   ```bash
   kubectl describe secret/<secret_name>
   ```

3. Configure the Kubernetes Python client to use the bearer token for authentication:
   ```python
   from kubernetes import client

   configuration = client.Configuration()
   configuration.api_key[""authorization""] = '<bearer_token>'
   configuration.api_key_prefix['authorization'] = 'bearer'
   configuration.host = 'https://<ip_of_api_server>'
   configuration.ssl_ca_cert = '<path_to_cluster_ca_certificate>'

   v1 = client.CoreV1Api(client.ApiClient(configuration))
   ```"
"### **Key Points:**
1. Use the Helm lookup function to retrieve ConfigMap items with a suffix of ""-version"" for volume definitions. 
   ```yaml
   {{- $lookups := lookup ""v1"" ""configmap"" .release.namespace """" }}
   volumes:
   {{- range $lookups.items }}
   {{- if hassuffix ""-version"" .metadata.name }}
     - name: {{ .metadata.name }}
       configmap:
         name: {{ .metadata.name }}
   {{- end }}
   {{- end }}
   ```

2. Mount the retrieved ConfigMap items in the container's volume mounts, specifying the mount path and subpath.
   ```yaml
   containers:
     - ...
       volumemounts:
   {{- range $lookups.items }}
   {{- if hassuffix ""-version"" .metadata.name }}
         - name: {{ .metadata.name }}
           mountpath: /tmp/{{ .metadata.name }}
           subpath: version
   {{- end }}
   {{- end }}
   ```

3. Remember to perform a Helm upgrade to update the chart whenever the resources in the cluster change, as the lookup does not automatically refresh."
"### **Key Points:**
1. Use the `index` function for array indexing in Helm charts to avoid nil pointer errors during template runs:  
   ```{{ $ingress := (index (lookup ""v1"" ""ingress"" ""mynamespace"" ""ingressname"").status.loadbalancer.ingress 0).hostname }}```

2. Implement a fallback mechanism for offline mode by asserting the lookup and providing a default value:  
   ```{{ $ingress := ""fake.example.com"" }}  
   {{ $maybelookup := (lookup ""v1"" ""ingress"" ""mynamespace"" ""ingressname"") }}  
   {{ if $maybelookup }}  
   {{   $ingress = (index $maybelookup.status.loadbalancer.ingress 0).hostname }}  
   {{ end }}```"
"### **Key Points:**
1. Modify the cronjob command to generate artificial load for VPA recommendations by using the following configuration:
   ```yaml
   apiVersion: batch/v1beta1
   kind: cronjob
   metadata:
     name: hello
   spec:
     schedule: ""*/1 * * * *""
     jobTemplate:
       spec:
         template:
           metadata:
             labels:
               app: hello
           spec:
             containers:
             - name: hello
               image: busybox
               imagePullPolicy: IfNotPresent
               args:
               - /bin/sh
               - -c
               - date; dd if=/dev/urandom | gzip -9 >> /dev/null
             restartPolicy: OnFailure
   ```
2. After running the modified cronjob, check the VPA recommendations using:
   ```bash
   kubectl describe vpa my-vpa
   ```"
"### **Key Points:**
1. Change the Kubernetes peer discovery port to 6443 in the RabbitMQ configuration to resolve connection issues. Code snippet: 
   ```yaml
   cluster_formation.k8s.port = 6443
   ```
2. Ensure the `cluster_formation.k8s.port` setting is added to the main `values.yaml` file instead of a custom values file for it to take effect."
"### **Key Points:**
1. Create an in-cluster configuration and clientset for Kubernetes interaction: 
   ```go
   config, err := rest.InClusterConfig()
   clientset, err := kubernetes.NewForConfig(config)
   ```
2. Retrieve and list all pods in the cluster, handling errors appropriately:
   ```go
   pods, err := clientset.CoreV1().Pods("""").List(context.TODO(), metav1.ListOptions{})
   ```
3. Check for a specific pod in the default namespace and handle potential errors:
   ```go
   _, err = clientset.CoreV1().Pods(""default"").Get(context.TODO(), ""example-xxxxx"", metav1.GetOptions{})
   ```
4. Implement a loop to repeat the pod retrieval every 10 seconds:
   ```go
   time.Sleep(10 * time.Second)
   ```"
"### **Key Points:**
1. To get detailed information about pods, use the command with the `-o wide` option: ```kubectl get pod -o wide```."
"### **Key Points:**
1. Create a missing service account to resolve the issue: ```kubectl create serviceaccount my-service-account```."
"### **Key Points:**
1. Ensure the `no_proxy` variable includes `.svc` and `.cluster.local` to prevent validation webhook requests from being routed through a proxy. 
   ```bash
   export no_proxy=.svc,.cluster.local
   ```
2. Modify the `/etc/kubernetes/manifests/kube-apiserver.yaml` file to automatically recreate the Kubernetes API server pod after updating the `no_proxy` configuration.
   ```yaml
   # Edit the kube-apiserver.yaml file as needed
   ```"
"### **Key Points:**
1. Initialize the Kubernetes cluster with the specified pod network CIDR and API server addresses using the command:  
   ```bash
   kubeadm init --pod-network-cidr=<ip-range> --apiserver-advertise-address=0.0.0.0 --apiserver-cert-extra-sans=<private_ip>[,<public_ip>,...]
   ```
2. Update the `.kube/config` file to replace the private IP with the public IP if using `kubectl` from a remote location.  
3. On the worker node, forward the private IP of the master node to its public IP before running `kubeadm join` with the command:  
   ```bash
   sudo iptables -t nat -A OUTPUT -d <private ip of master node> -j DNAT --to-destination <public ip of master node>
   ```
4. Ensure to forward worker private IPs on the master node similarly if they are affected by NAT issues."
"### **Key Points:**
1. A Kubernetes service is an abstract resource that exposes an application running on a set of pods as a network service, without requiring modifications to the application for service discovery."
"### **Key Points:**
1. Existing Docker images built with `docker build` are compatible with all CRI implementations, ensuring they work the same way across different runtimes.
2. If you are not using `/var/run/docker.sock`, you can continue using your existing setup without issues."
"### **Key Points:**
1. Specify the shell for command execution in the YAML configuration using a lifecycle prestop hook.  
   Code snippet: 
   ```yaml
   lifecycle:
     prestop:
       exec:
         command: [""/bin/sh"", ""-c"", ""echo pre stop!""]
   ```  
2. Note that a prestop hook is executed only when a pod is terminated, not when it is completed."
"### **Key Points:**
1. Enable service annotations for the Istio ingress gateway by configuring the `serviceannotations` field in the `istiooperator` resource.  
   ```yaml
   serviceannotations:
     manifest-generate: ""testserviceannotation""
   ```
2. Set the `externaltrafficpolicy` to `local` for the ingress gateway service to preserve the client source IP.  
   ```yaml
   externaltrafficpolicy: local
   ```
3. Configure sysctls in the security context of the ingress gateway to adjust the local port range.  
   ```yaml
   sysctls:
   - name: ""net.ipv4.ip_local_port_range""
     value: ""80 65535""
   ```"
"### **Key Points:**
1. In Helm v3, release information is stored in the same namespace as the release itself, unlike Helm v2 where it was stored in the kube-system namespace.
2. Helm v3 uses secrets as the default storage driver for release information instead of configmaps."
"### **Key Points:**
1. Update the ingress file with the correct Minikube IP and ensure to use HTTP instead of HTTPS, along with the correct `keycloak_hostname` value. 
   ```yaml
   apiVersion: v1
   kind: service
   metadata:
     name: keycloak
     labels:
       app: keycloak
   spec:
     ports:
     - name: http
       port: 8080
       targetPort: 8080
     selector:
       app: keycloak
     type: LoadBalancer
   ---
   apiVersion: apps/v1
   kind: deployment
   metadata:
     name: keycloak
     labels:
       app: keycloak
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: keycloak
     template:
       metadata:
         labels:
           app: keycloak
       spec:
         containers:
         - name: keycloak
           image: quay.io/keycloak/keycloak:20.0.3
           args: [""start-dev""]
           env:
           - name: keycloak_admin
             value: ""admin""
           - name: keycloak_admin_password
             value: ""admin""
           - name: kc_proxy
             value: ""edge""
           ports:
           - name: http
             containerPort: 8080
           readinessProbe:
             httpGet:
               path: /realms/master
               port: 8080
   ```

2. Use the provided YAML to create a LoadBalancer service for Keycloak, allowing access without ingress configuration. Check the external IP with `kubectl get svc -n <namespace-name>` and open it in a browser.
   ```yaml
   apiVersion: v1
   kind: service
   metadata:
     name: keycloak
     labels:
       app: keycloak
   spec:
     ports:
     - name: http
       port: 8080
       targetPort: 8080
     selector:
       app: keycloak
     type: LoadBalancer
   ```"
"### **Key Points:**
1. Use a relative path for local file transfers with `kubectl cp` to avoid issues with pod name interpretation. Example command: `kubectl cp default/postgresl-7c8b9-qs67z:/home/backup/db ./desktop/mydb1.dmp`.
2. If you encounter the message ""tar: removing leading '/' from member names,"" verify if the file was downloaded successfully."
"### **Key Points:**
1. To select a specific service account for pods, use the command: ```kubectl get pods --field-selector=spec.serviceaccountname=""default""```.
2. The `--field-selector` supports limited fields for selection, including: `metadata.name`, `metadata.namespace`, `spec.nodename`, `spec.restartpolicy`, `spec.schedulername`, `spec.serviceaccountname`, `status.phase`, `status.podip`, and `status.nominatednodename`."
"### **Key Points:**
1. Use a StatefulSet to manage database instances, which automatically generates different Persistent Volume Claims (PVCs) for each pod. The PVC naming convention is `<pvc_template_name>-<statefulset_name>-<podnumber>`.  
   Code snippet: 
   ```yaml
   volumeclaimtemplates:
   - metadata:
       name: mysql-pv-claim
     spec:
       accessmodes: [ ""readwriteonce"" ]
       storageclassname: standard
       resources:
         requests:
           storage: 20Gi
   ```

2. Define a Service for the StatefulSet to expose the MySQL database, ensuring it has a ClusterIP of none for headless service functionality.  
   Code snippet: 
   ```yaml
   kind: service
   metadata:
     name: wordpress-mysql
     labels:
       app: wordpress
   spec:
     ports:
       - port: 3306
     selector:
       app: wordpress
       tier: mysql
     clusterip: none
   ```"
"### **Key Points:**
1. Check the spec schema for configurations in Kubernetes resources: [Spec Schema Documentation](https://cloud.google.com/container-engine/docs/spec-schema).
2. Set the `runAsUser` in the security context of a pod to specify the user ID for the container: 
   ```yaml
   securityContext:
     runAsUser: 41
   ```"
"### **Key Points:**
1. Ensure proper indentation in YAML files used by Helm to avoid errors; specifically, maintain an indent of 4 spaces. 
2. Include a whitespace before the closing brace `}` when using the `{{ .files.get ""settings.yaml"" | indent 4 }}` syntax to ensure correct parsing."
"### **Key Points:**
1. Enable automount service account token in the pod spec by adding `automountServiceAccountToken: true` to your deployment configuration to resolve the error. 
   ```yaml
   automountServiceAccountToken: true
   ```"
"### **Key Points:**
1. Expose only the application listening port (port 80) for your container and service, without exposing port 443. 
   ```yaml
   ports:
     - containerPort: 80
   ```
2. Remove the annotation for backend protocol and set the service port to 80.
   ```yaml
   annotations:
     nginx.ingress.kubernetes.io/backend-protocol: ""http""
   servicePort: 80
   ```
3. Ensure the DNS name is included in the SSL certificate."
"### **Key Points:**
1. Use a DaemonSet to ensure one instance of the pod runs on every node in the cluster to prevent dropped packets when using `externalTrafficPolicy: local`. 
   ```yaml
   apiVersion: apps/v1
   kind: DaemonSet
   metadata:
     name: my-daemonset
   spec:
     selector:
       matchLabels:
         app: my-app
     template:
       metadata:
         labels:
           app: my-app
       spec:
         containers:
         - name: my-container
           image: my-image
   ```

2. Configure a load balancer to distribute traffic across all nodes and preserve the source IP of the original client to ensure proper load distribution and avoid traffic loss if a node goes down.
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: my-loadbalancer
   spec:
     type: LoadBalancer
     externalTrafficPolicy: Local
     selector:
       app: my-app
     ports:
     - port: 80
       targetPort: 8080
   ```"
"### **Key Points:**
1. Use `kubectl create` to create a replication controller with multiple ports specified for the container. Example configuration includes:
   ```yaml
   ports:
   - containerPort: 7000
     name: intra-node
   - containerPort: 7001
     name: tls-intra-node
   - containerPort: 7199
     name: jmx
   - containerPort: 9042
     name: cql
   ```"
"### **Key Points:**
1. Use the command to resolve your PipelineRun by event ID:  
   ```kubectl -n <your-namespace> get pr -l triggers.tekton.dev/tekton-eventid=<your-event-id>```.
2. Extract the status column or access the status condition using jsonpath:  
   ```jsonpath=status.conditions[0].type```."
"### **Key Points:**
1. No `--name` option is needed for Helm upgrade; use the syntax: ```helm upgrade [release] [chart]```."
"### **Key Points:**
1. Use the service DNS name instead of the server IP and port number to test the application, formatted as `my-svc.my-namespace.svc.cluster.local`."
"### **Key Points:**
1. Environment variables passed through the `env` field of a container are accessible in scripts run within the container unless explicitly unset. Example code snippet:
   ```yaml
   env:
   - name: auth_token
     valueFrom:
       secretKeyRef:
         name: test-secret
         key: servicetoken
   ```
2. To create a Kubernetes secret and use it as an environment variable in a pod, define the secret and reference it in the pod's configuration. Example code snippets:
   ```yaml
   apiVersion: v1
   kind: secret
   metadata:
     name: test-secret
   type: opaque
   data:
     servicetoken: mtizndu2nzg5mao=  # base64 encoded string: ""1234567890""
   ---
   apiVersion: v1
   kind: pod
   metadata:
     name: test
   spec:
     containers:
     - args:
       - echo
       - hello
       - $(auth_token)
       name: test
       image: centos:7
     restartPolicy: Never
   ```"
"### **Key Points:**
1. Use AWS SDK for Java 2.x to leverage the default credential provider chain, which includes web identity token support, reducing configuration complexity.
2. For services using AWS SDK v1, explicitly configure the web identity token provider with the following code snippet:
   ```java
   @bean
   public awscredentialsprovider awscredentialsprovider() {
       if (system.getenv(""aws_web_identity_token_file"") != null) {
           return webidentitytokencredentialsprovider.builder().build();
       }    
       return new defaultawscredentialsproviderchain();
   }
   ```
3. Always use the latest AWS SDK BOM dependency to ensure compatibility across modules."
"### **Key Points:**
1. Create a Role in Kubernetes for a specific namespace with full permissions: 
   ```yaml
   kind: Role
   apiVersion: rbac.authorization.k8s.io/v1
   metadata:
     namespace: my-namespace
     name: full-namespace
   rules:
   - apiGroups: [""*""] 
     resources: [""*""]
     verbs: [""*""]
   ```

2. Create a RoleBinding to bind the Role to a user group:
   ```bash
   kubectl create rolebinding my-namespace-binding --role=full-namespace --group=namespacegroup --namespace=my-namespace
   ```

3. Alternatively, define a RoleBinding in a YAML file:
   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: RoleBinding
   metadata:
     name: my-namespace-binding
     namespace: mynamespace
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: Role
     name: full-namespace
   subjects:
   - apiGroup: rbac.authorization.k8s.io
     kind: Group
     name: namespacegroup
   ```

4. Configure a ConfigMap to map users to groups:
   ```yaml
   mapUsers:
   - userarn: arn:aws:iam::573504862059:user/abc-user  
     username: abc-user
     groups:
       - namespacegroup
   ```"
"### **Key Points:**
1. The `spec.container.env.value` is defined as a string in Kubernetes, requiring conversion to boolean within the container. 
2. Reference for the environment variable definition can be found at: [Kubernetes API Reference](https://kubernetes.io/docs/api-reference/v1.6/#envvar-v1-core)."
"### **Key Points:**
1. Configure a readiness probe to check if a container is ready to serve traffic by executing a command. Use the following configuration in your pod definition:
   ```yaml
   readinessProbe:
     exec:
       command:
       - cat
       - /tmp/healthy
     initialDelaySeconds: 5
     periodSeconds: 5
   ```

2. To test the readiness probe, create the file `/tmp/healthy` in the container to indicate readiness:
   ```bash
   kubectl exec -it nginx -- bash
   touch /tmp/healthy
   exit
   ```

3. Verify the readiness status of the pod using:
   ```bash
   kubectl get pods nginx
   ```

4. To simulate unavailability, remove the `/tmp/healthy` file and check the readiness status again:
   ```bash
   kubectl exec -it nginx -- bash
   rm /tmp/healthy
   exit
   ```"
"### **Key Points:**
1. Add a liveness probe to check application health with the following configuration:
   ```yaml
   livenessProbe:
     httpGet:
       path: /healthz
       port: 8080
     initialDelaySeconds: 3
     periodSeconds: 3
   ```"
"### **Key Points:**
1. A pod is the smallest deployable unit in Kubernetes, containing one or more coupled containers that share the same network and resources. 
2. In Kubernetes, the hierarchy of deployment is: deployment manages replicasets, which manage pods, which in turn manage containers. 
3. The main components of Kubernetes include the control plane (api server, scheduler, controllers, etcd) and worker nodes (kubelet, kube-proxy, container runtime, and pods)."
"### **Key Points:**
1. Resource metrics for CPU or memory usage are provided by kubelet and collected by metric-server. 
2. Packets-per-second and requests-per-second metrics require a non-official custom metrics API, as there is no official provider for these metrics."
"### **Key Points:**
1. Access modes in Kubernetes Persistent Volumes (PVs) are matching criteria rather than enforced constraints; they describe the capabilities of the volume. For example, an NFS PV may support multiple access modes but can be exported as read-only. 
   
2. When creating a PersistentVolumeClaim (PVC), specify the desired access modes as a request. The claim can match a PV with broader access modes. Example code snippet:
   ```yaml
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: example-pvc
   spec:
     accessModes:
       - ReadOnlyMany
       - ReadWriteMany
       - ReadWriteOnce
     resources:
       requests:
         storage: 1Gi
   ```

3. Direct matches for PVCs are prioritized based on access modes and size; the volume must match or exceed the requested access modes and size. If a claim requests `ReadWriteOnce`, it can match an NFS PV that supports `ReadWriteOnce`, `ReadOnlyMany`, and `ReadWriteMany`. 

4. The storage provider is responsible for runtime errors due to invalid access mode usage. For example, if a PVC is marked as read-only, it should not attempt to use the volume's read-write capabilities. Example code snippet:
   ```yaml
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: example-pvc-rwo-rom
   spec:
     accessModes:
       - ReadOnlyMany
       - ReadWriteOnce
     resources:
       requests:
         storage: 1Gi
   ```"
"### **Key Points:**
1. Change the eviction hard thresholds in the kubelet configuration file to adjust resource limits. Update the `evictionhard` settings in `/var/lib/kubelet/config.yaml` as follows:
   ```yaml
   evictionhard:
     imagefs.available: 1%
     memory.available: 100mi
     nodefs.available: 1%
     nodefs.inodesfree: 1%
   ```
2. Use the `--experimental-allocatable-ignore-eviction` flag to completely disable eviction in the kubelet."
"### **Key Points:**
1. Use Helm templating to create environment-specific configurations in your YAML files, allowing for dynamic replacements based on the environment. Code snippet: 
   ```yaml
   key:  /xxx/{{ .values.environment }}/env_var_1
   ```
2. Provide environment-specific YAML files (e.g., `production.yaml`, `staging-1.yaml`) and use the `-f` flag during deployment to specify which environment to use. Code snippet: 
   ```bash
   helm install the-app . -f staging-1.yaml
   ```"
"### **Key Points:**
1. Use the command to retrieve the names of the pods associated with a service's endpoints: 
   ```bash
   kubectl get endpoints my-service -o json | jq -r 'select(.subsets != null) | select(.subsets[].addresses != null) | .subsets[].addresses[].targetref.name'
   ```
2. Run a script to check the status of each pod and confirm if they are ready:
   ```bash
   #!/usr/bin/env bash
   for podname in $(kubectl get endpoints my-service -o json | jq -r 'select(.subsets != null) | .subsets[].addresses[].targetref.name')
   do
       kubectl get pods -n demo $podname -o json | jq -r 'select(.status.conditions[].type == ""ready"") | .status.conditions[].type' | grep -x ready
   done
   ```"
"### **Key Points:**
1. Use `crictl` instead of Docker commands for managing containers on GKE nodes starting with version 1.19. 
   ```bash
   crictl images
   ```"
"### **Key Points:**
1. Ensure that the host port is correctly configured to avoid failed scheduling due to lack of free ports. The configuration to check is: `hostport: 8080`."
"### **Key Points:**
1. Use a rolling update strategy in your deployment configuration with `maxsurge` and `maxunavailable` set to 1 to control the rollout process.
   ```yaml
   strategy:
     rollingUpdate:
       maxSurge: 1
       maxUnavailable: 1
   ```
2. Implement a script to execute multiple `kubectl rollout restart` commands in parallel to test the deployment behavior.
   ```bash
   for var in 1..5; do
       kubectl rollout restart deployment webapp1 > /dev/null 2>&1 &
   done
   ```"
"### **Key Points:**
1. Add `externalIPs` to the service configuration in `service.yaml` for your pods (api and nginx).  
   Code snippet:  
   ```yaml
   externalIPs:
     - 10.10.10.130
   ```"
"### **Key Points:**
1. Create a Kubernetes secret for TLS with the specified key and certificate files using the command: 
   ```bash
   kubectl create secret generic production-tls --save-config --dry-run=client --from-file=./tls.key --from-file=./tls.crt -o yaml | kubectl apply -f -
   ```"
"### **Key Points:**
1. Use the `nginx.ingress.kubernetes.io/load-balance: ewma` annotation for effective load balancing and resource savings. 
   ```nginx.ingress.kubernetes.io/load-balance: ewma```
2. Avoid using `nginx.ingress.kubernetes.io/upstream-hash-by: ewma` as it relies on consistent hashing, which may not distribute requests evenly. 
   ```nginx.ingress.kubernetes.io/upstream-hash-by: ewma```"
"### **Key Points:**
1. Retrieve the first pod created with a name containing ""service-job"" by sorting pods by creation timestamp:  
   ```kubectl get pods --sort-by=.metadata.creationtimestamp --template '{{range .items}}{{.metadata.name}}{{&quot;\n&quot;}}{{end}}' | grep service-job- | head -1```"
"### **Key Points:**
1. To access a Minikube service from a bash prompt, use the command: ```curl $(minikube service hello-minikube --url)```.
2. For Windows command prompt users, first run: ```minikube service hello-minikube --url```, copy the output, and then execute: ```curl <output>```."
"### **Key Points:**
1. Configure an Ingress resource with specific annotations to disable SSL redirection and specify the ingress class: 
   ```yaml
   kubernetes.io/ingress.class: nginx
   nginx.ingress.kubernetes.io/ssl-redirect: ""false""
   ```
2. Define the Ingress rules to route traffic to the appropriate backend services without using a rewrite annotation, ensuring the full URI is preserved:
   ```yaml
   paths:
   - path: /
     backend:
       servicename: atsweb
       serviceport: 80
   - path: /api/
     backend:
       servicename: atsapi
       serviceport: 80
   ```"
"### **Key Points:**
1. Define the application configuration in the `_halpers.tpl` file to set up receivers, processors, and exporters for traces, metrics, and logs. 
   ```yaml
   {{- define ""appname.emsconfig"" -}}
   receivers:
     otlp:
       protocols:
         http:
   processors:
     batch:
   exporters:
     otlp/ems:
       endpoint: {{ .values.externalipservice.ip }}:{{ .values.externalipservice.port }}
   service:
     pipelines:
       traces:
         receivers: [otlp]
         processors: [batch]
         exporters: [otlp/ems]
       metrics:
         receivers: [otlp]
         processors: [batch]
         exporters: [otlp/ems]
       logs:
         receivers: [otlp]
         processors: [batch]
         exporters: [otlp/ems]
   {{- end }}
   ```

2. Configure the `values.yaml` file to specify the external IP and port for the service.
   ```yaml
   externalipservice:
     ip: 1.1.1.1
     port: 80
   ```

3. Create a `configmap.yaml` file to include the application configuration from `_halpers.tpl`.
   ```yaml
   apiVersion: v1
   kind: configmap
   metadata:
     name: simple-demo
   data:
     message: |-
       {{ include ""appname.emsconfig"" . | nindent 4}}
   ```"
"### **Key Points:**
1. Enable the HTTP load balancing add-on for the cluster using the command:  
   ```bash
   gcloud container clusters update [cluster_name] --update-addons httploadbalancing=enabled
   ```
2. Ensure backend services for multi-cluster ingress are configured with the same name, namespace, type (NodePort), and port number across all clusters."
"### **Key Points:**
1. Install the gcloud tool in your container to facilitate authentication with GKE. Reference the Dockerfile for gcloud installation: [gcloud Dockerfile](https://github.com/googlecloudplatform/cloud-builders/blob/master/gcloud/dockerfile).
2. Authenticate to GKE using the following command after installing gcloud: 
   ```bash
   gcloud container clusters get-credentials cents-ideas --zone europe-west3-a --project cents-ideas
   ```
3. Use the following Cloud Build step to run your commands:
   ```yaml
   - name: <link to your container>
     entrypoint: /bin/sh
     args:
     - -c
     - |
       gcloud container clusters get-credentials cents-ideas --zone europe-west3-a --project cents-ideas
       bazel run //:kubernetes.apply
   ```"
"### **Key Points:**
1. To access the output of a previous step in GitHub Actions, use the expression: ```${{ steps.get_service_list.outputs.result }}```.
2. To conditionally run a step based on the output of a previous step, use the following format: 
   ```yaml
   - name: create elb service
     if: steps.get_service_list.outputs.result != 'services/nginx-service'
     uses: consensys/kubernetes-action@master
     env:
       kube_config_data: ${{ secrets.kube_config_data }}
     with:
       args: create -f nginx_loadbalancer.yaml
   ```"
"### **Key Points:**
1. Configure the GitLab shell service type and node port in the `values.yaml` file for the official GitLab cloud-native helm chart: 
   ```yaml
   gitlab-shell:
     service:
       type: nodeport
       nodeport: 30022
   ```
2. Ensure all related objects in a larger-scale deployment are in the same namespace, as the GitLab shell subchart does not allow manual namespace configuration. Use the `helm install -n` option to set a different namespace for the entire deployment, not per component."
"### **Key Points:**
1. Use command substitution in shell or bash for dynamic command execution. Ensure to escape the dollar sign in Kubernetes manifests. Code snippet: `echo ""success $$(date)"" >> ...`"
"### **Key Points:**
1. Define a default backend in your Ingress definition to ensure only your specified backend is used, by adding the `spec.backend` field:
   ```yaml
   backend:
     servicename: my-bsc-service
     serviceport: 80
   ```
2. Example of a complete Ingress definition with the default backend:
   ```yaml
   apiVersion: extensions/v1beta1
   kind: Ingress
   metadata:
     name: my-bsc-ingress
   spec:
     backend:
       servicename: my-bsc-service
       serviceport: 80
     rules:
     - http:
         paths:
         - path: /*
           backend:
             servicename: my-bsc-service
             serviceport: 80
   ```"
"### **Key Points:**
1. Use subpath with expanded environment variables for mounting paths in Kubernetes (v1.17+). The configuration includes defining an environment variable and using it in the volume mount.  
   ```yaml
   containers:
   - env:
     - name: my_pod_name
       valueFrom:
         fieldRef:
           fieldPath: metadata.name
     volumeMounts:
     - mountPath: /cache
       name: shared-folder
       subPathExpr: $(my_pod_name)
   ```"
"### **Key Points:**
1. Use Helm's built-in properties like revision or name for unique names instead of generatename.  
   Reference: [Helm Issue #3348](https://github.com/helm/helm/issues/3348#issuecomment-482369133)"
"### **Key Points:**
1. Startup probe is an alpha feature in Kubernetes version 1.16, which may not be documented in GCP docs. 
2. On GKE, you can only create clusters up to version 1.14 and do not have access to the Kubernetes master.
3. With kubeadm, you have full control and can configure your Kubernetes environment as needed."
"### **Key Points:**
1. Remove the `rewrite-target` annotation from the ingress resource to prevent requests from being rewritten incorrectly, which currently targets a non-existent capture group. 
   ```nginx.ingress.kubernetes.io/rewrite-target: /$2```
2. Alternatively, modify the `rewrite-target` annotation to a valid path that supports your rewrite, such as `/`.
   ```nginx.ingress.kubernetes.io/rewrite-target: /```"
"### **Key Points:**
1. Set the `kubectl_redirect_url` for OIDC login authorization, typically in the format: ```http://localhost:port/callback```, and register it with your OIDC provider.
2. Use a `user_prefix` to prepend to user claims to avoid conflicts, with the default format being `issuer_uri#user`. You can disable it by setting `user_prefix` to `-`.
3. Specify the `userclaim` in the token under the claim name configured in `spec.authentication.oidc.userclaim` in the client configuration file.
4. Define the `cloudconsoleredirecturi` for OIDC, for example, for Google: ```https://console.cloud.google.com/kubernetes/oidc```.
5. To store `kind:clientconfig` with OIDC-based authentication, write to a file or use cloud storage for caching."
"### **Key Points:**
1. Increase Minikube's disk allocation to resolve image pull issues by using the command: ```minikube start --memory=8192 --cpus=4 --disk-size=50g```."
"### **Key Points:**
1. Use `curl` to check the service's ping response by accessing the service's cluster IP: ```curl user-service-cluster-ip/ping```.
2. Alternatively, access the service using its service name within the UI pod: ```curl http://user-service-svc/ping```."
"### **Key Points:**
1. Configure PostgreSQL database settings in the `values.yaml` file or a separate YAML file using the `-f` option during Helm install, including parameters like `postgresqldatabase` and `postgresqlpassword`.  
   ```yaml
   postgresql:
     postgresqldatabase: stackoverflow
     postgresqlpassword: enterimagedescriptionhere
   ```
2. Optionally, set a PostgreSQL user with admin-level privileges by including `postgresqluser` in the configuration.  
   ```yaml
   - name: pguser
     value: {{ .values.postgresql.postgresqluser }}
   ```"
"### **Key Points:**
1. Define chart dependencies in `chart.yaml` (Helm 3) or `requirements.yaml` (Helm 2) with the full chart name, repository URL, version, and local alias:
   ```yaml
   dependencies:
     - name: ibm-db2oltp-dev
       repository: http://localhost:10191
       version: 0.1.0
       alias: db1inst
     - name: ibm-db2oltp-dev
       repository: http://localhost:10191
       version: 0.1.0
       alias: db2inst
   ```
2. Configure values for the parent chart and its dependencies in `parentchart/values.yaml`:
   ```yaml
   someparentchartvaluex: x
   someparentchartvaluey: y

   db1inst:
     instname: user1
     db2inst: password1

   db2inst:
     instname: user2
     db2inst: password2
   ```"
"### **Key Points:**
1. Delete the current Alertmanager secret in the monitoring namespace:  
   ```kubectl delete secret alertmanager-prometheus-prometheus-oper-alertmanager -n monitoring```.
   
2. Create an `alertmanager.yaml` configuration file with the necessary alerting settings, including email configurations for notifications.

3. Create a new secret with the same name as the old one using the new configuration file:  
   ```kubectl create secret generic alertmanager-prometheus-prometheus-oper-alertmanager -n monitoring --from-file=alertmanager.yaml```."
"### **Key Points:**
1. Use `kubectl top` to check resource usage, ensuring that the metrics-server is running. 
2. For additional metrics, implement Prometheus along with tools like Node Exporter and cAdvisor to gather comprehensive data."
"### **Key Points:**
1. Use `kubectl version` to check connectivity to the API server. 
2. Use `kubectl cluster-info` for additional information about the cluster."
"### **Key Points:**
1. Configure a PersistentVolumeClaim (PVC) for Redis in the deployment definition to enable dynamic provisioning:
   ```yaml
   apiVersion: v1
   kind: persistentvolumeclaim
   metadata:
     name: your-mysql-pv-claim
     labels:
       app: redis
   spec:
     storageClassName: your-storage-class
     accessModes:
       - ReadWriteOnce
     resources:
       requests:
         storage: 8Gi
   ```

2. Add the PVC to the Redis deployment configuration under the volumes section:
   ```yaml
   volumes:
   - name: your-mysql-persistent-storage
     persistentVolumeClaim:
       claimName: your-mysql-pv-claim
   ```

3. Ensure that a StorageClass is defined for dynamic provisioning; if not, it must be created manually. Also, specify the mount path for the volume in the deployment configuration.

4. To enable dynamic storage provisioning, the cluster administrator must enable the `defaultStorageClass` admission controller on the API server by including it in the `--enable-admission-plugins` flag:
   ```
   --enable-admission-plugins=...,defaultStorageClass,...
   ```"
"### **Key Points:**
1. Upgrade to containerd version 1.5.2 to resolve IPv6 port-forwarding issues. 
   ```bash
   # Ensure you are using containerd version 1.5.2 or later
   ```
2. Modify the /etc/hosts file to resolve localhost to ::1 to fix DNS issues when port-forwarding.
   ```bash
   sed -i 's/::1 ip6-localhost ip6-loopback/::1 localhost ip6-localhost ip6-loopback/' /etc/hosts
   ```
3. Verify that your container runtime supports IPv6 (tcp6 binding).
   ```bash
   # Check container runtime documentation for IPv6 support
   ```"
"### **Key Points:**
1. To resolve the GKE quota issue, delete the existing resource quota: ```kubectl delete resourcequota gke-resource-quotas -n default```.
2. For recurring issues at scale, consider contacting GKE/GCP support or exploring alternatives outside of GKE."
"### **Key Points:**
1. Access elements of the `s3catalogs` array in `values.yaml` using the `index` function in `deployment.yaml` for catalog names and URLs. 
   ```yaml
   - name: s3catalogs__catalogs__catalogname_0
     value: ""{{ index .values.s3catalogs.catalogs 0 ""catalogname"" }}""
   - name: s3catalogs__catalogs__url_0
     value: ""{{ index .values.s3catalogs.catalogs 0 ""url"" }}""
   - name: s3catalogs__catalogs__catalogname_1
     value: ""{{ index .values.s3catalogs.catalogs 1 ""catalogname"" }}""
   - name: s3catalogs__catalogs__url_1
     value: ""{{ index .values.s3catalogs.catalogs 1 ""url"" }}""
   ```
2. Define the `s3catalogs` structure in `values.yaml` to include catalog names and URLs.
   ```yaml
   s3catalogs:
     catalogs:
       - catalogname: botvoice
         url: ""http://sandbox5.foo.com""
       - catalogname: wrongvoice
         url: ""http://sandbox5.bar.com""
   ```"
"### **Key Points:**
1. The Kubernetes service `service/kubernetes` automatically recovers due to a controller in the control plane that manages its state."
"### **Key Points:**
1. Set `.spec.minReadySeconds` to define the minimum number of seconds a pod must be ready without crashing to be considered available. Default is 0. 
   ```yaml
   spec:
     minReadySeconds: <number_of_seconds>
   ```
2. Use `initialDelaySeconds` to specify the delay before liveness or readiness probes are initiated after the container starts. 
   ```yaml
   readinessProbe:
     initialDelaySeconds: <number_of_seconds>
   ```"
"### **Key Points:**
1. Identify and delete broken release configmaps in Tiller's namespace using the following commands:
   ```bash
   $ kubectl get cm --all-namespaces -l owner=tiller
   $ kubectl delete cm nginx-ingress.v1 -n kube-system
   ```
2. Manually delete all release objects (deployments, services, ingresses, etc.) and reinstall the release using Helm.
3. If issues persist, download a newer release of Helm (e.g., v2.14.3) and update/reinstall Tiller."
"### **Key Points:**
1. Update the pull credentials on the cluster to resolve authentication or authorization errors with the registry. 
2. Ensure the Docker client does not downgrade to an unsupported API version to avoid receiving unsupported repository request errors from Artifactory."
"### **Key Points:**
1. Rename all labels except for the version of the ingress-nginx to avoid errors, ensuring consistency across resources. Example labels include:
   ```yaml
   app.kubernetes.io/name: ingress-nginx{{ ingress_type }}
   app.kubernetes.io/instance: ingress-nginx{{ ingress_type }}
   app.kubernetes.io/version: 1.0.0
   app.kubernetes.io/component: admission-webhook{{ ingress_type }}
   ```
2. Configure the validating webhook and service with consistent labels and settings to ensure proper functionality. Example configuration:
   ```yaml
   apiVersion: admissionregistration.k8s.io/v1
   kind: validatingwebhookconfiguration
   ...
   apiVersion: v1
   kind: service
   ...
   ```"
"### **Key Points:**
1. Use the `--ignore-not-found=true` flag to avoid errors when deleting deployments that may not exist. Code snippet: ```kubectl delete deployments --ignore-not-found=true -n ${namespace} --all --grace-period=10```."
"### **Key Points:**
1. Remove curly braces and use a period to delimit the `udp-services-configmap` argument in the Helm command:  
   ```--set controller.extraargs.udp-services-configmap=default/cm-udp-services```.
2. If using a `values.yaml` file, configure it as follows:  
   ```yaml
   controller:
     extraargs:
       udp-services-configmap: default/cm-udp-services
   ```"
"### **Key Points:**
1. Ensure you are using the correct Kubernetes context by checking your configuration with `kubectl config view`, and switch to the desired context if necessary.
2. Follow the instructions provided in the MicroK8s documentation to access the dashboard: [MicroK8s Dashboard Documentation](https://microk8s.io/docs/addon-dashboard)."
"### **Key Points:**
1. To query DNS for service location, use the format: ```my-svc.my-namespace.svc.cluster.local```.
2. To abstract port numbers and retrieve both port numbers and CNAME, query SRV records using: ```_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local```.
3. For a specific example, query Redis service in the default namespace with: ```_redis-client._tcp.redis-service.default.svc.cluster.local```.
4. Querying SRV records is more reliable than using environment variables for service location updates during pod lifetime."
"### **Key Points:**
1. To use the JSON merge patch strategy with the JavaScript Kubernetes client, add a header in the options: ```const options = { ""headers"": { ""content-type"": patchutils.patch_format_json_merge_patch } }```.
2. To check if a service account has annotations before using the JSON patch strategy, use the following command: ```kubectl get sa default -n somenamespace -o json | jq -e '.metadata | has(""annotations"")'```.
3. For adding an annotation using the JSON patch strategy, use this command: ```kubectl patch sa default -n somenamespace --type=json -p='[{""op"": ""add"", ""path"": ""/metadata/annotations/eks.amazonaws.com~1role-arn"", ""value"": ""ueah""}]'```.
4. If annotations do not exist, create them and then add the new annotation with this command: ```kubectl patch sa default -n somenamespace --type=json -p='[{""op"":""add"",""path"":""/metadata/annotations"",""value"":{}},{""op"":""add"",""path"":""/metadata/annotations/eks.amazonaws.com~1role-arn"",""value"": ""ueah""}]'```."
